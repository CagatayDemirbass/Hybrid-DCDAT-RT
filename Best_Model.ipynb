{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyLicBdqKyKw"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "\n",
        "!pip install torch==2.3.1+cu121 torchvision --extra-index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q  pycocotools tqdm torchmetrics lxml scipy\n",
        "!pip install -U \"datasets>=2.17.0\"  \"pyarrow>=14.0\"\n",
        "!pip install -U cmake ninja wheel\n",
        "!pip install --no-binary=mmcv mmcv==2.2.0"
      ],
      "metadata": {
        "id": "DQot12iNK32Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "\n",
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "-aEEv_NaK57Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "pip install -U cmake ninja wheel\n",
        "\n",
        "git clone --depth 1 --branch v0.14.6 https://github.com/SHI-Labs/NATTEN.git\n",
        "cd NATTEN\n",
        "\n",
        "export FORCE_CUDA=1\n",
        "export TORCH_CUDA_ARCH_LIST=\"8.0\"\n",
        "\n",
        "pip install .\n",
        "\n",
        "cd .."
      ],
      "metadata": {
        "id": "y3Y7M348K7B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf DAT && git clone -q https://github.com/LeapLabTHU/DAT.git"
      ],
      "metadata": {
        "id": "7uIB0SQaK8Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone --depth 1 https://github.com/OpenGVLab/DCNv4.git\n",
        "cd DCNv4/DCNv4_op\n",
        "export FORCE_CUDA=1\n",
        "export TORCH_CUDA_ARCH_LIST=\"8.0\"\n",
        "python -m pip install . --no-build-isolation -v   # 4-5 dk\n",
        "cd ../..\n"
      ],
      "metadata": {
        "id": "72teomMxK9YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import from actual DAT and DCNv4 modules\n",
        "import importlib\n",
        "from DAT.models.dat import DAT, LayerScale, TransformerStage\n",
        "from DAT.models.dat_blocks import LayerNormProxy, TransformerMLP, TransformerMLPWithConv\n",
        "from DAT.models.dat_blocks import LocalAttention, DAttentionBaseline, ShiftWindowAttention, PyramidAttention\n",
        "from DCNv4.modules.dcnv4 import DCNv4\n",
        "dat_mod = importlib.import_module(\"DAT.models.dat\")\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from __future__ import annotations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.layers import DropPath\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from mmcv.ops.multi_scale_deform_attn import MultiScaleDeformableAttention\n",
        "from timm.models.layers import create_act_layer\n",
        "from typing import Optional, List, Dict, Tuple,  Union\n",
        "\n",
        "\n",
        "\n",
        "class NormFactory:\n",
        "    SUPPORTED = {\"bn\", \"gn\", \"lnp\"}\n",
        "\n",
        "    def __init__(self, kind: str = \"gn\", gn_groups: int = 32):\n",
        "        kind = kind.lower()\n",
        "        if kind not in self.SUPPORTED:\n",
        "            raise ValueError(f\"kind must be one of {self.SUPPORTED}\")\n",
        "        self.kind = kind\n",
        "        self.gn_groups = gn_groups\n",
        "\n",
        "    def __call__(self, num_feat: int) -> nn.Module:\n",
        "        if self.kind == \"bn\":\n",
        "            return nn.BatchNorm2d(num_feat)\n",
        "        if self.kind == \"gn\":\n",
        "            g = math.gcd(self.gn_groups, num_feat) or 1\n",
        "            return nn.GroupNorm(g, num_feat)\n",
        "\n",
        "        if self.kind == \"lnp\":\n",
        "            return LayerNormProxy(num_feat)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. CrossScaleInjection\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class CrossScaleInjection(nn.Module):\n",
        "    \"\"\"Cross‑scale feature enjeksiyonu – kanal‑başına gating versiyonu\"\"\"\n",
        "    def __init__(self, low_ch: int, high_ch: int):\n",
        "        super().__init__()\n",
        "        self.align = nn.Conv2d(low_ch, high_ch, 1, bias=False)\n",
        "        self.norm = LayerNormProxy(high_ch)\n",
        "\n",
        "        # Channel based weight; starting 0.1\n",
        "        self.weight = nn.Parameter(torch.ones(1, high_ch, 1, 1) * 0.15)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.align.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, low_res: torch.Tensor, high_res: torch.Tensor) -> torch.Tensor:\n",
        "        low_aligned = self.align(low_res)\n",
        "        low_up = F.interpolate(low_aligned, size=high_res.shape[-2:],\n",
        "                              mode='bilinear', align_corners=False)\n",
        "        low_up = self.norm(low_up)\n",
        "        # sigmoid → [0,1] for per channel λ\n",
        "        return high_res + torch.sigmoid(self.weight) * low_up\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PGI– Programmable Gradient Injection\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class PGIModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Programmable Gradient Injection v2 – DDP‑safe\n",
        "    * If aux_channels is provided, a 1x1 ‘aux_adapter’ is set up during initialization (static, DDP‑safe).\n",
        "    * The auxiliary contribution is only added in training mode.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        reduction: int = 4,\n",
        "        lambda_bounds: tuple[float, float] = (0.2, 0.7),\n",
        "        init_lambda: float = 0.5,\n",
        "        use_bn: bool = True,\n",
        "        drop_path: float = 0.0,\n",
        "        norm_factory: NormFactory = NormFactory(\"gn\"),\n",
        "        aux_channels: int | None = None,   # ◀︎ yeni\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.lambda_min, self.lambda_max = lambda_bounds\n",
        "\n",
        "        # main branch\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, 3, 1, 1,\n",
        "                      groups=max(1, channels // 4), bias=False),\n",
        "            norm_factory(channels) if use_bn else nn.Identity(),\n",
        "        )\n",
        "\n",
        "        mid = max(channels // reduction, 32)\n",
        "        self.aux = nn.Sequential(\n",
        "            nn.Conv2d(channels, mid, 1, bias=False),\n",
        "            norm_factory(mid) if use_bn else nn.Identity(),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Conv2d(mid, channels, 1, bias=False),\n",
        "            norm_factory(channels) if use_bn else nn.Identity(),\n",
        "        )\n",
        "\n",
        "        # Aux channel adapter (static)\n",
        "        if aux_channels is None or aux_channels == channels:\n",
        "            self.aux_adapter = nn.Identity()\n",
        "        else:\n",
        "            self.aux_adapter = nn.Conv2d(aux_channels, channels, 1, bias=False)\n",
        "            nn.init.kaiming_normal_(self.aux_adapter.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "        # λ (learnable)\n",
        "        import math\n",
        "        init_logit = math.log((init_lambda - self.lambda_min) / (self.lambda_max - init_lambda))\n",
        "        self._lambda_logit = nn.Parameter(torch.tensor(init_logit))\n",
        "\n",
        "        # Gate start 0.6\n",
        "        self.gate = nn.Parameter(torch.ones(1, channels, 1, 1) * 0.5)\n",
        "\n",
        "        self.dp = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "        self._init_weights()\n",
        "\n",
        "    @property\n",
        "    def lambda_val(self) -> torch.Tensor:\n",
        "        σ = torch.sigmoid(self._lambda_logit)\n",
        "        return self.lambda_min + (self.lambda_max - self.lambda_min) * σ\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                if hasattr(m, 'weight'): nn.init.ones_(m.weight)\n",
        "                if hasattr(m, 'bias'):   nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _align_spatial(self, feat: torch.Tensor, target_hw: tuple[int, int]) -> torch.Tensor:\n",
        "        if feat.shape[-2:] != target_hw:\n",
        "            feat = F.interpolate(feat, size=target_hw, mode='bilinear', align_corners=False)\n",
        "        return feat\n",
        "\n",
        "    def forward(self, x: torch.Tensor, aux_input: torch.Tensor | None = None):\n",
        "        main_out = self.main(x)\n",
        "\n",
        "        if aux_input is not None and self.training:\n",
        "            aux = self.aux_adapter(aux_input)\n",
        "            aux = self._align_spatial(aux, x.shape[-2:])\n",
        "            aux = self.aux(aux) * self.gate\n",
        "            main_out = main_out + self.lambda_val * aux\n",
        "\n",
        "        return x + self.dp(main_out)\n",
        "\n",
        "def get_drop_path_rates(num_layers: int, max_rate: float) -> List[float]:\n",
        "    \"\"\"Generate drop path rates with cosine scheduling\n",
        "\n",
        "    Args:\n",
        "        num_layers: Total number of layers\n",
        "        max_rate: Maximum drop path rate\n",
        "\n",
        "    Returns:\n",
        "        List of drop path rates for each layer\n",
        "    \"\"\"\n",
        "    if num_layers <= 1:\n",
        "        return [0.0] * num_layers\n",
        "\n",
        "    # Cosine scheduling for smooth progression\n",
        "    rates = [\n",
        "        max_rate * (1.0 - math.cos(math.pi * i / (num_layers - 1))) * 0.5\n",
        "        for i in range(num_layers)\n",
        "    ]\n",
        "\n",
        "    return rates\n",
        "\n",
        "\n",
        "def init_weights_improved(model: nn.Module):\n",
        "    \"\"\"Improved weight initialization for all module types\"\"\"\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            # Special handling for different conv types\n",
        "            if 'dw' in name or module.groups > 1:  # Depthwise\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='linear')\n",
        "            elif 'pw' in name or module.kernel_size == (1, 1):  # Pointwise\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "            else:  # Regular conv\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.BatchNorm2d):\n",
        "            nn.init.ones_(module.weight)\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, LayerNormProxy):\n",
        "            if hasattr(module, 'weight'):\n",
        "                nn.init.ones_(module.weight)\n",
        "            if hasattr(module, 'bias'):\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            nn.init.trunc_normal_(module.weight, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, std=0.02)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Channel-wise Attention (SE)\n",
        "# -----------------------------------------------------------------------------\n",
        "class ChannelAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, channels: int, reduction: int = 8):\n",
        "        super().__init__()\n",
        "        mid = max(channels // reduction, 16)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(channels, mid, 1, bias=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Conv2d(mid, channels, 1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # --- parameters\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_out', nonlinearity='sigmoid')\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "        # ►–– init scale ≈\n",
        "        self.residual_scale = nn.Parameter(torch.tensor(-2.0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        y = self.avgpool(x)\n",
        "        y = self.relu(self.fc1(y))\n",
        "        y = self.sigmoid(self.fc2(y))\n",
        "\n",
        "        scale = torch.sigmoid(self.residual_scale)   # ∈(0,1)\n",
        "        return x * (1 - scale) + x * y * scale\n",
        "# 5. Gradient-clip helpers\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def clip_dcnv4_grads(model: nn.Module, max_norm: float = 0.2) -> None:\n",
        "    \"\"\"Clip gradients for DCNv4 offset/mask parameters - more aggressive\"\"\"\n",
        "    target_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is None:\n",
        "            continue\n",
        "        n_low = name.lower()\n",
        "        if \"offset\" in n_low or \"mask\" in n_low:\n",
        "            target_params.append(param)\n",
        "\n",
        "    if target_params:\n",
        "        torch.nn.utils.clip_grad_norm_(target_params, max_norm=max_norm)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Helper classes\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def fix_zero_init_params(model):\n",
        "    \"\"\"Fix parameters that are initialized to zero\"\"\"\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.dim() > 0 and torch.all(param == 0):\n",
        "            print(f\"Fixing zero-initialized parameter: {name}\")\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.01)\n",
        "            else:\n",
        "                nn.init.normal_(param, std=0.01)\n",
        "\n",
        "def apply_gradient_checkpointing(model):\n",
        "    \"\"\"Apply gradient checkpointing to backbone stages - IMPROVED\"\"\"\n",
        "    if hasattr(model, 'backbone'):\n",
        "        # Apply a checkpoint for each stage\n",
        "        for stage_name in ['st0', 'st1', 'st2', 'st3']:\n",
        "            if hasattr(model.backbone, stage_name):\n",
        "                stage = getattr(model.backbone, stage_name)\n",
        "                # Make the sequential module checkpoint-friendly\n",
        "                class CheckpointedSequential(nn.Module):\n",
        "                    def __init__(self, *layers):\n",
        "                        super().__init__()\n",
        "                        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "                    def forward(self, x):\n",
        "                        for layer in self.layers:\n",
        "                            x = checkpoint(layer, x, use_reentrant=False)\n",
        "                        return x\n",
        "\n",
        "                # Recreate Stage\n",
        "                checkpointed = CheckpointedSequential(*[block for block in stage])\n",
        "                setattr(model.backbone, stage_name, checkpointed)\n",
        "\n",
        "def init_conv(layer: nn.Conv2d, fan_out: bool = True):\n",
        "    \"\"\"Kaiming normal initialization\"\"\"\n",
        "    nn.init.kaiming_normal_(\n",
        "        layer.weight,\n",
        "        mode=\"fan_out\" if fan_out else \"fan_in\",\n",
        "        nonlinearity=\"relu\",\n",
        "    )\n",
        "    if layer.bias is not None:\n",
        "        nn.init.zeros_(layer.bias)\n",
        "\n",
        "\n",
        "class LNAct(nn.Module):\n",
        "    \"\"\"LayerNormProxy + SiLU\"\"\"\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNormProxy(channels)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.norm(x))\n",
        "\n",
        "\n",
        "class ConvLNAct(nn.Sequential):\n",
        "    \"\"\"Conv + LayerNorm + Act fusion\"\"\"\n",
        "    def __init__(self, in_ch: int, out_ch: int, k: int = 3,\n",
        "                s: int = 1, p: int = None, g: int = 1):\n",
        "        p = p if p is not None else k // 2\n",
        "        super().__init__(\n",
        "            nn.Conv2d(in_ch, out_ch, k, s, p, groups=g, bias=False),\n",
        "            LNAct(out_ch)\n",
        "        )\n",
        "        nn.init.kaiming_normal_(self[0].weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# LayerScale\n",
        "# -----------------------------------------------------------------------------\n",
        "class LayerScale(nn.Module):\n",
        "    \"\"\"\n",
        "    Learnable scalar per layer (γ) – Broadcast-Safe version.\n",
        "    Works with both (B,C,H,W) and (B,N,C) tensor layouts.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        inplace: bool = False,\n",
        "        init_values: float = 1.0,\n",
        "        depth: int | None = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.inplace = inplace\n",
        "        if depth is not None:\n",
        "            init_values = min(1.0, 0.5 * depth / 3)\n",
        "        self.weight = nn.Parameter(torch.ones(dim) * init_values)\n",
        "\n",
        "    def _shape_for(self, x: torch.Tensor) -> tuple[int, ...]:\n",
        "        \"\"\"\n",
        "        Determines the shape to broadcast the weight based on x's placement.\n",
        "        Priority: channel-end (… , C) → channel-first (B, C, …) → singular cases.\n",
        "        \"\"\"\n",
        "        C = self.weight.numel()\n",
        "        nd = x.dim()\n",
        "\n",
        "        # 4D  image: (B, C, H, W) veya (B, H, W, C)\n",
        "        if nd == 4:\n",
        "            if x.shape[1] == C:          # (B, C, H, W)\n",
        "                return (1, C, 1, 1)\n",
        "            if x.shape[-1] == C:         # (B, H, W, C) - nadir\n",
        "                return (1, 1, 1, C)\n",
        "\n",
        "        # 3D: (B, N, C) or (B, C, N) or (C, H, W)\n",
        "        if nd == 3:\n",
        "            if x.shape[-1] == C:         # (B, N, C)\n",
        "                return (1, 1, C)\n",
        "            if x.shape[1] == C:          # (B, C, N)\n",
        "                return (1, C, 1)\n",
        "            if x.shape[0] == C:          # (C, H, W) / (C, N, ?)such as extreme cases\n",
        "                return (C, 1, 1)\n",
        "\n",
        "        # 2D: (B, C) veya (C, B)\n",
        "        if nd == 2:\n",
        "            if x.shape[-1] == C:         # (B, C)\n",
        "                return (1, C)\n",
        "            if x.shape[0] == C:          # (C, B)\n",
        "                return (C, 1)\n",
        "\n",
        "\n",
        "        shape = [1] * max(1, nd)\n",
        "        shape[-1] = C\n",
        "        return tuple(shape)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        w = self.weight.view(*self._shape_for(x))\n",
        "        if self.inplace:\n",
        "            return x.mul_(w)\n",
        "        return x * w\n",
        "\n",
        "# Synchronize the LayerScale within DAT with this version (old behavior is preserved)\n",
        "dat_mod.LayerScale = LayerScale\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Stem\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class Stem(nn.Module):\n",
        "    \"\"\"Three 3×3 convolutions with stride pattern (2,1,2)\"\"\"\n",
        "    def __init__(self, out_channels: int = 128):\n",
        "        super().__init__()\n",
        "        c_mid = out_channels // 2\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, c_mid, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(c_mid, c_mid, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(c_mid, out_channels, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "        self.norm = LayerNormProxy(out_channels)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in [self.conv1, self.conv2, self.conv3]:\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.act(self.conv1(x))\n",
        "        x = self.act(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# DCNv4Lite\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class DCNv4Lite(nn.Module):\n",
        "    \"\"\"\n",
        "    • Fully wraps the DCNv4 path (2D→3D→2D), NO baseline conv/residual mix.\n",
        "    • Offset/mask parameters are initialized with small std (for stable fp32 training).\n",
        "    • Silently discards ‘mix_*’ and ‘conv.*’ keys in old checkpoints.\n",
        "\n",
        "    Args:\n",
        "        channels (int)            : Num of channels\n",
        "        group (int)               : DCNv4 group\n",
        "        kernel_size (int)         : Kernel\n",
        "        stride (int)              : Stride\n",
        "        pad (int)                 : Padding\n",
        "        dilation (int)            : Dilation\n",
        "        offset_scale (float)      : DCNv4 offset scale\n",
        "        without_pointwise (bool)  : Enable/disable the internal 1x1 project in DCNv4 (default: True)\n",
        "        init_offset_mask_std (float): offset/mask start std (default: 0.01)\n",
        "        **kwargs                  : It is forwarded to DCNv4 exactly as is.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 channels: int,\n",
        "                 group: int = 8,\n",
        "                 kernel_size: int = 3,\n",
        "                 stride: int = 1,\n",
        "                 pad: int = 1,\n",
        "                 dilation: int = 1,\n",
        "                 offset_scale: float = 0.1,\n",
        "                 *,\n",
        "                 without_pointwise: bool = True,\n",
        "                 init_offset_mask_std: float = 0.01,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        self.channels = int(channels)\n",
        "\n",
        "        #  DCNv4\n",
        "        self.dcn = DCNv4(\n",
        "            channels=channels, kernel_size=kernel_size, stride=stride, pad=pad,\n",
        "            dilation=dilation, group=group, offset_scale=offset_scale,\n",
        "            center_feature_scale=False, remove_center=False,\n",
        "            output_bias=True, without_pointwise=without_pointwise, **kwargs\n",
        "        )\n",
        "\n",
        "        # Offset/Mask secure init\n",
        "        self._init_dcn_params(std=init_offset_mask_std)\n",
        "\n",
        "        # Save geometry for representation (debug)\n",
        "        self._geom = dict(k=kernel_size, s=stride, p=pad, d=dilation, g=group,\n",
        "                          wop=without_pointwise)\n",
        "\n",
        "    # ---------------- init helpers ----------------\n",
        "    def _init_dcn_params(self, std: float = 0.01):\n",
        "        \"\"\"\n",
        "        Offset/mask parameters are initialized with small normals so that\n",
        "        sigmoid(0)=0.5 exits the plateau lock; other parameters remain in DCNv4's\n",
        "        default init.\n",
        "        \"\"\"\n",
        "        std = float(max(std, 1e-5))\n",
        "        for n, p in self.dcn.named_parameters():\n",
        "            n_low = n.lower()\n",
        "            if ('offset' in n_low) or ('mask' in n_low):\n",
        "                nn.init.normal_(p, std=std)\n",
        "\n",
        "    # ---------------- forward ----------------------\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, C, H, W) → y: (B, C, H, W)\n",
        "        The DCNv4 interface (B, HW, C) requires a 2D→3D→2D conversion.\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        x3d = x.permute(0, 2, 3, 1).contiguous().view(B, H * W, C)   # (B, HW, C)\n",
        "        y3d = self.dcn(x3d, shape=(H, W))                            # (B, HW, C)\n",
        "        y   = y3d.view(B, H, W, C).permute(0, 3, 1, 2).contiguous()  # (B, C, H, W)\n",
        "        return y\n",
        "\n",
        "    # --------------- helpers -------------------\n",
        "    @torch.no_grad()\n",
        "    def freeze_offsets(self, flag: bool = True):\n",
        "        \"\"\"Freeze/unfreeze offset/mask parameters (e.g., for warming).\"\"\"\n",
        "        for n, p in self.dcn.named_parameters():\n",
        "            n_low = n.lower()\n",
        "            if ('offset' in n_low) or ('mask' in n_low):\n",
        "                p.requires_grad = (not flag)\n",
        "\n",
        "    def offset_mask_parameters(self):\n",
        "        \"\"\"In the Optimizer, it yields the offset/mask parameters to provide separate LR/clip.\"\"\"\n",
        "        for n, p in self.dcn.named_parameters():\n",
        "            n_low = n.lower()\n",
        "            if ('offset' in n_low) or ('mask' in n_low):\n",
        "                yield p\n",
        "\n",
        "    # Ignore the mix/baseline keys remaining in old checkpoints\n",
        "    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):\n",
        "        obsolete = (\n",
        "            \"mix_weight\", \"mix_logit\", \"_mix_tau\", \"_mix_eps\", \"_mix_clamp\",\n",
        "            \"conv.weight\", \"conv.bias\"\n",
        "        )\n",
        "        for key in list(state_dict.keys()):\n",
        "            if key.startswith(prefix) and any(key.endswith(obs) for obs in obsolete):\n",
        "                state_dict.pop(key)\n",
        "        return super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        g = self._geom\n",
        "        return (f\"channels={self.channels}, k={g['k']}, s={g['s']}, p={g['p']}, \"\n",
        "                f\"d={g['d']}, group={g['g']}, without_pointwise={g['wop']}\")\n",
        "\n",
        "\n",
        "def boost_relative_position_bias(model: nn.Module, std: float = 0.02):\n",
        "    \"\"\"\n",
        "    Resets all `relative_position_bias_table` parameters in all ShiftWindow/LocalAttention modules belonging to the model.\n",
        "    \"\"\"\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"relative_position_bias_table\"):\n",
        "            nn.init.trunc_normal_(m.relative_position_bias_table, std=std)\n",
        "\n",
        "def init_decoder_sampling_offsets(decoder: nn.Module, bias_val: float = 0.1):\n",
        "    \"\"\"\n",
        "    Set the sampling_offsets bias in the deformable decoder layers\n",
        "    to a fixed value; a small shift triggers the gradient.\n",
        "    \"\"\"\n",
        "    for n, p in decoder.named_parameters():\n",
        "        if \"sampling_offsets.bias\" in n:\n",
        "            nn.init.constant_(p, bias_val)\n",
        "\n",
        "\n",
        "def apply_hybrid_fixup(model: nn.Module):\n",
        "    \"\"\"\n",
        "    a) Relative-pos bias table\n",
        "    b) Decoder sampling_offsets bias\n",
        "    c) (Optional) DCNv4 offset/mask gradient LR boost\n",
        "    \"\"\"\n",
        "    boost_relative_position_bias(model)\n",
        "    if hasattr(model, \"decoder\"):\n",
        "        init_decoder_sampling_offsets(model.decoder, 0.1)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Stage-0 Block\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class StageZeroBlock(nn.Module):\n",
        "    def __init__(self, channels=128, drop_path_rate=0.0,\n",
        "                 use_layer_scale=True, layer_scale_init=1.0,\n",
        "                 norm_factory=NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.conv3 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
        "        nn.init.kaiming_normal_(self.conv3.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "        self.se   = ChannelAttention(channels, reduction=8)\n",
        "        self.norm = norm_factory(channels)\n",
        "        self.scale = LayerScale(channels, init_values=layer_scale_init) if use_layer_scale else nn.Identity()\n",
        "        self.dp = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        out = self.conv3(x)\n",
        "        out = self.se(out)\n",
        "        out = self.norm(out)\n",
        "        out = self.scale(out)\n",
        "        out = self.dp(out)\n",
        "        return res + out\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Stage Blocks\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class StageOneBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    DCNv4Lite + SE + (optional) LayerScale + DropPath\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                channels: int = 128,\n",
        "                drop_path: float = 0.0,\n",
        "                use_layer_scale: bool = True,\n",
        "                layer_scale_init: float = 1.0,\n",
        "                norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.dcn = DCNv4Lite(channels, group=8)\n",
        "        self.se  = ChannelAttention(channels, reduction=8)\n",
        "        self.post_norm = norm_factory(channels)\n",
        "\n",
        "        self.scale = (LayerScale(channels, init_values=layer_scale_init)\n",
        "                      if use_layer_scale else nn.Identity())\n",
        "\n",
        "        self.dp = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        out = self.dcn(x)\n",
        "        out = self.se(out)\n",
        "        out = self.post_norm(out)\n",
        "        out = self.scale(out)\n",
        "        out = self.dp(out)\n",
        "        return res + out\n",
        "\n",
        "class DownABlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Depthwise‑Conv (s=2)  +  Pointwise‑Conv  +  Norm + Act\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                in_ch: int = 128,\n",
        "                out_ch: int = 256,\n",
        "                norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1) Depth‑wise strided conv\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1,\n",
        "                            groups=in_ch, bias=False)\n",
        "\n",
        "        # 2) Point‑wise projection\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "\n",
        "        # 3) Normalization + Activation\n",
        "        #    a) If the norm type is LayerNormProxy, use LNAct\n",
        "        #    b) In other cases, use norm + SiLU\n",
        "        nf_layer = norm_factory(out_ch)\n",
        "        if isinstance(nf_layer, LayerNormProxy):\n",
        "            # LNAct = LayerNormProxy + SiLU\n",
        "            from DAT.models.dat_blocks import LNAct as _LNAct\n",
        "            self.norm_act = _LNAct(out_ch)\n",
        "        else:\n",
        "            self.norm_act = nn.Sequential(nf_layer, nn.SiLU(inplace=True))\n",
        "\n",
        "        # --- Kaiming init ---\n",
        "        nn.init.kaiming_normal_(self.dw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.pw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.dw(x)\n",
        "        x = self.pw(x)\n",
        "        return self.norm_act(x)\n",
        "\n",
        "\n",
        "class StageTwoBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Stage‑2: Only DCNv4Lite path (no DAT, no GateFuse).\n",
        "    CSP split is kept for comparable FLOPs/latency.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 channels: int = 256,\n",
        "                 dcn_group: int = 16,\n",
        "                 drop_path: float = 0.1,\n",
        "                 norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        assert channels % 2 == 0, \"StageTwoBlock expects even channel count.\"\n",
        "        self.split_channels = channels // 2\n",
        "\n",
        "        # --- DCNv4 branch (only this one active) ---\n",
        "        self.dcn = DCNv4Lite(self.split_channels, group=max(1, dcn_group // 2))\n",
        "        self.post_norm = norm_factory(self.split_channels)\n",
        "        self.post_act  = nn.SiLU(inplace=True)\n",
        "\n",
        "        # --- Skip proj (for grad out) ---\n",
        "        self.skip_proj = nn.Sequential(\n",
        "            nn.Conv2d(self.split_channels, self.split_channels, 1, bias=False),\n",
        "            norm_factory(self.split_channels)\n",
        "        )\n",
        "\n",
        "        # --- Output merging ---\n",
        "        self.fusion = nn.Conv2d(channels, channels, 1, bias=False)\n",
        "        self.ln     = norm_factory(channels)\n",
        "        self.act    = nn.SiLU(inplace=True)\n",
        "        self.dp     = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "        # Init\n",
        "        nn.init.kaiming_normal_(self.fusion.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.skip_proj[0].weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # CSP split\n",
        "        x1, x2 = torch.split(x, self.split_channels, dim=1)\n",
        "\n",
        "        # Skip branch\n",
        "        x1 = self.skip_proj(x1)\n",
        "\n",
        "        # DCNv4 branch\n",
        "        x2 = self.dcn(x2)\n",
        "        x2 = self.post_act(self.post_norm(x2))\n",
        "\n",
        "        # Fuse & residual\n",
        "        out = torch.cat([x1, x2], dim=1)\n",
        "        out = self.fusion(out)\n",
        "        out = self.act(self.ln(out))\n",
        "        return x + self.dp(out)\n",
        "\n",
        "\n",
        "class DownBBlock(nn.Module):\n",
        "    \"\"\"DW 3×3 s2 → PW 1×1, 256c → 640c\"\"\"\n",
        "    def __init__(self, in_ch: int = 256, out_ch: int = 640,norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1,\n",
        "                          groups=in_ch, bias=False)\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.norm = norm_factory(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "        for m in [self.dw, self.pw]:\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.act(self.norm(self.pw(self.dw(x))))\n",
        "\n",
        "\n",
        "class StageThreeBlock(nn.Module):\n",
        "    \"\"\"Double DATHubLite + ChannelAttention\"\"\"\n",
        "    def __init__(self, in_ch: int = 640, out_ch: int = 640, drop_path: float = 0.15,norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.norm = norm_factory(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "        heads = max(4, out_ch // 64)\n",
        "        self.dat1 = DATBlock(out_ch, heads=heads)\n",
        "        self.dat2 = DATBlock(out_ch, heads=heads)\n",
        "        self.dp1 = DropPath(drop_path)\n",
        "        self.dp2 = DropPath(drop_path)\n",
        "        self.ca = ChannelAttention(out_ch, reduction=8)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.proj.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.act(self.norm(self.proj(x)))\n",
        "        res = x\n",
        "        x = self.dat1(x)\n",
        "        x = self.dp1(x) + res\n",
        "        res = x\n",
        "        x = self.dat2(x)\n",
        "        x = self.dp2(x) + res\n",
        "        x = self.ca(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DownCBlock(nn.Module):\n",
        "    \"\"\"DW 3×3 s2 → PW 1×1, 640c → 768c\"\"\"\n",
        "    def __init__(self, in_ch: int = 640, out_ch: int = 768,norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1,\n",
        "                          groups=in_ch, bias=False)\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.norm = norm_factory(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.dw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.pw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dw(x)\n",
        "        x = self.pw(x)\n",
        "\n",
        "        return self.act(self.norm(x))\n",
        "# -----------------------------------------------------------------------------\n",
        "# StageAwareBackbone\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class StageAwareBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    Four-tier backbone based on DAT + DCN\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                depths: list[int] = (3, 4, 8, 3),\n",
        "                drop_path_max: float = 0.0,\n",
        "                num_classes: int = 80,\n",
        "                voc_prior: bool = False,\n",
        "                norm_factory: NormFactory = NormFactory(\"gn\"),\n",
        "                use_layer_scale: bool = True,\n",
        "                layer_scale_init: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.voc_prior   = voc_prior\n",
        "\n",
        "        # ---- DropPath rates ----\n",
        "        total_blocks = sum(depths)\n",
        "        dpr = get_drop_path_rates(total_blocks, drop_path_max)\n",
        "        for i in range(depths[0] + depths[1]):   # st0, st1\n",
        "            dpr[i] = 0.0\n",
        "\n",
        "        # ---- Stem ----\n",
        "        self.stem = Stem(128)\n",
        "\n",
        "        # ---- Stage‑0 ----\n",
        "        dp_idx = 0\n",
        "        self.st0 = nn.Sequential(*[\n",
        "          StageZeroBlock(\n",
        "              128,\n",
        "              drop_path_rate=dpr[i],\n",
        "              use_layer_scale=use_layer_scale,\n",
        "              layer_scale_init=layer_scale_init,\n",
        "              norm_factory=norm_factory\n",
        "          )\n",
        "          for i in range(depths[0])\n",
        "      ])\n",
        "        dp_idx += depths[0]\n",
        "\n",
        "        # ---- Stage‑1 ----\n",
        "        self.st1 = nn.Sequential(*[\n",
        "            StageOneBlock(\n",
        "                128,\n",
        "                drop_path=dpr[dp_idx + i],\n",
        "                use_layer_scale=use_layer_scale,\n",
        "                layer_scale_init=layer_scale_init,\n",
        "                norm_factory=norm_factory)\n",
        "            for i in range(depths[1])\n",
        "        ])\n",
        "        dp_idx += depths[1]\n",
        "\n",
        "        # ---- PGI + Down/CSI ----\n",
        "        self.pgi_s1 = PGIModule(128, norm_factory=norm_factory, aux_channels=128)   # s0_out → s1_raw\n",
        "        self.da     = DownABlock(128, 256, norm_factory=norm_factory)\n",
        "        self.csi_s1_s2 = CrossScaleInjection(low_ch=128, high_ch=256)\n",
        "        self.pgi_s2 = PGIModule(256, norm_factory=norm_factory, aux_channels=128)\n",
        "\n",
        "        # ---- Stage‑2 ----\n",
        "        self.st2 = nn.Sequential(*[\n",
        "            StageTwoBlock(\n",
        "                256, dcn_group=16,\n",
        "                drop_path=dpr[dp_idx + i],\n",
        "                norm_factory=norm_factory)\n",
        "            for i in range(depths[2])\n",
        "        ])\n",
        "        dp_idx += depths[2]\n",
        "\n",
        "        # ---- DownB / PGI ----\n",
        "        self.db     = DownBBlock(256, 640, norm_factory=norm_factory)\n",
        "        self.pgi_s3 = PGIModule(640, norm_factory=norm_factory, aux_channels=256)\n",
        "\n",
        "        # ---- Stage‑3 ----\n",
        "        self.st3 = nn.Sequential(*[\n",
        "            StageThreeBlock(\n",
        "                640, 640,\n",
        "                drop_path=dpr[dp_idx + i],\n",
        "                norm_factory=norm_factory)\n",
        "            for i in range(depths[3])\n",
        "        ])\n",
        "\n",
        "        # ---- DownC ----\n",
        "        self.dc = DownCBlock(640, 768, norm_factory=norm_factory)\n",
        "\n",
        "        # ---- Auxiliary dense heads ----\n",
        "        self.aux_head_s1 = nn.Conv2d(128, num_classes + 4, 1, bias=True)\n",
        "        self.aux_head_s2 = nn.Conv2d(256, num_classes + 4, 1, bias=True)\n",
        "        self.aux_head_s3 = nn.Conv2d(640, num_classes + 4, 1, bias=True)\n",
        "        self._init_aux_heads()\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def _init_aux_heads(self):\n",
        "        prior_value = 19 if self.voc_prior else 99\n",
        "        for head in (self.aux_head_s1, self.aux_head_s2, self.aux_head_s3):\n",
        "            nn.init.normal_(head.weight, std=0.01)\n",
        "            nn.init.constant_(head.bias[: self.num_classes], -math.log(prior_value))\n",
        "            nn.init.constant_(head.bias[self.num_classes :], 0.0)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def forward(self, x: torch.Tensor, need_aux: bool = False):\n",
        "        x      = self.stem(x)\n",
        "        s0_out = self.st0(x)\n",
        "\n",
        "        s1_raw = self.st1(s0_out)\n",
        "        p3     = self.pgi_s1(s1_raw, aux_input=s0_out)\n",
        "\n",
        "        p3_down = self.da(p3)\n",
        "        s2_in   = self.csi_s1_s2(p3, p3_down)\n",
        "        s2_in   = self.pgi_s2(s2_in, aux_input=p3)\n",
        "\n",
        "        p4 = self.st2(s2_in)\n",
        "        p4_down = self.db(p4)\n",
        "        s3_in   = self.pgi_s3(p4_down, aux_input=p4)\n",
        "\n",
        "        p5 = self.st3(s3_in)\n",
        "        p6 = self.dc(p5)\n",
        "\n",
        "        if self.training and need_aux:\n",
        "            aux = {\n",
        "                's1': self.aux_head_s1(p3),\n",
        "                's2': self.aux_head_s2(p4),\n",
        "                's3': self.aux_head_s3(p5),\n",
        "            }\n",
        "            return (p3, p4, p5, p6), aux\n",
        "        return p3, p4, p5, p6\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# SpatialFuseNode\n",
        "# ----------------------------------------------------------\n",
        "class SpatialFuseNode(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatial-aware fuse (group-wise, production-ready, RPB-friendly)\n",
        "      • Group-average energy map per input (B,G,K,H,W)\n",
        "      • 1×1 projection per group (K→K), softplus-normalized\n",
        "      • Initial fully uniform: proj.weight=0, bias=0  → equal w for each branch\n",
        "      • Learnable temperature τ (with clamp), optional uniform floor (no dead branches)\n",
        "      • No detach anywhere → gradients are unclipped (including RPBs)\n",
        "\n",
        "    Args:\n",
        "        n_inputs (int): K, how many features to combine (>=2)\n",
        "        channels (int): C, number of channels\n",
        "        groups (int): G, number of groups (automatically drops to 1 if C % G ≠ 0)\n",
        "        tau_init (float): τ start (recommended: 0.9)\n",
        "        learnable_tau (bool): Should τ be learned?\n",
        "        eps (float): Numerical epsilon for normalization and splitting\n",
        "        init_noise (float): (backward compatibility parameter; not used with uniform init)\n",
        "        gate_floor (float): w ← (1-α)·w + α·(1/K); α∈[0,0.1] recommended, 0 closed\n",
        "        tau_bounds (tuple): τ lower/upper bounds (e.g., (0.5, 2.0))\n",
        "        uniform_init (bool): True ⇒ weight=0, bias=0 (full uniform initialization)\n",
        "        save_last (bool): If True, last w is saved (for debug/regularizer)\n",
        "        prenorm (str): “none” | “rms”  — RMS prenorm on the K axis (reduces saturation)\n",
        "\n",
        "    Notes:\n",
        "      • Returns extra_loss() for the entropy regularizer (optional, can be added to the loss).\n",
        "      • The last w (B,G,K,H,W) can be inspected with get_last_weights() (if save_last=True).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_inputs: int,\n",
        "                 channels: int,\n",
        "                 groups: int = 4,\n",
        "                 tau_init: float = 0.9,\n",
        "                 learnable_tau: bool = True,\n",
        "                 eps: float = 5e-4,\n",
        "                 init_noise: float = 1e-3,\n",
        "                 *,\n",
        "                 gate_floor: float = 0.0,\n",
        "                 tau_bounds: tuple = (0.5, 2.0),\n",
        "                 uniform_init: bool = True,\n",
        "                 save_last: bool = False,\n",
        "                 prenorm: str = \"none\"):\n",
        "        super().__init__()\n",
        "        assert n_inputs >= 2, \"SpatialFuseNode: should be n_inputs >= 2 .\"\n",
        "        self.K = int(n_inputs)\n",
        "        self.C = int(channels)\n",
        "        self.G = int(groups) if (groups > 0 and channels % groups == 0) else 1\n",
        "        self.gC = self.C // self.G\n",
        "        self.eps = float(eps)\n",
        "        self.gate_floor = float(gate_floor)\n",
        "        self.tau_lo, self.tau_hi = float(tau_bounds[0]), float(tau_bounds[1])\n",
        "        self.save_last = bool(save_last)\n",
        "        self.prenorm = str(prenorm).lower()\n",
        "        self._last_w = None  # debug amaçlı\n",
        "\n",
        "        # Group-wise K→K projection (produces spatially-varying logit)\n",
        "        # Takes input divided into G groups (B, G*K, H, W) → Output (B, G*K, H, W)\n",
        "        self.proj = nn.Conv2d(self.G * self.K, self.G * self.K, kernel_size=1,\n",
        "                              groups=self.G, bias=True)\n",
        "\n",
        "        # --- Uniform start (to prevent early arm saturation) ---\n",
        "        if uniform_init:\n",
        "            nn.init.zeros_(self.proj.bias)\n",
        "            with torch.no_grad():\n",
        "                self.proj.weight.zero_()\n",
        "        else:\n",
        "            nn.init.kaiming_uniform_(self.proj.weight, a=math.sqrt(5))\n",
        "            nn.init.zeros_(self.proj.bias)\n",
        "\n",
        "        # --- temperature τ ---\n",
        "        if learnable_tau:\n",
        "            self.log_tau = nn.Parameter(torch.log(torch.tensor(float(tau_init))))\n",
        "        else:\n",
        "            self.register_buffer(\"log_tau\", torch.log(torch.tensor(float(tau_init))), persistent=False)\n",
        "\n",
        "    # ------------------ helpers ------------------\n",
        "    @torch.no_grad()\n",
        "    def set_tau(self, tau: float):\n",
        "        \"\"\"To adjust τ during heating.\"\"\"\n",
        "        v = max(1e-3, float(tau))\n",
        "        t = torch.log(torch.tensor(v, device=self.log_tau.device, dtype=self.log_tau.dtype))\n",
        "        self.log_tau.copy_(t)\n",
        "\n",
        "    def get_last_weights(self):\n",
        "        \"\"\"The last calculated w (B, G, K, H, W). If save_last=True, it is saved.\"\"\"\n",
        "        return self._last_w\n",
        "\n",
        "    def extra_loss(self) -> dict:\n",
        "        \"\"\"\n",
        "        Optional regulator: gate entropy (higher values result in more balanced branching).\n",
        "        You can add a small coefficient on the loss side (e.g., 1e-4..5e-4).\n",
        "        \"\"\"\n",
        "        if self._last_w is None:\n",
        "            return {}\n",
        "        p = self._last_w.clamp_min(1e-8)\n",
        "        # Entropy: -sum_k p log p / log(K)  → [0,1]\n",
        "        ent = -(p * p.log()).sum(dim=2) / math.log(self.K)   # (B,G,H,W)\n",
        "        return {\"gate_entropy\": ent.mean()}\n",
        "\n",
        "    def _group_pool(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # (B,C,H,W) → (B,G,H,W), average per group\n",
        "        B, C, H, W = x.shape\n",
        "        return x.view(B, self.G, self.gC, H, W).mean(dim=2)\n",
        "\n",
        "    # ------------------ forward ------------------\n",
        "    def forward(self, *features: torch.Tensor) -> torch.Tensor:\n",
        "        # All inputs must be in the same format.\n",
        "        K = len(features)\n",
        "        assert K == self.K, f\"SpatialFuseNode: {self.K} giriş bekleniyordu, {K} geldi.\"\n",
        "        B, C, H, W = features[0].shape\n",
        "        for f in features:\n",
        "            assert f.shape == (B, C, H, W), \"Tüm giriş feature'lar (B,C,H,W) aynı şekil olmalı.\"\n",
        "\n",
        "        # 1) Grup havuzu → (B,G,K,H,W)\n",
        "        gp = [self._group_pool(f) for f in features]\n",
        "        gp_cat = torch.stack(gp, dim=2)  # (B,G,K,H,W)\n",
        "\n",
        "        # 2) (optional) RMS prenorm → logit scale control on the K axis\n",
        "        if self.prenorm == \"rms\":\n",
        "            rms = gp_cat.pow(2).mean(dim=2, keepdim=True).add(1e-6).sqrt()\n",
        "            gp_cat = gp_cat / rms\n",
        "\n",
        "        # 3) Projection and gate logits\n",
        "        x = gp_cat.flatten(1, 2)                      # (B, G*K, H, W)\n",
        "        logits = self.proj(x).view(B, self.G, self.K, H, W)\n",
        "\n",
        "        # 4) Softplus-normalize + τ\n",
        "        tau = self.log_tau.exp().clamp(self.tau_lo, self.tau_hi)\n",
        "        w = F.softplus(logits / tau) + 1e-9           # (B,G,K,H,W), her yerde >0\n",
        "        w = w / (w.sum(dim=2, keepdim=True) + self.eps)\n",
        "\n",
        "        # 5) Uniform floor (no dead leg, gradient flows to every leg)\n",
        "        if self.gate_floor > 0.0:\n",
        "            u = 1.0 / float(self.K)\n",
        "            w = (1.0 - self.gate_floor) * w + self.gate_floor * u  # no need to normalize again\n",
        "\n",
        "        if self.save_last:\n",
        "            self._last_w = w.detach()\n",
        "\n",
        "        # 6) Distribute weights to channels efficiently (no repeats, memory-friendly)\n",
        "        #    w_k: (B,G,1,H,W), f: (B,G,gC,H,W) → contribution (B,C,H,W)\n",
        "        out = None\n",
        "        for k, f in enumerate(features):\n",
        "            wk = w[:, :, k, :, :].unsqueeze(2)                 # (B,G,1,H,W)\n",
        "            contrib = (f.view(B, self.G, self.gC, H, W) * wk).view(B, C, H, W)\n",
        "            out = contrib if out is None else (out + contrib)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class RMSNorm2d(nn.Module):\n",
        "    def __init__(self, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = float(eps)\n",
        "    def forward(self, x):\n",
        "        return x / (x.pow(2).mean(dim=(2,3), keepdim=True).add(self.eps).sqrt())\n",
        "\n",
        "def _init_laplacian_dw(dw: nn.Conv2d):\n",
        "    with torch.no_grad():\n",
        "        k = torch.tensor([[0., 1., 0.],\n",
        "                          [1.,-4., 1.],\n",
        "                          [0., 1., 0.]], dtype=dw.weight.dtype, device=dw.weight.device)\n",
        "        w = torch.zeros_like(dw.weight)\n",
        "        w[:, 0, :, :] = k\n",
        "        dw.weight.copy_(w)\n",
        "\n",
        "def _ste_boost(x: torch.Tensor, gain: float) -> torch.Tensor:\n",
        "    return x if gain <= 0 else (x + gain * (x - x.detach()))\n",
        "\n",
        "class _FPNResBlock(nn.Module):\n",
        "    def __init__(self, channels: int, drop_path: float = 0.0, norm_factory: 'NormFactory' = None):\n",
        "        super().__init__()\n",
        "        nf = norm_factory or NormFactory(\"gn\")\n",
        "        self.conv = ConvLNAct(channels, channels, k=3)\n",
        "        self.ls   = LayerScale(channels, init_values=0.6)\n",
        "        self.dp   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "    def forward(self, x):\n",
        "        return x + self.dp(self.ls(self.conv(x)))\n",
        "\n",
        "class LightBiFPN(nn.Module):\n",
        "    \"\"\"\n",
        "    LightBiFPN v4.1 — spatial‑aware, RPB‑friendly, repeat‑shared up/edge\n",
        "      • SpatialFuseNode (group‑based 2D weights)\n",
        "      • No‑blur in first top‑down (bilinear + norm/act)\n",
        "      • Up-refine: DW 3×3 + (optional) Laplacian residual (small LS)\n",
        "      • RMS prenorm (optional)\n",
        "      • Low DropPath schedule (0.0 → 0.01)\n",
        "      • Light grad-boost for p3\n",
        "      • **New**: 3 up/edge blocks and **shared between iterations**\n",
        "                  → No “NEVER UPDATED”, no parameter waste\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: tuple[int,int,int,int] = (128, 256, 640, 768),\n",
        "        out: int = 256,\n",
        "        repeats: int = 2,\n",
        "        *,\n",
        "        norm_factory: 'NormFactory' | None = None,\n",
        "        # ---- fuse ----\n",
        "        use_spatial_fuse: bool = True,\n",
        "        fuse_groups: int = 4,\n",
        "        fuse_tau_init: float = 0.9,\n",
        "        fuse_learn_tau: bool = True,\n",
        "        fuse_eps: float = 5e-4,\n",
        "        init_noise: float = 1e-4,\n",
        "        # ---- refine & norm ----\n",
        "        prenorm: str = \"rms\",            # \"none\" | \"rms\"\n",
        "        no_blur_first: bool = True,\n",
        "        edge_enhance: bool = True,\n",
        "        edge_ls_init: float = 0.03,\n",
        "        # ---- drop path ----\n",
        "        dp_top_second: float = 0.01,\n",
        "        dp_bot_second: float = 0.01,\n",
        "        # ---- grad boost ----\n",
        "        grad_boost_low: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        in3, in4, in5, in6 = in_channels\n",
        "        self.repeats = int(repeats)\n",
        "        self.out = int(out)\n",
        "        self.norm_factory = norm_factory or NormFactory(\"gn\")\n",
        "        self.prenorm = prenorm.lower()\n",
        "        self.no_blur_first = bool(no_blur_first)\n",
        "        self.edge_enhance = bool(edge_enhance)\n",
        "        self.grad_boost_low = float(grad_boost_low)\n",
        "        self.use_spatial_fuse = bool(use_spatial_fuse)\n",
        "        self.fuse_groups = int(fuse_groups)\n",
        "        self.fuse_tau_init = float(fuse_tau_init)\n",
        "        self.fuse_learn_tau = bool(fuse_learn_tau)\n",
        "        self.fuse_eps = float(fuse_eps)\n",
        "        self.init_noise = float(init_noise)\n",
        "\n",
        "        # 1x1 projections\n",
        "        self.p3_in = ConvLNAct(in3, out, k=1, p=0)\n",
        "        self.p4_in = ConvLNAct(in4, out, k=1, p=0)\n",
        "        self.p5_in = ConvLNAct(in5, out, k=1, p=0)\n",
        "        self.p6_in = ConvLNAct(in6, out, k=1, p=0)\n",
        "\n",
        "        # DropPath schedule\n",
        "        top_dps = [0.0, float(dp_top_second)]\n",
        "        bot_dps = [0.0, float(dp_bot_second)]\n",
        "        self.top_convs = nn.ModuleList([\n",
        "            _FPNResBlock(out, drop_path=top_dps[i // 3], norm_factory=self.norm_factory)\n",
        "            for i in range(3 * self.repeats)\n",
        "        ])\n",
        "        self.bot_convs = nn.ModuleList([\n",
        "            _FPNResBlock(out, drop_path=bot_dps[i // 4], norm_factory=self.norm_factory)\n",
        "            for i in range(4 * self.repeats)\n",
        "        ])\n",
        "\n",
        "        # Fuse nodes\n",
        "        def _make_fuse(K: int):\n",
        "            from typing import Callable\n",
        "\n",
        "            return SpatialFuseNode(\n",
        "                                      K, channels=out, groups=self.fuse_groups,\n",
        "                                      tau_init=1.4, learnable_tau=True,\n",
        "                                      tau_bounds=(0.8, 1.8),\n",
        "                                      prenorm=\"rms\",\n",
        "                                      gate_floor=0.02,          # no death\n",
        "                                      uniform_init=True,\n",
        "                                      eps=5e-4,\n",
        "                                      save_last=True\n",
        "                                  )\n",
        "\n",
        "        self.fuse2_top = nn.ModuleList([_make_fuse(2) for _ in range(3 * self.repeats)])\n",
        "        self.fuse2_bot = nn.ModuleList([_make_fuse(2) for _ in range(3 * self.repeats)])\n",
        "        self.fuse3_bot = nn.ModuleList([_make_fuse(3) for _ in range(1 * self.repeats)])\n",
        "\n",
        "        # ---  3 up/edge blocks and shared between repeats ---\n",
        "        self.up_dw, self.up_na = nn.ModuleList(), nn.ModuleList()\n",
        "        self.edge_dw, self.edge_ls = nn.ModuleList(), nn.ModuleList()\n",
        "        for _ in range(3):  # only for 3 items\n",
        "            dw = nn.Conv2d(out, out, 3, 1, 1, groups=out, bias=False)\n",
        "            nn.init.kaiming_normal_(dw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            self.up_dw.append(dw)\n",
        "\n",
        "            nf_layer = self.norm_factory(out)\n",
        "            if isinstance(nf_layer, LayerNormProxy):\n",
        "                from DAT.models.dat_blocks import LNAct as _LNAct\n",
        "                self.up_na.append(_LNAct(out))\n",
        "            else:\n",
        "                self.up_na.append(nn.Sequential(nf_layer, nn.SiLU(inplace=True)))\n",
        "\n",
        "            edw = nn.Conv2d(out, out, 3, 1, 1, groups=out, bias=False)\n",
        "            _init_laplacian_dw(edw)\n",
        "            self.edge_dw.append(edw)\n",
        "            self.edge_ls.append(LayerScale(out, init_values=edge_ls_init))\n",
        "\n",
        "        # High-level passthrough LS (p5, p4, p3)\n",
        "        self.keep_top = nn.ModuleList([LayerScale(out, init_values=0.10) for _ in range(3)])\n",
        "\n",
        "        # Input balancing\n",
        "        self.balance = RMSNorm2d(eps=1e-6) if self.prenorm == \"rms\" else nn.Identity()\n",
        "\n",
        "    # --- helpers ---\n",
        "    def _up_refine_shared(self, x: torch.Tensor, size_hw: tuple[int,int], step_id: int, rep_idx: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        step_id: [0,1,2] → corresponds to each top-down step, independent of repeat\n",
        "        \"\"\"\n",
        "        x = F.interpolate(x, size=size_hw, mode=\"bilinear\", align_corners=False)\n",
        "        if self.no_blur_first and rep_idx == 0:\n",
        "            # no-blur first repeat: only norm/act\n",
        "            x = self.up_na[step_id](x)\n",
        "        else:\n",
        "            x = self.up_dw[step_id](x)\n",
        "            if self.edge_enhance:\n",
        "                x = x + self.edge_ls[step_id](self.edge_dw[step_id](x))\n",
        "            x = self.up_na[step_id](x)\n",
        "        return x\n",
        "\n",
        "    def _bal(self, *xs):\n",
        "        return (tuple(self.balance(x) for x in xs) if self.prenorm != \"none\" else xs)\n",
        "\n",
        "    def forward(self, p3, p4, p5, p6):\n",
        "        # Proj\n",
        "        p3, p4, p5, p6 = self.p3_in(p3), self.p4_in(p4), self.p5_in(p5), self.p6_in(p6)\n",
        "\n",
        "        # Low-level grad-boost\n",
        "        if self.training and self.grad_boost_low > 0:\n",
        "            p3 = _ste_boost(p3, self.grad_boost_low)\n",
        "\n",
        "        tconv = bconv = f2t = f2b = f3b = 0\n",
        "\n",
        "        for rep in range(self.repeats):\n",
        "            # ---------- Top‑down ----------\n",
        "            # step_id = 0: p6→p5\n",
        "            u5 = self._up_refine_shared(p6, p5.shape[-2:], step_id=0, rep_idx=rep)\n",
        "            p5_b, u5_b = self._bal(p5, u5)\n",
        "            p5_td = self.fuse2_top[f2t](p5_b, u5_b); f2t += 1\n",
        "            p5_td = self.top_convs[tconv](p5_td); tconv += 1\n",
        "            p5_td = p5_td + self.keep_top[0](p5)\n",
        "\n",
        "            # step_id = 1: p5_td→p4\n",
        "            u4 = self._up_refine_shared(p5_td, p4.shape[-2:], step_id=1, rep_idx=rep)\n",
        "            p4_b, u4_b = self._bal(p4, u4)\n",
        "            p4_td = self.fuse2_top[f2t](p4_b, u4_b); f2t += 1\n",
        "            p4_td = self.top_convs[tconv](p4_td); tconv += 1\n",
        "            p4_td = p4_td + self.keep_top[1](p4)\n",
        "\n",
        "            # step_id = 2: p4_td→p3\n",
        "            u3 = self._up_refine_shared(p4_td, p3.shape[-2:], step_id=2, rep_idx=rep)\n",
        "            p3_b, u3_b = self._bal(p3, u3)\n",
        "            p3_td = self.fuse2_top[f2t](p3_b, u3_b); f2t += 1\n",
        "            p3_td = self.top_convs[tconv](p3_td); tconv += 1\n",
        "            p3_td = p3_td + self.keep_top[2](p3)\n",
        "\n",
        "            # ---------- Bottom‑up ----------\n",
        "            p3 = self.fuse2_bot[f2b](*self._bal(p3, p3_td)); f2b += 1\n",
        "            p3 = self.bot_convs[bconv](p3); bconv += 1\n",
        "\n",
        "            p4 = self.fuse2_bot[f2b](*self._bal(p4_td, F.max_pool2d(p3, 2))); f2b += 1\n",
        "            p4 = self.bot_convs[bconv](p4); bconv += 1\n",
        "\n",
        "            p5 = self.fuse3_bot[f3b](*self._bal(p5, F.max_pool2d(p4, 2), p5_td)); f3b += 1\n",
        "            p5 = self.bot_convs[bconv](p5); bconv += 1\n",
        "\n",
        "            p6 = self.fuse2_bot[f2b](*self._bal(p6, F.max_pool2d(p5, 2))); f2b += 1\n",
        "            p6 = self.bot_convs[bconv](p6); bconv += 1\n",
        "\n",
        "        return p3, p4, p5, p6\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Positional Encoding\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class Pos2d(nn.Module):\n",
        "    \"\"\"2D sinusoidal positional encoding\"\"\"\n",
        "    def __init__(self, c: int = 320, max_h: int = 640, max_w: int = 640):\n",
        "        super().__init__()\n",
        "        assert c % 4 == 0\n",
        "        self.cq = c // 4\n",
        "        pos = self._build(max_h, max_w, c)\n",
        "        self.register_buffer(\"pos_table\", pos, persistent=False)\n",
        "\n",
        "    def _build(self, H: int, W: int, C: int) -> torch.Tensor:\n",
        "        yv, xv = torch.meshgrid(\n",
        "            torch.linspace(0, 1, H), torch.linspace(0, 1, W), indexing=\"ij\"\n",
        "        )\n",
        "        div = torch.exp(torch.arange(0, self.cq) * (-math.log(10000.0) / self.cq))\n",
        "        pos_x = (xv[..., None] * div).reshape(H, W, -1)\n",
        "        pos_y = (yv[..., None] * div).reshape(H, W, -1)\n",
        "        pos = torch.cat([pos_y.sin(), pos_y.cos(), pos_x.sin(), pos_x.cos()], dim=2)\n",
        "        return pos.permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        _, _, H, W = x.shape\n",
        "        return self.pos_table[:, :, :H, :W]\n",
        "\n",
        "\n",
        "\n",
        "import inspect\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# DATBlock –  TransformerStage wrapper\n",
        "# -----------------------------------------------------------------------------\n",
        "class DATBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                channels: int,\n",
        "                depth: int = 2,\n",
        "                heads: int | None = None,\n",
        "                window_size: int = 8,\n",
        "                drop: float = 0.0):\n",
        "        super().__init__()\n",
        "        heads = heads or max(4, channels // 32)\n",
        "\n",
        "        cfg = dict(\n",
        "            fmap_size           = (window_size, window_size),\n",
        "            window_size         = window_size,\n",
        "            ns_per_pt           = 4,\n",
        "            dim_in              = channels,\n",
        "            dim_embed           = channels,\n",
        "            depths              = depth,               # INT\n",
        "            stage_spec          = 'N' * depth,\n",
        "            n_groups            = 1,\n",
        "            use_pe              = True,\n",
        "            sr_ratio            = 1,\n",
        "            heads               = heads,               # INT\n",
        "            heads_q             = [heads] * depth,     # LIST\n",
        "            stride              = 1,\n",
        "            offset_range_factor = 2,\n",
        "            dwc_pe              = True,\n",
        "            no_off              = False,\n",
        "            fixed_pe            = False,\n",
        "            attn_drop           = drop,\n",
        "            proj_drop           = drop,\n",
        "            expansion           = 4,\n",
        "            drop                = drop,\n",
        "            drop_path_rate      = [drop] * depth,      # LIST\n",
        "            use_dwc_mlp         = False,\n",
        "            ksize               = 3,\n",
        "            nat_ksize           = 7,\n",
        "            k_qna               = 8,\n",
        "            nq_qna              = 9,\n",
        "            qna_activation      = \"relu\",\n",
        "            layer_scale_value   = 0.3,\n",
        "            use_lpu             = False,\n",
        "            log_cpb             = True,\n",
        "        )\n",
        "\n",
        "        from DAT.models.dat import TransformerStage\n",
        "        self.stage = TransformerStage(**cfg)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x = (B,C,H,W) –\n",
        "        if hasattr(self.stage, 'fmap_size'):\n",
        "            self.stage.fmap_size = x.shape[-2:]\n",
        "        try:\n",
        "            return self.stage(x)\n",
        "        except TypeError:                           # some versions (x,H,W) require\n",
        "            H, W = x.shape[-2:]\n",
        "            return self.stage(x, H, W)\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Deformable Encoder\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class GLUFFN(nn.Module):\n",
        "    \"\"\"\n",
        "    GLU-based FFN:\n",
        "      - First linear: d_model -> 2*inner\n",
        "      - Gate: SwiGLU (SiLU) or GEGLU (GELU)\n",
        "      - Second linear: inner -> d_model\n",
        "\n",
        "    inner width:\n",
        "      • if ffn_dim is given: inner = ffn_dim // 2  (2*inner = ffn_dim)\n",
        "      • otherwise: inner = round(d_model * ffn_mult)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        ffn_dim: Optional[int] = None,\n",
        "        ffn_mult: float = 2.0,\n",
        "        act: str = \"swiglu\",\n",
        "        drop: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if ffn_dim is not None:\n",
        "            inner = max(32, int(ffn_dim) // 2)\n",
        "        else:\n",
        "            inner = max(32, int(round(d_model * float(ffn_mult))))\n",
        "        self.fc1 = nn.Linear(d_model, inner * 2, bias=True)\n",
        "        self.fc2 = nn.Linear(inner, d_model, bias=True)\n",
        "        self.drop = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "\n",
        "        act = act.lower()\n",
        "        if act not in (\"swiglu\", \"geglu\"):\n",
        "            raise ValueError(\"GLUFFN.act must be 'swiglu' or 'geglu'\")\n",
        "        self.act_kind = act\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        xh = self.fc1(x)\n",
        "        x_lin, x_gate = xh.chunk(2, dim=-1)\n",
        "        if self.act_kind == \"swiglu\":\n",
        "            gated = F.silu(x_gate) * x_lin\n",
        "        else:  # 'geglu'\n",
        "            gated = F.gelu(x_gate) * x_lin\n",
        "        return self.drop(self.fc2(gated))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DeformEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    MS-Deformable self-attention + GLU-FFN, Pre-Norm + LayerScale + DropPath.\n",
        "\n",
        "    Args (backward-compatible):\n",
        "        d_model, n_heads, n_levels, n_points\n",
        "        ffn_dim:   optional (if given, GLU inner width is ffn_dim//2)\n",
        "        ffn_mult:  used if ffn_dim is not specified (default 2.0)\n",
        "        ffn_act:   ‘swiglu’ (default) or ‘geglu’\n",
        "        drop, drop_path\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int = 320,\n",
        "        n_heads: int = 10,\n",
        "        n_levels: int = 4,\n",
        "        n_points: int = 4,\n",
        "        *,\n",
        "        ffn_dim: Optional[int] = None,\n",
        "        ffn_mult: float = 2.0,\n",
        "        ffn_act: str = \"swiglu\",\n",
        "        drop: float = 0.0,\n",
        "        drop_path: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn = MultiScaleDeformableAttention(\n",
        "            embed_dims=d_model, num_heads=n_heads,\n",
        "            num_levels=n_levels, num_points=n_points,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ls1   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop1 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp1   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        self.ffn   = GLUFFN(d_model, ffn_dim=ffn_dim, ffn_mult=ffn_mult, act=ffn_act, drop=drop)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ls2   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop2 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp2   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_encoder_reference_points(\n",
        "        spatial_shapes: torch.Tensor, B: int, device, dtype\n",
        "    ) -> torch.Tensor:\n",
        "        ref_list = []\n",
        "        for (H, W) in spatial_shapes.tolist():\n",
        "            ref_y, ref_x = torch.meshgrid(\n",
        "                torch.linspace(0.5, H - 0.5, H, device=device, dtype=dtype) / H,\n",
        "                torch.linspace(0.5, W - 0.5, W, device=device, dtype=dtype) / W,\n",
        "                indexing=\"ij\",\n",
        "            )\n",
        "            ref = torch.stack((ref_x, ref_y), dim=-1).reshape(-1, 2)  # (HW,2)\n",
        "            ref_list.append(ref)\n",
        "        return torch.cat(ref_list, dim=0)[None, :, None, :].repeat(B, 1, spatial_shapes.size(0), 1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.Tensor,             # (B, sumHW, C)\n",
        "        pos: torch.Tensor,             # (B, sumHW, C)\n",
        "        spatial_shapes: torch.Tensor,  # (L, 2)\n",
        "        lvl_start_idx: torch.Tensor,   # (L,)\n",
        "        key_padding_mask: Optional[torch.Tensor] = None,\n",
        "    ) -> torch.Tensor:\n",
        "\n",
        "        B, N, C = src.shape\n",
        "        ref_pts = self._make_encoder_reference_points(\n",
        "            spatial_shapes=spatial_shapes, B=B, device=src.device, dtype=src.dtype\n",
        "        )  # (B,N,L,2)\n",
        "\n",
        "        x = src\n",
        "        q = self.norm1(x)\n",
        "        attn_out = self.self_attn(\n",
        "            query=q, value=q,\n",
        "            reference_points=ref_pts,\n",
        "            spatial_shapes=spatial_shapes,\n",
        "            level_start_index=lvl_start_idx,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            query_pos=pos,\n",
        "        )\n",
        "        x = x + self.dp1(self.ls1(self.drop1(attn_out)))\n",
        "\n",
        "        y = self.ffn(self.norm2(x))\n",
        "        x = x + self.dp2(self.ls2(self.drop2(y)))\n",
        "        return x\n",
        "\n",
        "class TinyDeformEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Two layers are recommended (lightweight and effective).\n",
        "    get_drop_path_rates() is available;\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_layers: int = 1,\n",
        "                 d_model: int = 320,\n",
        "                 n_heads: int = 10,\n",
        "                 n_levels: int = 4,\n",
        "                 n_points: int = 4,\n",
        "                 ffn_dim: int = 1024,\n",
        "                 drop: float = 0.0,\n",
        "                 drop_path_max: float = 0.1):\n",
        "        super().__init__()\n",
        "        dpr = get_drop_path_rates(num_layers, drop_path_max)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DeformEncoderLayer(d_model=d_model,\n",
        "                               n_heads=n_heads,\n",
        "                               n_levels=n_levels,\n",
        "                               n_points=n_points,\n",
        "                               ffn_dim=ffn_dim,\n",
        "                               drop=drop,\n",
        "                               drop_path=dpr[i])\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self,\n",
        "                src: torch.Tensor,             # (B, sumHW, C)\n",
        "                pos: torch.Tensor,             # (B, sumHW, C)\n",
        "                spatial_shapes: torch.Tensor,  # (L,2)\n",
        "                lvl_start_idx: torch.Tensor,   # (L,)\n",
        "                key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        for ly in self.layers:\n",
        "            src = ly(src, pos, spatial_shapes, lvl_start_idx, key_padding_mask)\n",
        "        return src\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# RT-Deform Decoder (Fixed IoU-aware padding)\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class DeformDecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder:\n",
        "      Pre-Norm + LayerScale + DropPath + GLU-FFN + tanh-bounded ref update.\n",
        "\n",
        "    Args (backward-compatible):\n",
        "        d_model, n_heads, n_levels, n_points\n",
        "        ffn_dim:   optional (for GLU inner width, 2*inner = ffn_dim)\n",
        "        ffn_mult:  used if ffn_dim is not specified\n",
        "        ffn_act:   ‘swiglu’ | ‘geglu’\n",
        "        drop, drop_path\n",
        "        refine_scale: float or (lo,hi)  — you can provide layer_id/num_layers for the schedule\n",
        "        grad_eps: very small nudge\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int = 320,\n",
        "        n_heads: int = 10,\n",
        "        n_levels: int = 4,\n",
        "        n_points: int = 4,\n",
        "        *,\n",
        "        ffn_dim: Optional[int] = None,\n",
        "        ffn_mult: float = 2.0,\n",
        "        ffn_act: str = \"swiglu\",\n",
        "        drop: float = 0.0,\n",
        "        drop_path: float = 0.1,\n",
        "        refine_scale: Union[float, Tuple[float, float]] = 0.5,\n",
        "        layer_id: Optional[int] = None,\n",
        "        num_layers: Optional[int] = None,\n",
        "        grad_eps: float = 1e-4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.grad_eps = float(grad_eps)\n",
        "\n",
        "        # refine scale\n",
        "        if isinstance(refine_scale, (tuple, list)):\n",
        "            lo, hi = float(refine_scale[0]), float(refine_scale[1])\n",
        "            if (num_layers is not None) and (layer_id is not None) and (num_layers > 1):\n",
        "                t = float(layer_id) / float(num_layers - 1)\n",
        "                self.refine_scale = lo + (hi - lo) * t\n",
        "            else:\n",
        "                self.refine_scale = 0.5 * (lo + hi)\n",
        "        else:\n",
        "            self.refine_scale = float(refine_scale)\n",
        "\n",
        "        # Self-Attn (MHA) + Pre-Norm\n",
        "        self.self_attn = nn.MultiheadAttention(\n",
        "            embed_dim=d_model, num_heads=n_heads, batch_first=True\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ls1   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop1 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp1   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # Cross-Attn (MS-Deformable) + Pre-Norm\n",
        "        self.cross_attn = MultiScaleDeformableAttention(\n",
        "            embed_dims=d_model, num_heads=n_heads,\n",
        "            num_levels=n_levels, num_points=n_points,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ls2   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop2 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp2   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # FFN (GLU) + Pre-Norm\n",
        "        self.ffn   = GLUFFN(d_model, ffn_dim=ffn_dim, ffn_mult=ffn_mult, act=ffn_act, drop=drop)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.ls3   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop3 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp3   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # Ref delta head (tanh clamp)\n",
        "        self.ref_mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model, 2),\n",
        "        )\n",
        "        for m in self.ref_mlp:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
        "\n",
        "    @staticmethod\n",
        "    def _inv_sigmoid(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "        x = x.clamp(eps, 1.0 - eps)\n",
        "        return torch.log(x) - torch.log(1.0 - x)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt: torch.Tensor,                 # (B, N, C)\n",
        "        ref_pts: torch.Tensor,             # (B, N, L, 2)\n",
        "        src: torch.Tensor,                 # (B, S, C)\n",
        "        query_pos: torch.Tensor,           # (B, N, C)\n",
        "        spatial_shapes: torch.Tensor,      # (L, 2)\n",
        "        lvl_start_idx: torch.Tensor,       # (L,)\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        x = tgt\n",
        "        q = self.norm1(x)\n",
        "        sa_out = self.self_attn(q, q, q, need_weights=False)[0]\n",
        "        x = x + self.dp1(self.ls1(self.drop1(sa_out)))\n",
        "\n",
        "        q = self.norm2(x)\n",
        "        ca_out = self.cross_attn(\n",
        "            query=q, value=src,\n",
        "            reference_points=ref_pts,\n",
        "            spatial_shapes=spatial_shapes,\n",
        "            level_start_index=lvl_start_idx,\n",
        "            query_pos=query_pos,\n",
        "        )\n",
        "        x = x + self.dp2(self.ls2(self.drop2(ca_out)))\n",
        "\n",
        "        y = self.ffn(self.norm3(x))\n",
        "        x = x + self.dp3(self.ls3(self.drop3(y)))\n",
        "\n",
        "        # tanh-bounded iterative ref update\n",
        "        delta = torch.tanh(self.ref_mlp(x)) * self.refine_scale      # (B,N,2)\n",
        "        new_ref = torch.sigmoid(self._inv_sigmoid(ref_pts) + delta.unsqueeze(2).to(ref_pts.dtype))\n",
        "\n",
        "        # tiny nudge\n",
        "        x = x + self.grad_eps * delta.mean(dim=2, keepdim=True)\n",
        "        return x, new_ref\n",
        "\n",
        "\n",
        "class HeadPrep(nn.Module):\n",
        "    def __init__(self, d_model: int, ls_init: float = 0.80, ln_eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(d_model, eps=ln_eps)\n",
        "        self.ls   = LayerScale(d_model, init_values=ls_init)\n",
        "    def forward(self, x):  # x: (B, N, C)\n",
        "        return self.ls(self.norm(x))\n",
        "\n",
        "class RTDeformDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    ABLATION — RT‑Deform Decoder (No IoU head, No Denoising)\n",
        "\n",
        "\n",
        "    * Denoising (DN) queries and the associated label/box noise mechanism have been removed.\n",
        "    * The interface signature is preserved; the dn_queries / use_iou_aware parameters are ignored for ablation.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_obj_classes: int = 20,\n",
        "                 include_background: bool = True,\n",
        "                 *,\n",
        "                 num_queries: int = 300,\n",
        "                 num_layers: int = 4,\n",
        "                 d_model: int = 320,\n",
        "                 attn_n_heads: int = 10,\n",
        "                 dn_queries: int = 0,                 # ignored\n",
        "                 use_iou_aware: bool = False,         # ignored\n",
        "                 # Encoder\n",
        "                 use_encoder: bool = True,\n",
        "                 encoder_layers: int = 1,\n",
        "                 encoder_drop_path_max: float = 0.1,\n",
        "                 # (The following are retained for interface compatibility, not used)\n",
        "                 iou_k_ratio: float = 0.75,\n",
        "                 mqs_enable: bool = True,             # seed/MQS mechanism is not present here\n",
        "                 mqs_obj_ratio: float = 0.30,\n",
        "                 mqs_grid_ratio: float = 0.20,\n",
        "                 mqs_levels: Tuple[int, ...] = (1, 2, 3),\n",
        "                 mqs_local_max_kernel: int = 3,\n",
        "                 mqs_train_only: bool = False,\n",
        "                 seed_enable: bool = True,\n",
        "                 seed_alpha_obj_init: float = 0.55,\n",
        "                 seed_alpha_grid_init: float = 0.4,\n",
        "                 seed_mlp_expansion: float = 1.0,\n",
        "                 use_proposals: bool = True,\n",
        "                 proposal_topk: Optional[int] = None,\n",
        "                 proposal_ratio: float = 0.70,\n",
        "                 min_mqs: int = 60,\n",
        "                 min_left_queries: int = 16,\n",
        "                 emit_obj_logits_in_eval: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        # Number of classes\n",
        "        self.num_obj_classes = num_obj_classes\n",
        "        self.include_background = include_background\n",
        "        self.num_pred = num_obj_classes + int(include_background)\n",
        "\n",
        "        self.num_queries = int(num_queries)\n",
        "        self.num_layers  = int(num_layers)\n",
        "        self.d_model     = int(d_model)\n",
        "        self.use_encoder = bool(use_encoder)\n",
        "\n",
        "        # Position and level embed\n",
        "        self.query_pos  = nn.Embedding(self.num_queries, d_model)  # learned positional (N,C)\n",
        "        self.query_feat = nn.Embedding(self.num_queries, d_model)  # learned content   (N,C)\n",
        "        self.pos_embed   = Pos2d(d_model)\n",
        "        self.level_embed = nn.Parameter(torch.randn(4, d_model))\n",
        "\n",
        "        # Encoder (optional)\n",
        "        if self.use_encoder:\n",
        "            self.encoder = TinyDeformEncoder(\n",
        "                num_layers=encoder_layers,\n",
        "                d_model=d_model, n_heads=attn_n_heads,\n",
        "                n_levels=4, n_points=4,\n",
        "                ffn_dim=d_model * 4,\n",
        "                drop=0.0,\n",
        "                drop_path_max=encoder_drop_path_max\n",
        "            )\n",
        "\n",
        "        # Decoder\n",
        "        n_points_list = [4] * (num_layers - 2) + [2, 2] if num_layers >= 2 else [4]\n",
        "        self.layers = nn.ModuleList([\n",
        "            DeformDecoderLayer(d_model=d_model,\n",
        "                               n_heads=attn_n_heads,\n",
        "                               n_levels=4,\n",
        "                               n_points=n_points_list[i],\n",
        "                               ffn_dim=d_model * 4,\n",
        "                               refine_scale=(0.15, 0.45),\n",
        "                               layer_id=i, num_layers=num_layers)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Head prep + classification / box heads\n",
        "        self.head_prep = HeadPrep(d_model, ls_init=0.80, ln_eps=1e-6)\n",
        "        self.cls_head  = nn.Linear(d_model, self.num_pred)\n",
        "        self.box_head  = nn.Linear(d_model, 4)\n",
        "\n",
        "        # Aux head'ler (except last layer)\n",
        "        if self.num_layers > 1:\n",
        "            self.aux_cls_heads = nn.ModuleList(nn.Linear(d_model, self.num_pred) for _ in range(self.num_layers - 1))\n",
        "            self.aux_box_heads = nn.ModuleList(nn.Linear(d_model, 4)            for _ in range(self.num_layers - 1))\n",
        "        else:\n",
        "            self.aux_cls_heads = nn.ModuleList([])\n",
        "            self.aux_box_heads = nn.ModuleList([])\n",
        "\n",
        "        # Small MLP for initial reference\n",
        "        self.ref_init = nn.Linear(d_model, 2)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    # ------------------- init helpers -------------------\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.query_pos.weight,  std=0.02)\n",
        "        nn.init.normal_(self.query_feat.weight, std=0.02)\n",
        "        nn.init.normal_(self.level_embed,       std=0.02)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.ref_init.weight); nn.init.zeros_(self.ref_init.bias)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.box_head.weight, gain=1.0); nn.init.zeros_(self.box_head.bias)\n",
        "        nn.init.xavier_uniform_(self.cls_head.weight, gain=1.0)\n",
        "\n",
        "        for c, b in zip(self.aux_cls_heads, self.aux_box_heads):\n",
        "            nn.init.xavier_uniform_(c.weight, gain=1.0)\n",
        "            nn.init.xavier_uniform_(b.weight, gain=1.0); nn.init.zeros_(b.bias)\n",
        "\n",
        "        self.reset_class_bias()\n",
        "\n",
        "    def reset_class_bias(self, object_prior: float = .2, no_object_bias: float = -2.):\n",
        "        obj_bias = -math.log((1. - object_prior) / object_prior)\n",
        "        with torch.no_grad():\n",
        "            self.cls_head.bias[:] = 0.\n",
        "            self.cls_head.bias[: self.num_obj_classes] = obj_bias\n",
        "            if self.include_background:\n",
        "                self.cls_head.bias[-1] = no_object_bias\n",
        "            for aux in self.aux_cls_heads:\n",
        "                aux.bias[:] = self.cls_head.bias\n",
        "\n",
        "    # ------------------- forward -------------------\n",
        "    def forward(self,\n",
        "                p3: torch.Tensor, p4: torch.Tensor,\n",
        "                p5: torch.Tensor, p6: torch.Tensor,\n",
        "                targets: Optional[List[Dict]] = None\n",
        "                ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        B = p3.size(0)\n",
        "        device = p3.device\n",
        "        feats = [p6, p5, p4, p3]  # from largest to smallest (B, C, H, W)\n",
        "        L = len(feats)\n",
        "\n",
        "        # Per‑level src & pos (B, ΣHW, C)\n",
        "        src_list, pos_list = [], []\n",
        "        spatial_shapes = torch.zeros(L, 2, dtype=torch.long, device=device)  # (L,2) = (H,W)\n",
        "\n",
        "        for i, f in enumerate(feats):\n",
        "            B_, C, H, W = f.shape\n",
        "            pos_lvl = self.pos_embed(f).expand(B_, -1, -1, -1) + self.level_embed[i].view(1, -1, 1, 1)\n",
        "            src_list.append(f.flatten(2).transpose(1, 2))              # (B, HW, C)\n",
        "            pos_list.append(pos_lvl.flatten(2).transpose(1, 2))         # (B, HW, C)\n",
        "            spatial_shapes[i, 0] = H\n",
        "            spatial_shapes[i, 1] = W\n",
        "\n",
        "        src = torch.cat(src_list, dim=1)                         # (B, ΣHW, C)\n",
        "        pos = torch.cat(pos_list, dim=1).to(dtype=src.dtype)     # (B, ΣHW, C)\n",
        "\n",
        "        # level start index: (L,)\n",
        "        numel_per_level = spatial_shapes[:, 0] * spatial_shapes[:, 1]\n",
        "        lvl_start_idx = torch.cat(\n",
        "            [numel_per_level.new_zeros(1), numel_per_level.cumsum(0)[:-1]],\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        # Encoder (optional)\n",
        "        memory = self.encoder(src, pos, spatial_shapes, lvl_start_idx) if self.use_encoder else src\n",
        "        if pos.size(0) != memory.size(0):\n",
        "            pos = pos.expand(memory.size(0), -1, -1)\n",
        "        pos = pos.to(dtype=memory.dtype)\n",
        "\n",
        "        # Learned queries\n",
        "        content = self.query_feat.weight.unsqueeze(0).expand(B, -1, -1)  # (B,N,C)\n",
        "        qpos    = self.query_pos.weight.unsqueeze(0).expand(B, -1, -1)   # (B,N,C)\n",
        "        init_ref = torch.sigmoid(self.ref_init(qpos))                    # (B,N,2)\n",
        "        ref_pts  = init_ref.unsqueeze(2).expand(-1, -1, 4, -1)           # (B,N,4,2)\n",
        "\n",
        "        tgt = content\n",
        "        aux_out = []\n",
        "\n",
        "        # Decoder\n",
        "        for lid, layer in enumerate(self.layers):\n",
        "            tgt, ref_pts = layer(\n",
        "                tgt=tgt, ref_pts=ref_pts, src=memory,\n",
        "                query_pos=qpos, spatial_shapes=spatial_shapes, lvl_start_idx=lvl_start_idx\n",
        "            )\n",
        "\n",
        "            if self.training and lid < self.num_layers - 1:\n",
        "                h_aux = self.head_prep(tgt)  # LN + LayerScale\n",
        "                aux_logits = self.aux_cls_heads[lid](h_aux)\n",
        "                aux_boxes  = self.aux_box_heads[lid](h_aux).sigmoid()\n",
        "                aux_out.append({\"pred_logits\": aux_logits, \"pred_boxes\": aux_boxes})\n",
        "\n",
        "        # Final heads\n",
        "        h = self.head_prep(tgt)\n",
        "        logits = self.cls_head(h)\n",
        "        boxes  = self.box_head(h).sigmoid()\n",
        "\n",
        "        out: Dict[str, torch.Tensor] = {\"pred_logits\": logits, \"pred_boxes\": boxes,\n",
        "                                        \"final_ref_pts\": ref_pts.contiguous()}\n",
        "        if aux_out and self.training:\n",
        "            out[\"aux_outputs\"] = aux_out\n",
        "        return out\n",
        "# -----------------------------------------------------------------------------\n",
        "# MiniBackbone\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class MiniBackbone(nn.Module):\n",
        "    \"\"\"Lightweight version with CSI and PGI\"\"\"\n",
        "    def __init__(self, depths: List[int] = [6, 6, 12, 6],\n",
        "                drop_path_max: float = 0.2,\n",
        "                num_classes: int = 20):\n",
        "        super().__init__()\n",
        "        self.backbone = StageAwareBackbone(depths, drop_path_max, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, need_aux: bool = False):\n",
        "        # Match the return format of StageAwareBackbone\n",
        "        return self.backbone(x, need_aux)\n",
        "\n",
        "\n",
        "\n",
        "def _looks_like_norm_name(name: str) -> bool:\n",
        "    low = name.lower()\n",
        "\n",
        "    return any(k in low for k in [\n",
        "        \".norm\", \"bn\", \"groupnorm\", \"layernorm\", \"gn\", \"ln.\", \"ln_\", \"lnact\", \"ln_act\", \"lnproxy\", \"layernormproxy\", \"rmsnorm\"\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "class HybridDCDATRT(nn.Module):\n",
        "    \"\"\"\n",
        "    End‑to‑End Hybrid‑DCDAT‑RT detector (DAT + DCNv4 backbone + RT‑Deform Decoder)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_classes:   int = 20,\n",
        "                 num_queries:   int = 100,\n",
        "                 depths:        List[int] = (2, 2, 2, 2),\n",
        "                 drop_path_max: float = 0.0,\n",
        "                 backbone_norm_factory: 'NormFactory' = None,\n",
        "                 neck_norm_factory:     'NormFactory' = None,\n",
        "                 use_layer_scale: bool = True,\n",
        "                 layer_scale_init: float = 1.0,\n",
        "                 # Decoder\n",
        "                 d_model:    int = 320,\n",
        "                 dec_layers: int = 4,\n",
        "                 dn_queries: int = 100,\n",
        "                 # Features\n",
        "                 use_aux_loss: bool = True,\n",
        "                 use_iou_aware: bool = False,\n",
        "                 # LR multipliers\n",
        "                 backbone_lr: float = 0.1,\n",
        "                 head_lr:     float = 1.0,\n",
        "                 # Dataset\n",
        "                 voc_prior: bool = False,\n",
        "                 # Decoder Encoder kontrolü\n",
        "                 decoder_use_encoder: bool = True,\n",
        "                 decoder_encoder_layers: int = 2,\n",
        "                 decoder_encoder_drop_path_max: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # default NormFactory\n",
        "        backbone_norm_factory = backbone_norm_factory or NormFactory(\"gn\")\n",
        "        neck_norm_factory     = neck_norm_factory     or NormFactory(\"gn\")\n",
        "\n",
        "        self.num_classes   = num_classes\n",
        "        self.use_aux_loss  = use_aux_loss\n",
        "\n",
        "        # ---------- Backbone (DAT + CSI + PGI) ----------\n",
        "        self.backbone = StageAwareBackbone(\n",
        "            depths          = depths,\n",
        "            drop_path_max   = drop_path_max,\n",
        "            num_classes     = num_classes,\n",
        "            voc_prior       = voc_prior,\n",
        "            norm_factory    = backbone_norm_factory,\n",
        "            use_layer_scale = use_layer_scale,\n",
        "            layer_scale_init= layer_scale_init\n",
        "        )\n",
        "\n",
        "        # ---------- Neck (Light‑BiFPN) ----------\n",
        "\n",
        "        self.neck = LightBiFPN(\n",
        "          in_channels=(128,256,640,768),\n",
        "          out=d_model,\n",
        "          repeats=2,\n",
        "          use_spatial_fuse=True,\n",
        "          fuse_groups=4,\n",
        "          fuse_tau_init=1.4,\n",
        "          fuse_learn_tau=True,\n",
        "          fuse_eps=5e-4,\n",
        "          grad_boost_low=0.1,     # << 0.5 → 0.3\n",
        "          dp_top_second=0.01, dp_bot_second=0.01,\n",
        "          no_blur_first=True,\n",
        "          edge_enhance=True,\n",
        "          edge_ls_init=0.03,\n",
        "          norm_factory=neck_norm_factory,\n",
        "      )\n",
        "\n",
        "        # ---------- Decoder (RT‑Deform) ----------\n",
        "        self.decoder = RTDeformDecoder(\n",
        "            num_obj_classes = num_classes,\n",
        "            include_background = True,\n",
        "            num_queries     = num_queries,\n",
        "            num_layers      = dec_layers,\n",
        "            d_model         = d_model,\n",
        "            dn_queries      = dn_queries,\n",
        "            use_iou_aware   = use_iou_aware,\n",
        "            use_encoder                 = decoder_use_encoder,\n",
        "            encoder_layers              = decoder_encoder_layers,\n",
        "            encoder_drop_path_max       = decoder_encoder_drop_path_max,\n",
        "        )\n",
        "\n",
        "        # (optional)\n",
        "        self._lr_mult = {\n",
        "            \"backbone\": backbone_lr,\n",
        "            \"head\":     head_lr,\n",
        "            \"bias\":     2.0,\n",
        "            \"dcn_bias\": 10.0,\n",
        "        }\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                targets: Optional[List[Dict]] = None) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        The output dictionary is compatible with RTDeformDecoder outputs:\n",
        "          - ‘pred_logits’, ‘pred_boxes’, optional ‘pred_ious’\n",
        "          - in training: ‘aux_outputs’, ‘dn_meta’, ‘query_selection_mask’\n",
        "          - optional: ‘aux_dense’ (auxiliary dense headers from the backbone)\n",
        "        \"\"\"\n",
        "        if not x.is_cuda:\n",
        "            raise RuntimeError(\"HybridDCDATRT expects CUDA tensor input.\")\n",
        "\n",
        "        # Backbone\n",
        "        if self.training and self.use_aux_loss:\n",
        "            (p3, p4, p5, p6), aux_dense = self.backbone(x, need_aux=True)\n",
        "        else:\n",
        "            p3, p4, p5, p6 = self.backbone(x, need_aux=False)\n",
        "            aux_dense      = None\n",
        "\n",
        "        # Neck\n",
        "        p3, p4, p5, p6 = self.neck(p3, p4, p5, p6)\n",
        "\n",
        "        # Decoder\n",
        "        if self.training and targets is not None:\n",
        "            dec_out = self.decoder(p3, p4, p5, p6, targets)\n",
        "        else:\n",
        "            dec_out = self.decoder(p3, p4, p5, p6)\n",
        "\n",
        "        if aux_dense is not None:\n",
        "            dec_out[\"aux_dense\"] = aux_dense\n",
        "        return dec_out\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "\n",
        "    def param_groups(self,\n",
        "                    base_lr: float = 1e-4,\n",
        "                    *,\n",
        "                    weight_decay: float = 0.02,\n",
        "                    bb_mult: float = 0.5,\n",
        "                    dec_mult: float = 2.0,\n",
        "                    bias_mult: float = 1.0,      # Bias boost\n",
        "                    dcn_mult: float = 1.5,       # DCN offset/mask boost\n",
        "                    rpb_mult: float = 5.0,       # RPB boost\n",
        "                    gate_mult: float = 1.0,      # Gate params boost\n",
        "                    ls_mult: float = 0.3,        # LayerScale reduction\n",
        "                    pos_mult: float = 1.5):\n",
        "        \"\"\"\n",
        "        parameter grouping with optimized LR scheduling.\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        buckets = {\n",
        "            # Backbone\n",
        "            \"bb_w\": [], \"bb_b\": [], \"bb_norm\": [],\n",
        "            \"dcn_off\": [],  # DCN offset/mask\n",
        "\n",
        "            # Neck / Head\n",
        "            \"hd_w\": [], \"hd_b\": [], \"hd_norm\": [],\n",
        "            \"fuse_gate\": [],  # Fusion gates\n",
        "\n",
        "            # Decoder\n",
        "            \"dec_w\": [], \"dec_b\": [], \"dec_norm\": [],\n",
        "            \"deform_off_b\": [],  # Deformable attention bias\n",
        "\n",
        "            # Special\n",
        "            \"pos_embed\": [], \"rpb_p\": [], \"ls_gamma\": [],\n",
        "            \"dat_scale_p\": [], \"scalars_gain\": [],\n",
        "        }\n",
        "\n",
        "        def is_norm_name(n: str) -> bool:\n",
        "            return any(x in n for x in [\".norm.\", \".bn.\", \".ln.\", \".gn.\"])\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            if not param.requires_grad:\n",
        "                continue\n",
        "\n",
        "            # --- Special cases (prefix-independent) ---\n",
        "\n",
        "            # RPB tables (needs high LR)\n",
        "            if name.endswith(\".rpb\") or \"relative_position_bias\" in name:\n",
        "                buckets[\"rpb_p\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Positional embeddings\n",
        "            if re.search(r\"(level_embed(_seed)?|query_pos|query_feat|label_enc|pos_table)\", name):\n",
        "                buckets[\"pos_embed\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # LayerScale gamma\n",
        "            if re.search(r\"(layer_scales\\.\\d+\\.weight|\\.scale\\.weight$|\\.ls\\.weight$)\", name):\n",
        "                buckets[\"ls_gamma\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Scalar gates\n",
        "            if any(name.endswith(x) for x in [\"residual_scale\", \"_lambda_logit\", \"dc_res_logit\", \"seed_logit_obj\", \"seed_logit_grid\"]):\n",
        "                buckets[\"scalars_gain\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # DAT scale parameters\n",
        "            if \"dat_logit\" in name or \"dat_log_tau\" in name:\n",
        "                buckets[\"dat_scale_p\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Neck fusion gates\n",
        "            if \"neck.fuse\" in name and \"log_tau\" in name:\n",
        "                buckets[\"fuse_gate\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Deformable attention sampling bias\n",
        "            if \"sampling_offsets.bias\" in name:\n",
        "                buckets[\"deform_off_b\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # --- Module-based categorization ---\n",
        "\n",
        "            # Backbone\n",
        "            if name.startswith(\"backbone.\"):\n",
        "                # DCN offset/mask (special treatment)\n",
        "                if \"offset_mask\" in name and \"dcn\" in name:\n",
        "                    buckets[\"dcn_off\"].append(param)\n",
        "                elif name.endswith(\".bias\") and not is_norm_name(name):\n",
        "                    buckets[\"bb_b\"].append(param)\n",
        "                elif is_norm_name(name):\n",
        "                    buckets[\"bb_norm\"].append(param)\n",
        "                else:\n",
        "                    buckets[\"bb_w\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Neck\n",
        "            if name.startswith(\"neck.\"):\n",
        "                if name.endswith(\".bias\") and not is_norm_name(name):\n",
        "                    buckets[\"hd_b\"].append(param)\n",
        "                elif is_norm_name(name):\n",
        "                    buckets[\"hd_norm\"].append(param)\n",
        "                else:\n",
        "                    buckets[\"hd_w\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Decoder (default)\n",
        "            if name.endswith(\".bias\") and not is_norm_name(name):\n",
        "                buckets[\"dec_b\"].append(param)\n",
        "            elif is_norm_name(name):\n",
        "                buckets[\"dec_norm\"].append(param)\n",
        "            else:\n",
        "                buckets[\"dec_w\"].append(param)\n",
        "\n",
        "        # --- Build parameter groups ---\n",
        "        wd = weight_decay\n",
        "        groups = []\n",
        "\n",
        "        # Backbone groups\n",
        "        if buckets[\"bb_w\"]:\n",
        "            groups.append({\"params\": buckets[\"bb_w\"], \"lr\": base_lr*bb_mult, \"weight_decay\": wd, \"name\": \"bb_w\"})\n",
        "        if buckets[\"bb_b\"]:\n",
        "            groups.append({\"params\": buckets[\"bb_b\"], \"lr\": base_lr*bb_mult*bias_mult, \"weight_decay\": 0.0, \"name\": \"bb_b\"})\n",
        "        if buckets[\"bb_norm\"]:\n",
        "            groups.append({\"params\": buckets[\"bb_norm\"], \"lr\": base_lr*bb_mult, \"weight_decay\": 0.0, \"name\": \"bb_norm\"})\n",
        "\n",
        "        # DCN offset/mask (HIGH LR!)\n",
        "        if buckets[\"dcn_off\"]:\n",
        "            groups.append({\"params\": buckets[\"dcn_off\"], \"lr\": base_lr*dcn_mult, \"weight_decay\": 0.0, \"name\": \"dcn_off\"})\n",
        "\n",
        "        # Neck/Head groups\n",
        "        if buckets[\"hd_w\"]:\n",
        "            groups.append({\"params\": buckets[\"hd_w\"], \"lr\": base_lr, \"weight_decay\": wd, \"name\": \"hd_w\"})\n",
        "        if buckets[\"hd_b\"]:\n",
        "            groups.append({\"params\": buckets[\"hd_b\"], \"lr\": base_lr*bias_mult, \"weight_decay\": 0.0, \"name\": \"hd_b\"})\n",
        "        if buckets[\"hd_norm\"]:\n",
        "            groups.append({\"params\": buckets[\"hd_norm\"], \"lr\": base_lr, \"weight_decay\": 0.0, \"name\": \"hd_norm\"})\n",
        "\n",
        "        # Decoder groups\n",
        "        if buckets[\"dec_w\"]:\n",
        "            groups.append({\"params\": buckets[\"dec_w\"], \"lr\": base_lr*dec_mult, \"weight_decay\": wd, \"name\": \"dec_w\"})\n",
        "        if buckets[\"dec_b\"]:\n",
        "            groups.append({\"params\": buckets[\"dec_b\"], \"lr\": base_lr*dec_mult*bias_mult, \"weight_decay\": 0.0, \"name\": \"dec_b\"})\n",
        "        if buckets[\"dec_norm\"]:\n",
        "            groups.append({\"params\": buckets[\"dec_norm\"], \"lr\": base_lr*dec_mult, \"weight_decay\": 0.0, \"name\": \"dec_norm\"})\n",
        "\n",
        "        # Special parameters with custom LR\n",
        "        if buckets[\"deform_off_b\"]:\n",
        "            groups.append({\"params\": buckets[\"deform_off_b\"], \"lr\": base_lr*dcn_mult, \"weight_decay\": 0.0, \"name\": \"deform_off_b\"})\n",
        "        if buckets[\"fuse_gate\"]:\n",
        "            groups.append({\"params\": buckets[\"fuse_gate\"], \"lr\": base_lr*gate_mult, \"weight_decay\": 0.0, \"name\": \"fuse_gate\"})\n",
        "        if buckets[\"rpb_p\"]:\n",
        "            groups.append({\"params\": buckets[\"rpb_p\"], \"lr\": base_lr*rpb_mult, \"weight_decay\": 0.0, \"name\": \"rpb_p\"})\n",
        "        if buckets[\"pos_embed\"]:\n",
        "            groups.append({\"params\": buckets[\"pos_embed\"], \"lr\": base_lr*pos_mult, \"weight_decay\": 0.0, \"name\": \"pos_embed\"})\n",
        "        if buckets[\"ls_gamma\"]:\n",
        "            groups.append({\"params\": buckets[\"ls_gamma\"], \"lr\": base_lr*ls_mult, \"weight_decay\": 0.0, \"name\": \"ls_gamma\"})\n",
        "        if buckets[\"dat_scale_p\"]:\n",
        "            groups.append({\"params\": buckets[\"dat_scale_p\"], \"lr\": base_lr*gate_mult, \"weight_decay\": 0.0, \"name\": \"dat_scale_p\"})\n",
        "        if buckets[\"scalars_gain\"]:\n",
        "            groups.append({\"params\": buckets[\"scalars_gain\"], \"lr\": base_lr*gate_mult, \"weight_decay\": 0.0, \"name\": \"scalars_gain\"})\n",
        "\n",
        "        # Validation\n",
        "        in_groups = {id(p) for g in groups for p in g[\"params\"]}\n",
        "        missing = [n for n,p in self.named_parameters() if p.requires_grad and id(p) not in in_groups]\n",
        "        assert not missing, f\"Missing params in groups: {len(missing)} params (e.g., {missing[:5]})\"\n",
        "\n",
        "        # Debug logging (optional)\n",
        "        if not hasattr(self, '_param_groups_logged'):\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"Parameter Groups Summary\")\n",
        "            print(\"=\"*60)\n",
        "            for g in groups:\n",
        "                n_params = sum(p.numel() for p in g[\"params\"])\n",
        "                if n_params > 0:\n",
        "                    lr_mult = g[\"lr\"] / base_lr\n",
        "                    print(f\"{g['name']:20s}: {n_params:10,} params | LR: {lr_mult:6.1f}x | WD: {g['weight_decay']:.3f}\")\n",
        "            print(\"=\"*60 + \"\\n\")\n",
        "            self._param_groups_logged = True\n",
        "\n",
        "        return groups\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# build_optimizer – returns only Optimizer (scheduler opsiyonel)\n",
        "# -----------------------------------------------------------------------------\n",
        "def build_optimizer(model: nn.Module,\n",
        "                    base_lr: float = 2e-4,\n",
        "                    weight_decay: float = 0.02,\n",
        "                    *,\n",
        "                    return_scheduler: bool = False):\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # ---------- 1) LAZY-PARAM WARM-UP (only backbone) ----------\n",
        "    was_training = model.training\n",
        "    model.train()  # PGI routes should be in train mode\n",
        "    with torch.no_grad():\n",
        "        for s in (640, 320):\n",
        "            dummy = torch.zeros(1, 3, s, s, device=device)\n",
        "            _ = model.backbone(dummy, need_aux=True)\n",
        "    model.train(was_training)\n",
        "\n",
        "    # ---------- 2) PARAM GRUPLARI ----------\n",
        "    groups = model.param_groups(base_lr)\n",
        "\n",
        "    # Norm weights WD=0 (GN/LNProxy vb.)\n",
        "    ln_keys = ('.norm', '.ln_act', 'ln_proxy')\n",
        "    #  Note: This check is heuristic by name; it can be customized as needed.\n",
        "    for g in groups:\n",
        "        params_in_group = set(map(id, g['params']))\n",
        "        if any(any(k in n for k in ln_keys)\n",
        "               for n, p in model.named_parameters() if id(p) in params_in_group):\n",
        "            g[\"weight_decay\"] = 0.0\n",
        "\n",
        "    # ---------- 3) OPTIMIZER ----------\n",
        "    optim = torch.optim.AdamW(\n",
        "        groups, lr=base_lr,\n",
        "        betas=(0.9, 0.999), eps=1e-6,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    if not return_scheduler:\n",
        "        return optim\n",
        "\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optim, T_max=100, eta_min=base_lr * 0.05)\n",
        "    return optim, sched\n",
        "\n",
        "\n",
        "def build_model(num_classes: int = 20,\n",
        "                norm: str = \"gn\",            # \"gn\", \"lnp\" veya \"bn\"\n",
        "                use_layer_scale: bool = True,\n",
        "                layer_scale_init: float = 1.0,\n",
        "                **kwargs) -> HybridDCDATRT:\n",
        "    \"\"\"\n",
        "    High-level constructor. The most commonly used settings come with defaults.\n",
        "    HybridDCDATRT __init__ parameters (e.g., dec_layers, dn_queries,\n",
        "    decoder_use_encoder, etc.) can be overridden via `kwargs`.\n",
        "    \"\"\"\n",
        "    defaults = dict(\n",
        "        # Backbone\n",
        "        depths         = (2, 2, 4, 1),\n",
        "        drop_path_max  = 0.2,\n",
        "        # Decoder\n",
        "        d_model        = 320,\n",
        "        dec_layers     = 4,\n",
        "        dn_queries     = 100,\n",
        "        num_queries    = 200,\n",
        "        use_aux_loss   = True,\n",
        "        use_iou_aware  = False,\n",
        "        # Dataset\n",
        "        voc_prior      = False,\n",
        "\n",
        "        decoder_use_encoder           = True,\n",
        "        decoder_encoder_layers        = 2,\n",
        "        decoder_encoder_drop_path_max = 0.1,\n",
        "    )\n",
        "    defaults.update(kwargs)\n",
        "\n",
        "    nf = NormFactory(norm)\n",
        "    model = HybridDCDATRT(\n",
        "        num_classes           = num_classes,\n",
        "        backbone_norm_factory = nf,\n",
        "        neck_norm_factory     = nf,\n",
        "        use_layer_scale       = use_layer_scale,\n",
        "        layer_scale_init      = layer_scale_init,\n",
        "        **defaults\n",
        "    )\n",
        "    # init_weights_improved(model)\n",
        "    # apply_hybrid_fixup(model)\n",
        "    # boost_relative_position_bias(model, std=0.05)\n",
        "    return model\n",
        "# -----------------------------------------------------------------------------\n",
        "# Test dummy code\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     import time\n",
        "\n",
        "#     assert torch.cuda.is_available(), \"CUDA required for this model.\"\n",
        "#     device = torch.device(\"cuda\")\n",
        "\n",
        "#     print(\"=\"*80)\n",
        "#     print(\"Building Hybrid-DCDAT-RT Model...\")\n",
        "#     print(\"=\"*80)\n",
        "\n",
        "#     model = build_model(\n",
        "#         num_classes=80,\n",
        "#         depths=[2, 2, 2, 2],  # ResNet-50 like depth\n",
        "#         drop_path_max=0,\n",
        "#         num_queries=200,\n",
        "#         voc_prior=False,norm=\"gn\"  # Use COCO prior\n",
        "#     ).to(device)\n",
        "#     optimizer = build_optimizer(model, base_lr=2e-4)\n",
        "#     # Test forward pass\n",
        "#     print(\"\\n🚀 Testing forward pass...\")\n",
        "#     model.eval()\n",
        "#     dummy = torch.randn(2, 3, 640, 640, device=device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         out = model(dummy)\n",
        "#         print(\"✅ Forward pass successful!\")\n",
        "#         print(f\"Output keys: {list(out.keys())}\")\n",
        "#         print(f\"Predictions shape: {out['pred_logits'].shape}, {out['pred_boxes'].shape}\")\n",
        "\n",
        "#     # Test training mode with targets\n",
        "#     print(\"\\n🚀 Testing training mode...\")\n",
        "#     model.train()\n",
        "\n",
        "#     # Create more realistic targets with proper box format\n",
        "#     targets = []\n",
        "#     for _ in range(2):  # batch size 2\n",
        "#         num_objs = torch.randint(1, 10, (1,)).item()\n",
        "#         # Generate boxes in (cx, cy, w, h) format, all normalized to [0, 1]\n",
        "#         centers = torch.rand(num_objs, 2, device=device)\n",
        "#         sizes = torch.rand(num_objs, 2, device=device) * 0.5  # max size 0.5\n",
        "#         boxes = torch.cat([centers, sizes], dim=1)\n",
        "\n",
        "#         # Ensure boxes are valid (centers must be at least half-size from borders)\n",
        "#         half_sizes = boxes[:, 2:4] / 2\n",
        "#         boxes[:, 0] = boxes[:, 0].clamp(min=half_sizes[:, 0], max=1-half_sizes[:, 0])\n",
        "#         boxes[:, 1] = boxes[:, 1].clamp(min=half_sizes[:, 1], max=1-half_sizes[:, 1])\n",
        "\n",
        "#         targets.append({\n",
        "#             'boxes': boxes,\n",
        "#             'labels': torch.randint(0, 80, (num_objs,), device=device)\n",
        "#         })\n",
        "\n",
        "#     out = model(dummy, targets)\n",
        "#     print(\"✅ Training mode successful!\")\n",
        "#     print(f\"Output keys: {list(out.keys())}\")\n",
        "#     if 'aux_dense' in out:\n",
        "#         print(f\"Auxiliary dense outputs: {list(out['aux_dense'].keys())}\")\n",
        "#     if 'dn_meta' in out:\n",
        "#         print(f\"Denoising queries: {out['dn_meta']['dn_queries']}\")\n",
        "\n",
        "#     # Test gradient flow\n",
        "#     print(\"\\n🚀 Testing gradient flow...\")\n",
        "#     loss = out['pred_logits'].sum() + out['pred_boxes'].sum()\n",
        "#     loss.backward()\n",
        "\n",
        "#     # Check that gradients flow through all parts\n",
        "#     has_grad = {\n",
        "#         'backbone': any(p.grad is not None for n, p in model.named_parameters() if 'backbone' in n),\n",
        "#         'neck': any(p.grad is not None for n, p in model.named_parameters() if 'neck' in n),\n",
        "#         'decoder': any(p.grad is not None for n, p in model.named_parameters() if 'decoder' in n),\n",
        "#     }\n",
        "\n",
        "#     for module, has in has_grad.items():\n",
        "#         print(f\"  {module}: {'✅' if has else '❌'} gradients\")\n",
        "\n",
        "#     # Test parameter groups\n",
        "#     print(\"\\n🚀 Testing parameter groups...\")\n",
        "#     param_groups = model.param_groups(base_lr=2e-4)\n",
        "#     print(f\"Number of parameter groups: {len(param_groups)}\")\n",
        "#     for i, group in enumerate(param_groups):\n",
        "#         print(f\"  Group {i}: {len(group['params'])} params, lr={group['lr']:.6f}, wd={group['weight_decay']}\")\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*80)\n",
        "#     print(\"✅ All tests passed! Model is ready.\")\n",
        "#     print(\"=\"*80)"
      ],
      "metadata": {
        "id": "ZBiuE7QdK_Na"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}