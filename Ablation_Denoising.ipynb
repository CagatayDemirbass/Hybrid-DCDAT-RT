{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka6x7aHr80nx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM2R6llDUsWI"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "\n",
        "!pip install torch==2.3.1+cu121 torchvision --extra-index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjreK-ElUwHI"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q  pycocotools tqdm torchmetrics lxml scipy\n",
        "!pip install -U \"datasets>=2.17.0\"  \"pyarrow>=14.0\"\n",
        "!pip install -U cmake ninja wheel\n",
        "!pip install --no-binary=mmcv mmcv==2.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LleuWwc3U6Oo"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y numpy\n",
        "\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb03u1K_U7rp"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install -U cmake ninja wheel\n",
        "\n",
        "git clone --depth 1 --branch v0.14.6 https://github.com/SHI-Labs/NATTEN.git\n",
        "cd NATTEN\n",
        "\n",
        "export FORCE_CUDA=1\n",
        "export TORCH_CUDA_ARCH_LIST=\"8.0\"\n",
        "\n",
        "pip install .\n",
        "\n",
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJuogQ06U-x4"
      },
      "outputs": [],
      "source": [
        "!rm -rf DAT && git clone -q https://github.com/LeapLabTHU/DAT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRIt-13XU_9-"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "git clone --depth 1 https://github.com/OpenGVLab/DCNv4.git\n",
        "cd DCNv4/DCNv4_op\n",
        "export FORCE_CUDA=1\n",
        "export TORCH_CUDA_ARCH_LIST=\"8.0\"\n",
        "python -m pip install . --no-build-isolation -v\n",
        "cd ../..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWfwrVR7eoxS"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!pip -q install opendatasets\n",
        "import opendatasets as od\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/vijayabhaskar96/pascal-voc-2007-and-2012\",\n",
        "    data_dir=\"/content/voc\")          # → /content/voc/VOCdevkit/{VOC2007,VOC2012}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLeZy5E6e2ZP"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import VOCDetection\n",
        "from torch.utils.data import ConcatDataset\n",
        "ROOT = \"/content/voc/pascal-voc-2007-and-2012\"\n",
        "train07 = VOCDetection(ROOT, \"2007\", \"trainval\")   # 5011\n",
        "train12 = VOCDetection(ROOT, \"2012\", \"trainval\")   # 11540\n",
        "combined = ConcatDataset([train07, train12])       # 16551\n",
        "test07   = VOCDetection(ROOT, \"2007\", \"test\")      # 4952x\n",
        "\n",
        "print(len(train07), len(train12), len(combined), len(test07))\n",
        "# 5011 11540 16551 4952"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRFBa9e7mKqx",
        "outputId": "cdd6fd87-643d-4847-c868-c599dfeb0c10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Import from actual DAT and DCNv4 modules\n",
        "import importlib\n",
        "from DAT.models.dat import DAT, LayerScale, TransformerStage\n",
        "from DAT.models.dat_blocks import LayerNormProxy, TransformerMLP, TransformerMLPWithConv\n",
        "from DAT.models.dat_blocks import LocalAttention, DAttentionBaseline, ShiftWindowAttention, PyramidAttention\n",
        "from DCNv4.modules.dcnv4 import DCNv4\n",
        "dat_mod = importlib.import_module(\"DAT.models.dat\")\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from __future__ import annotations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.layers import DropPath\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from mmcv.ops.multi_scale_deform_attn import MultiScaleDeformableAttention\n",
        "from timm.models.layers import create_act_layer\n",
        "from typing import Optional, List, Dict, Tuple,  Union\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NormFactory:\n",
        "    SUPPORTED = {\"bn\", \"gn\", \"lnp\"}\n",
        "\n",
        "    def __init__(self, kind: str = \"gn\", gn_groups: int = 32):\n",
        "        kind = kind.lower()\n",
        "        if kind not in self.SUPPORTED:\n",
        "            raise ValueError(f\"kind must be one of {self.SUPPORTED}\")\n",
        "        self.kind = kind\n",
        "        self.gn_groups = gn_groups\n",
        "\n",
        "    def __call__(self, num_feat: int) -> nn.Module:\n",
        "        if self.kind == \"bn\":\n",
        "            return nn.BatchNorm2d(num_feat)\n",
        "        if self.kind == \"gn\":\n",
        "            g = math.gcd(self.gn_groups, num_feat) or 1\n",
        "            return nn.GroupNorm(g, num_feat)\n",
        "\n",
        "        if self.kind == \"lnp\":\n",
        "            return LayerNormProxy(num_feat)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. CrossScaleInjection\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class CrossScaleInjection(nn.Module):\n",
        "    \"\"\"Cross‑scale feature enjeksiyonu – kanal‑başına gating versiyonu\"\"\"\n",
        "    def __init__(self, low_ch: int, high_ch: int):\n",
        "        super().__init__()\n",
        "        self.align = nn.Conv2d(low_ch, high_ch, 1, bias=False)\n",
        "        self.norm = LayerNormProxy(high_ch)\n",
        "\n",
        "        # Channel based weight; starting 0.1\n",
        "        self.weight = nn.Parameter(torch.ones(1, high_ch, 1, 1) * 0.15)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.align.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, low_res: torch.Tensor, high_res: torch.Tensor) -> torch.Tensor:\n",
        "        low_aligned = self.align(low_res)\n",
        "        low_up = F.interpolate(low_aligned, size=high_res.shape[-2:],\n",
        "                              mode='bilinear', align_corners=False)\n",
        "        low_up = self.norm(low_up)\n",
        "        # sigmoid → [0,1] for per channel λ\n",
        "        return high_res + torch.sigmoid(self.weight) * low_up\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PGI– Programmable Gradient Injection\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "class PGIModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Programmable Gradient Injection v2 – DDP‑safe\n",
        "    * aux_channels verilirse init'te 1x1 'aux_adapter' kurulur (statik, DDP‑safe).\n",
        "    * Aux katkısı sadece training modunda eklenir.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        reduction: int = 4,\n",
        "        lambda_bounds: tuple[float, float] = (0.2, 0.7),\n",
        "        init_lambda: float = 0.5,\n",
        "        use_bn: bool = True,\n",
        "        drop_path: float = 0.0,\n",
        "        norm_factory: NormFactory = NormFactory(\"gn\"),\n",
        "        aux_channels: int | None = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.lambda_min, self.lambda_max = lambda_bounds\n",
        "\n",
        "        # main branch\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, 3, 1, 1,\n",
        "                      groups=max(1, channels // 4), bias=False),\n",
        "            norm_factory(channels) if use_bn else nn.Identity(),\n",
        "        )\n",
        "\n",
        "        mid = max(channels // reduction, 32)\n",
        "        self.aux = nn.Sequential(\n",
        "            nn.Conv2d(channels, mid, 1, bias=False),\n",
        "            norm_factory(mid) if use_bn else nn.Identity(),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Conv2d(mid, channels, 1, bias=False),\n",
        "            norm_factory(channels) if use_bn else nn.Identity(),\n",
        "        )\n",
        "\n",
        "        # Aux channel adapter (static)\n",
        "        if aux_channels is None or aux_channels == channels:\n",
        "            self.aux_adapter = nn.Identity()\n",
        "        else:\n",
        "            self.aux_adapter = nn.Conv2d(aux_channels, channels, 1, bias=False)\n",
        "            nn.init.kaiming_normal_(self.aux_adapter.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "        # λ (learnable)\n",
        "        import math\n",
        "        init_logit = math.log((init_lambda - self.lambda_min) / (self.lambda_max - init_lambda))\n",
        "        self._lambda_logit = nn.Parameter(torch.tensor(init_logit))\n",
        "\n",
        "        # Gate start 0.6\n",
        "        self.gate = nn.Parameter(torch.ones(1, channels, 1, 1) * 0.5)\n",
        "\n",
        "        self.dp = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "        self._init_weights()\n",
        "\n",
        "    @property\n",
        "    def lambda_val(self) -> torch.Tensor:\n",
        "        σ = torch.sigmoid(self._lambda_logit)\n",
        "        return self.lambda_min + (self.lambda_max - self.lambda_min) * σ\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                if hasattr(m, 'weight'): nn.init.ones_(m.weight)\n",
        "                if hasattr(m, 'bias'):   nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _align_spatial(self, feat: torch.Tensor, target_hw: tuple[int, int]) -> torch.Tensor:\n",
        "        if feat.shape[-2:] != target_hw:\n",
        "            feat = F.interpolate(feat, size=target_hw, mode='bilinear', align_corners=False)\n",
        "        return feat\n",
        "\n",
        "    def forward(self, x: torch.Tensor, aux_input: torch.Tensor | None = None):\n",
        "        main_out = self.main(x)\n",
        "\n",
        "        if aux_input is not None and self.training:\n",
        "            aux = self.aux_adapter(aux_input)\n",
        "            aux = self._align_spatial(aux, x.shape[-2:])\n",
        "            aux = self.aux(aux) * self.gate\n",
        "            main_out = main_out + self.lambda_val * aux\n",
        "\n",
        "        return x + self.dp(main_out)\n",
        "\n",
        "def get_drop_path_rates(num_layers: int, max_rate: float) -> List[float]:\n",
        "    \"\"\"Generate drop path rates with cosine scheduling\n",
        "\n",
        "    Args:\n",
        "        num_layers: Total number of layers\n",
        "        max_rate: Maximum drop path rate\n",
        "\n",
        "    Returns:\n",
        "        List of drop path rates for each layer\n",
        "    \"\"\"\n",
        "    if num_layers <= 1:\n",
        "        return [0.0] * num_layers\n",
        "\n",
        "    # Cosine scheduling for smooth progression\n",
        "    rates = [\n",
        "        max_rate * (1.0 - math.cos(math.pi * i / (num_layers - 1))) * 0.5\n",
        "        for i in range(num_layers)\n",
        "    ]\n",
        "\n",
        "    return rates\n",
        "\n",
        "\n",
        "def init_weights_improved(model: nn.Module):\n",
        "    \"\"\"Improved weight initialization for all module types\"\"\"\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            # Special handling for different conv types\n",
        "            if 'dw' in name or module.groups > 1:  # Depthwise\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='linear')\n",
        "            elif 'pw' in name or module.kernel_size == (1, 1):  # Pointwise\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "            else:  # Regular conv\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.BatchNorm2d):\n",
        "            nn.init.ones_(module.weight)\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, LayerNormProxy):\n",
        "            if hasattr(module, 'weight'):\n",
        "                nn.init.ones_(module.weight)\n",
        "            if hasattr(module, 'bias'):\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            nn.init.trunc_normal_(module.weight, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, std=0.02)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Channel-wise Attention (SE)\n",
        "# -----------------------------------------------------------------------------\n",
        "class ChannelAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, channels: int, reduction: int = 8):\n",
        "        super().__init__()\n",
        "        mid = max(channels // reduction, 16)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(channels, mid, 1, bias=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Conv2d(mid, channels, 1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # --- parameters\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_out', nonlinearity='sigmoid')\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "        # ►–– init scale ≈\n",
        "        self.residual_scale = nn.Parameter(torch.tensor(-2.0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        y = self.avgpool(x)\n",
        "        y = self.relu(self.fc1(y))\n",
        "        y = self.sigmoid(self.fc2(y))\n",
        "\n",
        "        scale = torch.sigmoid(self.residual_scale)   # ∈(0,1)\n",
        "        return x * (1 - scale) + x * y * scale\n",
        "\n",
        "\n",
        "# 5. Gradient-clip helpers\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def clip_dcnv4_grads(model: nn.Module, max_norm: float = 0.2) -> None:\n",
        "    \"\"\"Clip gradients for DCNv4 offset/mask parameters - more aggressive\"\"\"\n",
        "    target_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is None:\n",
        "            continue\n",
        "        n_low = name.lower()\n",
        "        if \"offset\" in n_low or \"mask\" in n_low:\n",
        "            target_params.append(param)\n",
        "\n",
        "    if target_params:\n",
        "        torch.nn.utils.clip_grad_norm_(target_params, max_norm=max_norm)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Helper classes\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def fix_zero_init_params(model):\n",
        "    \"\"\"Fix parameters that are initialized to zero\"\"\"\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.dim() > 0 and torch.all(param == 0):\n",
        "            print(f\"Fixing zero-initialized parameter: {name}\")\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.01)\n",
        "            else:\n",
        "                nn.init.normal_(param, std=0.01)\n",
        "\n",
        "def apply_gradient_checkpointing(model):\n",
        "    \"\"\"Apply gradient checkpointing to backbone stages - IMPROVED\"\"\"\n",
        "    if hasattr(model, 'backbone'):\n",
        "        # Apply a checkpoint for each stage\n",
        "        for stage_name in ['st0', 'st1', 'st2', 'st3']:\n",
        "            if hasattr(model.backbone, stage_name):\n",
        "                stage = getattr(model.backbone, stage_name)\n",
        "                # Make the sequential module checkpoint-friendly\n",
        "                class CheckpointedSequential(nn.Module):\n",
        "                    def __init__(self, *layers):\n",
        "                        super().__init__()\n",
        "                        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "                    def forward(self, x):\n",
        "                        for layer in self.layers:\n",
        "                            x = checkpoint(layer, x, use_reentrant=False)\n",
        "                        return x\n",
        "\n",
        "                # Recreate Stage\n",
        "                checkpointed = CheckpointedSequential(*[block for block in stage])\n",
        "                setattr(model.backbone, stage_name, checkpointed)\n",
        "\n",
        "def init_conv(layer: nn.Conv2d, fan_out: bool = True):\n",
        "    \"\"\"Kaiming normal initialization\"\"\"\n",
        "    nn.init.kaiming_normal_(\n",
        "        layer.weight,\n",
        "        mode=\"fan_out\" if fan_out else \"fan_in\",\n",
        "        nonlinearity=\"relu\",\n",
        "    )\n",
        "    if layer.bias is not None:\n",
        "        nn.init.zeros_(layer.bias)\n",
        "\n",
        "\n",
        "class LNAct(nn.Module):\n",
        "    \"\"\"LayerNormProxy + SiLU\"\"\"\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNormProxy(channels)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.norm(x))\n",
        "\n",
        "\n",
        "class ConvLNAct(nn.Sequential):\n",
        "    \"\"\"Conv + LayerNorm + Act fusion\"\"\"\n",
        "    def __init__(self, in_ch: int, out_ch: int, k: int = 3,\n",
        "                s: int = 1, p: int = None, g: int = 1):\n",
        "        p = p if p is not None else k // 2\n",
        "        super().__init__(\n",
        "            nn.Conv2d(in_ch, out_ch, k, s, p, groups=g, bias=False),\n",
        "            LNAct(out_ch)\n",
        "        )\n",
        "        nn.init.kaiming_normal_(self[0].weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# LayerScale\n",
        "# -----------------------------------------------------------------------------\n",
        "class LayerScale(nn.Module):\n",
        "    \"\"\"\n",
        "    Learnable scalar per layer (γ) – Broadcast-Safe version.\n",
        "    Works with both (B,C,H,W) and (B,N,C) tensor layouts.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        inplace: bool = False,\n",
        "        init_values: float = 1.0,\n",
        "        depth: int | None = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.inplace = inplace\n",
        "        if depth is not None:\n",
        "            init_values = min(1.0, 0.5 * depth / 3)\n",
        "        self.weight = nn.Parameter(torch.ones(dim) * init_values)\n",
        "\n",
        "    def _shape_for(self, x: torch.Tensor) -> tuple[int, ...]:\n",
        "        \"\"\"\n",
        "        Determines the shape to broadcast the weight based on x's placement.\n",
        "        Priority: channel-end (… , C) → channel-first (B, C, …) → singular cases.\n",
        "        \"\"\"\n",
        "        C = self.weight.numel()\n",
        "        nd = x.dim()\n",
        "\n",
        "        # 4D  image: (B, C, H, W) veya (B, H, W, C)\n",
        "        if nd == 4:\n",
        "            if x.shape[1] == C:          # (B, C, H, W)\n",
        "                return (1, C, 1, 1)\n",
        "            if x.shape[-1] == C:         # (B, H, W, C) - nadir\n",
        "                return (1, 1, 1, C)\n",
        "\n",
        "        # 3D: (B, N, C) or (B, C, N) or (C, H, W)\n",
        "        if nd == 3:\n",
        "            if x.shape[-1] == C:         # (B, N, C)\n",
        "                return (1, 1, C)\n",
        "            if x.shape[1] == C:          # (B, C, N)\n",
        "                return (1, C, 1)\n",
        "            if x.shape[0] == C:          # (C, H, W) / (C, N, ?)such as extreme cases\n",
        "                return (C, 1, 1)\n",
        "\n",
        "        # 2D: (B, C) veya (C, B)\n",
        "        if nd == 2:\n",
        "            if x.shape[-1] == C:         # (B, C)\n",
        "                return (1, C)\n",
        "            if x.shape[0] == C:          # (C, B)\n",
        "                return (C, 1)\n",
        "\n",
        "\n",
        "        shape = [1] * max(1, nd)\n",
        "        shape[-1] = C\n",
        "        return tuple(shape)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        w = self.weight.view(*self._shape_for(x))\n",
        "        if self.inplace:\n",
        "            return x.mul_(w)\n",
        "        return x * w\n",
        "\n",
        "# Synchronize the LayerScale within DAT with this version (old behavior is preserved)\n",
        "dat_mod.LayerScale = LayerScale\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Stem\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class Stem(nn.Module):\n",
        "    \"\"\"Three 3×3 convolutions with stride pattern (2,1,2)\"\"\"\n",
        "    def __init__(self, out_channels: int = 128):\n",
        "        super().__init__()\n",
        "        c_mid = out_channels // 2\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, c_mid, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(c_mid, c_mid, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(c_mid, out_channels, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "        self.norm = LayerNormProxy(out_channels)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in [self.conv1, self.conv2, self.conv3]:\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.act(self.conv1(x))\n",
        "        x = self.act(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# DCNv4Lite\n",
        "# -----------------------------------------------------------------------------\n",
        "## FULL DCNV4\n",
        "\n",
        "\n",
        "class DCNv4Lite(nn.Module):\n",
        "    \"\"\"\n",
        "    • Fully wraps the DCNv4 path (2D→3D→2D), NO baseline conv/residual mix.\n",
        "    • Offset/mask parameters are initialized with small std (for stable fp32 training).\n",
        "    • Silently discards ‘mix_*’ and ‘conv.*’ keys in old checkpoints.\n",
        "\n",
        "    Args:\n",
        "        channels (int)            : Num of channels\n",
        "        group (int)               : DCNv4 group\n",
        "        kernel_size (int)         : Kernel\n",
        "        stride (int)              : Stride\n",
        "        pad (int)                 : Padding\n",
        "        dilation (int)            : Dilation\n",
        "        offset_scale (float)      : DCNv4 offset scale\n",
        "        without_pointwise (bool)  : Enable/disable the internal 1x1 project in DCNv4 (default: True)\n",
        "        init_offset_mask_std (float): offset/mask start std (default: 0.01)\n",
        "        **kwargs                  : It is forwarded to DCNv4 exactly as is.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 channels: int,\n",
        "                 group: int = 8,\n",
        "                 kernel_size: int = 3,\n",
        "                 stride: int = 1,\n",
        "                 pad: int = 1,\n",
        "                 dilation: int = 1,\n",
        "                 offset_scale: float = 0.1,\n",
        "                 *,\n",
        "                 without_pointwise: bool = True,\n",
        "                 init_offset_mask_std: float = 0.01,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        self.channels = int(channels)\n",
        "\n",
        "        #  DCNv4\n",
        "        self.dcn = DCNv4(\n",
        "            channels=channels, kernel_size=kernel_size, stride=stride, pad=pad,\n",
        "            dilation=dilation, group=group, offset_scale=offset_scale,\n",
        "            center_feature_scale=False, remove_center=False,\n",
        "            output_bias=True, without_pointwise=without_pointwise, **kwargs\n",
        "        )\n",
        "\n",
        "        # Offset/Mask secure init\n",
        "        self._init_dcn_params(std=init_offset_mask_std)\n",
        "\n",
        "        # Save geometry for representation (debug)\n",
        "        self._geom = dict(k=kernel_size, s=stride, p=pad, d=dilation, g=group,\n",
        "                          wop=without_pointwise)\n",
        "\n",
        "    # ---------------- init helpers ----------------\n",
        "    def _init_dcn_params(self, std: float = 0.01):\n",
        "        \"\"\"\n",
        "        Offset/mask parameters are initialized with small normals so that\n",
        "        sigmoid(0)=0.5 exits the plateau lock; other parameters remain in DCNv4's\n",
        "        default init.\n",
        "        \"\"\"\n",
        "        std = float(max(std, 1e-5))\n",
        "        for n, p in self.dcn.named_parameters():\n",
        "            n_low = n.lower()\n",
        "            if ('offset' in n_low) or ('mask' in n_low):\n",
        "                nn.init.normal_(p, std=std)\n",
        "\n",
        "    # ---------------- forward ----------------------\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, C, H, W) → y: (B, C, H, W)\n",
        "        The DCNv4 interface (B, HW, C) requires a 2D→3D→2D conversion.\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        x3d = x.permute(0, 2, 3, 1).contiguous().view(B, H * W, C)   # (B, HW, C)\n",
        "        y3d = self.dcn(x3d, shape=(H, W))                            # (B, HW, C)\n",
        "        y   = y3d.view(B, H, W, C).permute(0, 3, 1, 2).contiguous()  # (B, C, H, W)\n",
        "        return y\n",
        "\n",
        "    # --------------- helpers -------------------\n",
        "    @torch.no_grad()\n",
        "    def freeze_offsets(self, flag: bool = True):\n",
        "        \"\"\"Freeze/unfreeze offset/mask parameters (e.g., for warming).\"\"\"\n",
        "        for n, p in self.dcn.named_parameters():\n",
        "            n_low = n.lower()\n",
        "            if ('offset' in n_low) or ('mask' in n_low):\n",
        "                p.requires_grad = (not flag)\n",
        "\n",
        "    def offset_mask_parameters(self):\n",
        "        \"\"\"In the Optimizer, it yields the offset/mask parameters to provide separate LR/clip.\"\"\"\n",
        "        for n, p in self.dcn.named_parameters():\n",
        "            n_low = n.lower()\n",
        "            if ('offset' in n_low) or ('mask' in n_low):\n",
        "                yield p\n",
        "\n",
        "    # Ignore the mix/baseline keys remaining in old checkpoints\n",
        "    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):\n",
        "        obsolete = (\n",
        "            \"mix_weight\", \"mix_logit\", \"_mix_tau\", \"_mix_eps\", \"_mix_clamp\",\n",
        "            \"conv.weight\", \"conv.bias\"\n",
        "        )\n",
        "        for key in list(state_dict.keys()):\n",
        "            if key.startswith(prefix) and any(key.endswith(obs) for obs in obsolete):\n",
        "                state_dict.pop(key)\n",
        "        return super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        g = self._geom\n",
        "        return (f\"channels={self.channels}, k={g['k']}, s={g['s']}, p={g['p']}, \"\n",
        "                f\"d={g['d']}, group={g['g']}, without_pointwise={g['wop']}\")\n",
        "\n",
        "\n",
        "def boost_relative_position_bias(model: nn.Module, std: float = 0.02):\n",
        "    \"\"\"\n",
        "    Resets all `relative_position_bias_table` parameters in all ShiftWindow/LocalAttention modules belonging to the model.\n",
        "    \"\"\"\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"relative_position_bias_table\"):\n",
        "            nn.init.trunc_normal_(m.relative_position_bias_table, std=std)\n",
        "\n",
        "def init_decoder_sampling_offsets(decoder: nn.Module, bias_val: float = 0.1):\n",
        "    \"\"\"\n",
        "    Set the sampling_offsets bias in the deformable decoder layers\n",
        "    to a fixed value; a small shift triggers the gradient.\n",
        "    \"\"\"\n",
        "    for n, p in decoder.named_parameters():\n",
        "        if \"sampling_offsets.bias\" in n:\n",
        "            nn.init.constant_(p, bias_val)\n",
        "\n",
        "\n",
        "\n",
        "def apply_hybrid_fixup(model: nn.Module):\n",
        "    \"\"\"\n",
        "    a) Relative-pos bias table\n",
        "    b) Decoder sampling_offsets bias\n",
        "    c) (Optional) DCNv4 offset/mask gradient LR boost\n",
        "    \"\"\"\n",
        "    boost_relative_position_bias(model)\n",
        "    if hasattr(model, \"decoder\"):\n",
        "        init_decoder_sampling_offsets(model.decoder, 0.1)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Stage-0 Block\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class StageZeroBlock(nn.Module):\n",
        "    def __init__(self, channels=128, drop_path_rate=0.0,\n",
        "                 use_layer_scale=True, layer_scale_init=1.0,\n",
        "                 norm_factory=NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.conv3 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
        "        nn.init.kaiming_normal_(self.conv3.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "        self.se   = ChannelAttention(channels, reduction=8)\n",
        "        self.norm = norm_factory(channels)\n",
        "        self.scale = LayerScale(channels, init_values=layer_scale_init) if use_layer_scale else nn.Identity()\n",
        "        self.dp = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        out = self.conv3(x)\n",
        "        out = self.se(out)\n",
        "        out = self.norm(out)\n",
        "        out = self.scale(out)\n",
        "        out = self.dp(out)\n",
        "        return res + out\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Stage Blocks\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class StageOneBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    DCNv4Lite + SE + (optional) LayerScale + DropPath\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                channels: int = 128,\n",
        "                drop_path: float = 0.0,\n",
        "                use_layer_scale: bool = True,\n",
        "                layer_scale_init: float = 1.0,\n",
        "                norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.dcn = DCNv4Lite(channels, group=8)\n",
        "        self.se  = ChannelAttention(channels, reduction=8)\n",
        "        self.post_norm = norm_factory(channels)\n",
        "\n",
        "        self.scale = (LayerScale(channels, init_values=layer_scale_init)\n",
        "                      if use_layer_scale else nn.Identity())\n",
        "\n",
        "        self.dp = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        out = self.dcn(x)\n",
        "        out = self.se(out)\n",
        "        out = self.post_norm(out)\n",
        "        out = self.scale(out)\n",
        "        out = self.dp(out)\n",
        "        return res + out\n",
        "\n",
        "\n",
        "class DownABlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Depthwise‑Conv (s=2)  +  Pointwise‑Conv  +  Norm + Act\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                in_ch: int = 128,\n",
        "                out_ch: int = 256,\n",
        "                norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1) Depth‑wise strided conv\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1,\n",
        "                            groups=in_ch, bias=False)\n",
        "\n",
        "        # 2) Point‑wise projection\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "\n",
        "        # 3) Normalization + Activation\n",
        "        #    a) If the norm type is LayerNormProxy, use LNAct\n",
        "        #    b) In other cases, use norm + SiLU\n",
        "        nf_layer = norm_factory(out_ch)\n",
        "        if isinstance(nf_layer, LayerNormProxy):\n",
        "            # LNAct = LayerNormProxy + SiLU\n",
        "            from DAT.models.dat_blocks import LNAct as _LNAct\n",
        "            self.norm_act = _LNAct(out_ch)\n",
        "        else:\n",
        "            self.norm_act = nn.Sequential(nf_layer, nn.SiLU(inplace=True))\n",
        "\n",
        "        # --- Kaiming init ---\n",
        "        nn.init.kaiming_normal_(self.dw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.pw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.dw(x)\n",
        "        x = self.pw(x)\n",
        "        return self.norm_act(x)\n",
        "\n",
        "\n",
        "class StageTwoBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Stage‑2: Only DCNv4Lite path (no DAT, no GateFuse).\n",
        "    CSP split is kept for comparable FLOPs/latency.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 channels: int = 256,\n",
        "                 dcn_group: int = 16,\n",
        "                 drop_path: float = 0.1,\n",
        "                 norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        assert channels % 2 == 0, \"StageTwoBlock expects even channel count.\"\n",
        "        self.split_channels = channels // 2\n",
        "\n",
        "        # --- DCNv4 branch (only this one active) ---\n",
        "        self.dcn = DCNv4Lite(self.split_channels, group=max(1, dcn_group // 2))\n",
        "        self.post_norm = norm_factory(self.split_channels)\n",
        "        self.post_act  = nn.SiLU(inplace=True)\n",
        "\n",
        "        # --- Skip proj (for grad out) ---\n",
        "        self.skip_proj = nn.Sequential(\n",
        "            nn.Conv2d(self.split_channels, self.split_channels, 1, bias=False),\n",
        "            norm_factory(self.split_channels)\n",
        "        )\n",
        "\n",
        "        # --- Output merging ---\n",
        "        self.fusion = nn.Conv2d(channels, channels, 1, bias=False)\n",
        "        self.ln     = norm_factory(channels)\n",
        "        self.act    = nn.SiLU(inplace=True)\n",
        "        self.dp     = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "        # Init\n",
        "        nn.init.kaiming_normal_(self.fusion.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.skip_proj[0].weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # CSP split\n",
        "        x1, x2 = torch.split(x, self.split_channels, dim=1)\n",
        "\n",
        "        # Skip branch\n",
        "        x1 = self.skip_proj(x1)\n",
        "\n",
        "        # DCNv4 branch\n",
        "        x2 = self.dcn(x2)\n",
        "        x2 = self.post_act(self.post_norm(x2))\n",
        "\n",
        "        # Fuse & residual\n",
        "        out = torch.cat([x1, x2], dim=1)\n",
        "        out = self.fusion(out)\n",
        "        out = self.act(self.ln(out))\n",
        "        return x + self.dp(out)\n",
        "\n",
        "class DownBBlock(nn.Module):\n",
        "    \"\"\"DW 3×3 s2 → PW 1×1, 256c → 640c\"\"\"\n",
        "    def __init__(self, in_ch: int = 256, out_ch: int = 640,norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1,\n",
        "                          groups=in_ch, bias=False)\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.norm = norm_factory(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "        for m in [self.dw, self.pw]:\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.act(self.norm(self.pw(self.dw(x))))\n",
        "\n",
        "\n",
        "class StageThreeBlock(nn.Module):\n",
        "    \"\"\"Double DATHubLite + ChannelAttention\"\"\"\n",
        "    def __init__(self, in_ch: int = 640, out_ch: int = 640, drop_path: float = 0.15,norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.norm = norm_factory(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "        heads = max(4, out_ch // 64)\n",
        "        self.dat1 = DATBlock(out_ch, heads=heads)\n",
        "        self.dat2 = DATBlock(out_ch, heads=heads)\n",
        "        self.dp1 = DropPath(drop_path)\n",
        "        self.dp2 = DropPath(drop_path)\n",
        "        self.ca = ChannelAttention(out_ch, reduction=8)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.proj.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.act(self.norm(self.proj(x)))\n",
        "        res = x\n",
        "        x = self.dat1(x)\n",
        "        x = self.dp1(x) + res\n",
        "        res = x\n",
        "        x = self.dat2(x)\n",
        "        x = self.dp2(x) + res\n",
        "        x = self.ca(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DownCBlock(nn.Module):\n",
        "    \"\"\"DW 3×3 s2 → PW 1×1, 640c → 768c\"\"\"\n",
        "    def __init__(self, in_ch: int = 640, out_ch: int = 768,norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1,\n",
        "                          groups=in_ch, bias=False)\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.norm = norm_factory(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.dw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.pw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dw(x)\n",
        "        x = self.pw(x)\n",
        "\n",
        "        return self.act(self.norm(x))\n",
        "# -----------------------------------------------------------------------------\n",
        "# StageAwareBackbone (\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class StageAwareBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    Four-tier backbone based on DAT + DCN\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                depths: list[int] = (3, 4, 8, 3),\n",
        "                drop_path_max: float = 0.0,\n",
        "                num_classes: int = 80,\n",
        "                voc_prior: bool = False,\n",
        "                norm_factory: NormFactory = NormFactory(\"gn\"),\n",
        "                use_layer_scale: bool = True,\n",
        "                layer_scale_init: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.voc_prior   = voc_prior\n",
        "\n",
        "        # ---- DropPath rates ----\n",
        "        total_blocks = sum(depths)\n",
        "        dpr = get_drop_path_rates(total_blocks, drop_path_max)\n",
        "        for i in range(depths[0] + depths[1]):   # st0, st1\n",
        "            dpr[i] = 0.0\n",
        "\n",
        "        # ---- Stem ----\n",
        "        self.stem = Stem(128)\n",
        "\n",
        "        # ---- Stage‑0 ----\n",
        "        dp_idx = 0\n",
        "        self.st0 = nn.Sequential(*[\n",
        "          StageZeroBlock(\n",
        "              128,\n",
        "              drop_path_rate=dpr[i],\n",
        "              use_layer_scale=use_layer_scale,\n",
        "              layer_scale_init=layer_scale_init,\n",
        "              norm_factory=norm_factory\n",
        "          )\n",
        "          for i in range(depths[0])\n",
        "      ])\n",
        "        dp_idx += depths[0]\n",
        "\n",
        "        # ---- Stage‑1 ----\n",
        "        self.st1 = nn.Sequential(*[\n",
        "            StageOneBlock(\n",
        "                128,\n",
        "                drop_path=dpr[dp_idx + i],\n",
        "                use_layer_scale=use_layer_scale,\n",
        "                layer_scale_init=layer_scale_init,\n",
        "                norm_factory=norm_factory)\n",
        "            for i in range(depths[1])\n",
        "        ])\n",
        "        dp_idx += depths[1]\n",
        "\n",
        "        # ---- PGI + Down/CSI ----\n",
        "        self.pgi_s1 = PGIModule(128, norm_factory=norm_factory, aux_channels=128)   # s0_out → s1_raw\n",
        "        self.da     = DownABlock(128, 256, norm_factory=norm_factory)\n",
        "        self.csi_s1_s2 = CrossScaleInjection(low_ch=128, high_ch=256)\n",
        "        self.pgi_s2 = PGIModule(256, norm_factory=norm_factory, aux_channels=128)\n",
        "\n",
        "        # ---- Stage‑2 ----\n",
        "        self.st2 = nn.Sequential(*[\n",
        "            StageTwoBlock(\n",
        "                256, dcn_group=16,\n",
        "                drop_path=dpr[dp_idx + i],\n",
        "                norm_factory=norm_factory)\n",
        "            for i in range(depths[2])\n",
        "        ])\n",
        "        dp_idx += depths[2]\n",
        "\n",
        "        # ---- DownB / PGI ----\n",
        "        self.db     = DownBBlock(256, 640, norm_factory=norm_factory)\n",
        "        self.pgi_s3 = PGIModule(640, norm_factory=norm_factory, aux_channels=256)\n",
        "\n",
        "        # ---- Stage‑3 ----\n",
        "        self.st3 = nn.Sequential(*[\n",
        "            StageThreeBlock(\n",
        "                640, 640,\n",
        "                drop_path=dpr[dp_idx + i],\n",
        "                norm_factory=norm_factory)\n",
        "            for i in range(depths[3])\n",
        "        ])\n",
        "\n",
        "        # ---- DownC ----\n",
        "        self.dc = DownCBlock(640, 768, norm_factory=norm_factory)\n",
        "\n",
        "        # ---- Auxiliary dense heads ----\n",
        "        self.aux_head_s1 = nn.Conv2d(128, num_classes + 4, 1, bias=True)\n",
        "        self.aux_head_s2 = nn.Conv2d(256, num_classes + 4, 1, bias=True)\n",
        "        self.aux_head_s3 = nn.Conv2d(640, num_classes + 4, 1, bias=True)\n",
        "        self._init_aux_heads()\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def _init_aux_heads(self):\n",
        "        prior_value = 19 if self.voc_prior else 99\n",
        "        for head in (self.aux_head_s1, self.aux_head_s2, self.aux_head_s3):\n",
        "            nn.init.normal_(head.weight, std=0.01)\n",
        "            nn.init.constant_(head.bias[: self.num_classes], -math.log(prior_value))\n",
        "            nn.init.constant_(head.bias[self.num_classes :], 0.0)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def forward(self, x: torch.Tensor, need_aux: bool = False):\n",
        "        x      = self.stem(x)\n",
        "        s0_out = self.st0(x)\n",
        "\n",
        "        s1_raw = self.st1(s0_out)\n",
        "        p3     = self.pgi_s1(s1_raw, aux_input=s0_out)\n",
        "\n",
        "        p3_down = self.da(p3)\n",
        "        s2_in   = self.csi_s1_s2(p3, p3_down)\n",
        "        s2_in   = self.pgi_s2(s2_in, aux_input=p3)\n",
        "\n",
        "        p4 = self.st2(s2_in)\n",
        "        p4_down = self.db(p4)\n",
        "        s3_in   = self.pgi_s3(p4_down, aux_input=p4)\n",
        "\n",
        "        p5 = self.st3(s3_in)\n",
        "        p6 = self.dc(p5)\n",
        "\n",
        "        if self.training and need_aux:\n",
        "            aux = {\n",
        "                's1': self.aux_head_s1(p3),\n",
        "                's2': self.aux_head_s2(p4),\n",
        "                's3': self.aux_head_s3(p5),\n",
        "            }\n",
        "            return (p3, p4, p5, p6), aux\n",
        "        return p3, p4, p5, p6\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# SpatialFuseNode – softplus-normalized, gradient-safe\n",
        "# ----------------------------------------------------------\n",
        "class SpatialFuseNode(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatial-aware fuse (group-wise, production-ready, RPB-friendly)\n",
        "      • Group-average energy map per input (B,G,K,H,W)\n",
        "      • 1×1 projection per group (K→K), softplus-normalized\n",
        "      • Initial fully uniform: proj.weight=0, bias=0  → equal w for each branch\n",
        "      • Learnable temperature τ (with clamp), optional uniform floor (no dead branches)\n",
        "      • No detach anywhere → gradients are unclipped (including RPBs)\n",
        "\n",
        "    Args:\n",
        "        n_inputs (int): K, how many features to combine (>=2)\n",
        "        channels (int): C, number of channels\n",
        "        groups (int): G, number of groups (automatically drops to 1 if C % G ≠ 0)\n",
        "        tau_init (float): τ start (recommended: 0.9)\n",
        "        learnable_tau (bool): Should τ be learned?\n",
        "        eps (float): Numerical epsilon for normalization and splitting\n",
        "        init_noise (float): (backward compatibility parameter; not used with uniform init)\n",
        "        gate_floor (float): w ← (1-α)·w + α·(1/K); α∈[0,0.1] recommended, 0 closed\n",
        "        tau_bounds (tuple): τ lower/upper bounds (e.g., (0.5, 2.0))\n",
        "        uniform_init (bool): True ⇒ weight=0, bias=0 (full uniform initialization)\n",
        "        save_last (bool): If True, last w is saved (for debug/regularizer)\n",
        "        prenorm (str): “none” | “rms”  — RMS prenorm on the K axis (reduces saturation)\n",
        "\n",
        "    Notes:\n",
        "      • Returns extra_loss() for the entropy regularizer (optional, can be added to the loss).\n",
        "      • The last w (B,G,K,H,W) can be inspected with get_last_weights() (if save_last=True).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_inputs: int,\n",
        "                 channels: int,\n",
        "                 groups: int = 4,\n",
        "                 tau_init: float = 0.9,\n",
        "                 learnable_tau: bool = True,\n",
        "                 eps: float = 5e-4,\n",
        "                 init_noise: float = 1e-3,\n",
        "                 *,\n",
        "                 gate_floor: float = 0.0,\n",
        "                 tau_bounds: tuple = (0.5, 2.0),\n",
        "                 uniform_init: bool = True,\n",
        "                 save_last: bool = False,\n",
        "                 prenorm: str = \"none\"):\n",
        "        super().__init__()\n",
        "        assert n_inputs >= 2, \"SpatialFuseNode: should be n_inputs >= 2 .\"\n",
        "        self.K = int(n_inputs)\n",
        "        self.C = int(channels)\n",
        "        self.G = int(groups) if (groups > 0 and channels % groups == 0) else 1\n",
        "        self.gC = self.C // self.G\n",
        "        self.eps = float(eps)\n",
        "        self.gate_floor = float(gate_floor)\n",
        "        self.tau_lo, self.tau_hi = float(tau_bounds[0]), float(tau_bounds[1])\n",
        "        self.save_last = bool(save_last)\n",
        "        self.prenorm = str(prenorm).lower()\n",
        "        self._last_w = None  # debug amaçlı\n",
        "\n",
        "        # Group-wise K→K projection (produces spatially-varying logit)\n",
        "        # Takes input divided into G groups (B, G*K, H, W) → Output (B, G*K, H, W)\n",
        "        self.proj = nn.Conv2d(self.G * self.K, self.G * self.K, kernel_size=1,\n",
        "                              groups=self.G, bias=True)\n",
        "\n",
        "        # --- Uniform start (to prevent early arm saturation) ---\n",
        "        if uniform_init:\n",
        "            nn.init.zeros_(self.proj.bias)\n",
        "            with torch.no_grad():\n",
        "                self.proj.weight.zero_()\n",
        "        else:\n",
        "            nn.init.kaiming_uniform_(self.proj.weight, a=math.sqrt(5))\n",
        "            nn.init.zeros_(self.proj.bias)\n",
        "\n",
        "        # --- temperature τ ---\n",
        "        if learnable_tau:\n",
        "            self.log_tau = nn.Parameter(torch.log(torch.tensor(float(tau_init))))\n",
        "        else:\n",
        "            self.register_buffer(\"log_tau\", torch.log(torch.tensor(float(tau_init))), persistent=False)\n",
        "\n",
        "    # ------------------ helpers ------------------\n",
        "    @torch.no_grad()\n",
        "    def set_tau(self, tau: float):\n",
        "        \"\"\"To adjust τ during heating.\"\"\"\n",
        "        v = max(1e-3, float(tau))\n",
        "        t = torch.log(torch.tensor(v, device=self.log_tau.device, dtype=self.log_tau.dtype))\n",
        "        self.log_tau.copy_(t)\n",
        "\n",
        "    def get_last_weights(self):\n",
        "        \"\"\"The last calculated w (B, G, K, H, W). If save_last=True, it is saved.\"\"\"\n",
        "        return self._last_w\n",
        "\n",
        "    def extra_loss(self) -> dict:\n",
        "        \"\"\"\n",
        "        Optional regulator: gate entropy (higher values result in more balanced branching).\n",
        "        You can add a small coefficient on the loss side (e.g., 1e-4..5e-4).\n",
        "        \"\"\"\n",
        "        if self._last_w is None:\n",
        "            return {}\n",
        "        p = self._last_w.clamp_min(1e-8)\n",
        "        # Entropy: -sum_k p log p / log(K)  → [0,1]\n",
        "        ent = -(p * p.log()).sum(dim=2) / math.log(self.K)   # (B,G,H,W)\n",
        "        return {\"gate_entropy\": ent.mean()}\n",
        "\n",
        "    def _group_pool(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # (B,C,H,W) → (B,G,H,W), average per group\n",
        "        B, C, H, W = x.shape\n",
        "        return x.view(B, self.G, self.gC, H, W).mean(dim=2)\n",
        "\n",
        "    # ------------------ forward ------------------\n",
        "    def forward(self, *features: torch.Tensor) -> torch.Tensor:\n",
        "        # All inputs must be in the same format.\n",
        "        K = len(features)\n",
        "        assert K == self.K, f\"SpatialFuseNode: {self.K} giriş bekleniyordu, {K} geldi.\"\n",
        "        B, C, H, W = features[0].shape\n",
        "        for f in features:\n",
        "            assert f.shape == (B, C, H, W), \"Tüm giriş feature'lar (B,C,H,W) aynı şekil olmalı.\"\n",
        "\n",
        "        # 1) Grup havuzu → (B,G,K,H,W)\n",
        "        gp = [self._group_pool(f) for f in features]\n",
        "        gp_cat = torch.stack(gp, dim=2)  # (B,G,K,H,W)\n",
        "\n",
        "        # 2) (optional) RMS prenorm → logit scale control on the K axis\n",
        "        if self.prenorm == \"rms\":\n",
        "            rms = gp_cat.pow(2).mean(dim=2, keepdim=True).add(1e-6).sqrt()\n",
        "            gp_cat = gp_cat / rms\n",
        "\n",
        "        # 3) Projection and gate logits\n",
        "        x = gp_cat.flatten(1, 2)                      # (B, G*K, H, W)\n",
        "        logits = self.proj(x).view(B, self.G, self.K, H, W)\n",
        "\n",
        "        # 4) Softplus-normalize + τ\n",
        "        tau = self.log_tau.exp().clamp(self.tau_lo, self.tau_hi)\n",
        "        w = F.softplus(logits / tau) + 1e-9           # (B,G,K,H,W), her yerde >0\n",
        "        w = w / (w.sum(dim=2, keepdim=True) + self.eps)\n",
        "\n",
        "        # 5) Uniform floor (no dead leg, gradient flows to every leg)\n",
        "        if self.gate_floor > 0.0:\n",
        "            u = 1.0 / float(self.K)\n",
        "            w = (1.0 - self.gate_floor) * w + self.gate_floor * u  # no need to normalize again\n",
        "\n",
        "        if self.save_last:\n",
        "            self._last_w = w.detach()\n",
        "\n",
        "        # 6) Distribute weights to channels efficiently (no repeats, memory-friendly)\n",
        "        #    w_k: (B,G,1,H,W), f: (B,G,gC,H,W) → contribution (B,C,H,W)\n",
        "        out = None\n",
        "        for k, f in enumerate(features):\n",
        "            wk = w[:, :, k, :, :].unsqueeze(2)                 # (B,G,1,H,W)\n",
        "            contrib = (f.view(B, self.G, self.gC, H, W) * wk).view(B, C, H, W)\n",
        "            out = contrib if out is None else (out + contrib)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class RMSNorm2d(nn.Module):\n",
        "    def __init__(self, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = float(eps)\n",
        "    def forward(self, x):\n",
        "        return x / (x.pow(2).mean(dim=(2,3), keepdim=True).add(self.eps).sqrt())\n",
        "\n",
        "def _init_laplacian_dw(dw: nn.Conv2d):\n",
        "    with torch.no_grad():\n",
        "        k = torch.tensor([[0., 1., 0.],\n",
        "                          [1.,-4., 1.],\n",
        "                          [0., 1., 0.]], dtype=dw.weight.dtype, device=dw.weight.device)\n",
        "        w = torch.zeros_like(dw.weight)\n",
        "        w[:, 0, :, :] = k\n",
        "        dw.weight.copy_(w)\n",
        "\n",
        "def _ste_boost(x: torch.Tensor, gain: float) -> torch.Tensor:\n",
        "    return x if gain <= 0 else (x + gain * (x - x.detach()))\n",
        "\n",
        "class _FPNResBlock(nn.Module):\n",
        "    def __init__(self, channels: int, drop_path: float = 0.0, norm_factory: 'NormFactory' = None):\n",
        "        super().__init__()\n",
        "        nf = norm_factory or NormFactory(\"gn\")\n",
        "        self.conv = ConvLNAct(channels, channels, k=3)\n",
        "        self.ls   = LayerScale(channels, init_values=0.6)\n",
        "        self.dp   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "    def forward(self, x):\n",
        "        return x + self.dp(self.ls(self.conv(x)))\n",
        "\n",
        "class LightBiFPN(nn.Module):\n",
        "    \"\"\"\n",
        "    LightBiFPN v4.1 — spatial‑aware, RPB‑friendly, repeat‑shared up/edge\n",
        "      • SpatialFuseNode (group‑based 2D weights)\n",
        "      • No‑blur in first top‑down (bilinear + norm/act)\n",
        "      • Up-refine: DW 3×3 + (optional) Laplacian residual (small LS)\n",
        "      • RMS prenorm (optional)\n",
        "      • Low DropPath schedule (0.0 → 0.01)\n",
        "      • Light grad-boost for p3\n",
        "      • **New**: 3 up/edge blocks and **shared between iterations**\n",
        "                  → No “NEVER UPDATED”, no parameter waste\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: tuple[int,int,int,int] = (128, 256, 640, 768),\n",
        "        out: int = 256,\n",
        "        repeats: int = 2,\n",
        "        *,\n",
        "        norm_factory: 'NormFactory' | None = None,\n",
        "        # ---- fuse ----\n",
        "        use_spatial_fuse: bool = True,\n",
        "        fuse_groups: int = 4,\n",
        "        fuse_tau_init: float = 0.9,\n",
        "        fuse_learn_tau: bool = True,\n",
        "        fuse_eps: float = 5e-4,\n",
        "        init_noise: float = 1e-4,\n",
        "        # ---- refine & norm ----\n",
        "        prenorm: str = \"rms\",            # \"none\" | \"rms\"\n",
        "        no_blur_first: bool = True,\n",
        "        edge_enhance: bool = True,\n",
        "        edge_ls_init: float = 0.03,\n",
        "        # ---- drop path ----\n",
        "        dp_top_second: float = 0.01,\n",
        "        dp_bot_second: float = 0.01,\n",
        "        # ---- grad boost ----\n",
        "        grad_boost_low: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        in3, in4, in5, in6 = in_channels\n",
        "        self.repeats = int(repeats)\n",
        "        self.out = int(out)\n",
        "        self.norm_factory = norm_factory or NormFactory(\"gn\")\n",
        "        self.prenorm = prenorm.lower()\n",
        "        self.no_blur_first = bool(no_blur_first)\n",
        "        self.edge_enhance = bool(edge_enhance)\n",
        "        self.grad_boost_low = float(grad_boost_low)\n",
        "        self.use_spatial_fuse = bool(use_spatial_fuse)\n",
        "        self.fuse_groups = int(fuse_groups)\n",
        "        self.fuse_tau_init = float(fuse_tau_init)\n",
        "        self.fuse_learn_tau = bool(fuse_learn_tau)\n",
        "        self.fuse_eps = float(fuse_eps)\n",
        "        self.init_noise = float(init_noise)\n",
        "\n",
        "        # 1x1 projections\n",
        "        self.p3_in = ConvLNAct(in3, out, k=1, p=0)\n",
        "        self.p4_in = ConvLNAct(in4, out, k=1, p=0)\n",
        "        self.p5_in = ConvLNAct(in5, out, k=1, p=0)\n",
        "        self.p6_in = ConvLNAct(in6, out, k=1, p=0)\n",
        "\n",
        "        # DropPath schedule\n",
        "        top_dps = [0.0, float(dp_top_second)]\n",
        "        bot_dps = [0.0, float(dp_bot_second)]\n",
        "        self.top_convs = nn.ModuleList([\n",
        "            _FPNResBlock(out, drop_path=top_dps[i // 3], norm_factory=self.norm_factory)\n",
        "            for i in range(3 * self.repeats)\n",
        "        ])\n",
        "        self.bot_convs = nn.ModuleList([\n",
        "            _FPNResBlock(out, drop_path=bot_dps[i // 4], norm_factory=self.norm_factory)\n",
        "            for i in range(4 * self.repeats)\n",
        "        ])\n",
        "\n",
        "        # Fuse nodes\n",
        "        def _make_fuse(K: int):\n",
        "            from typing import Callable\n",
        "\n",
        "            return SpatialFuseNode(\n",
        "                                      K, channels=out, groups=self.fuse_groups,\n",
        "                                      tau_init=1.4, learnable_tau=True,\n",
        "                                      tau_bounds=(0.8, 1.8),\n",
        "                                      prenorm=\"rms\",\n",
        "                                      gate_floor=0.02,          # no death\n",
        "                                      uniform_init=True,\n",
        "                                      eps=5e-4,\n",
        "                                      save_last=True\n",
        "                                  )\n",
        "\n",
        "        self.fuse2_top = nn.ModuleList([_make_fuse(2) for _ in range(3 * self.repeats)])\n",
        "        self.fuse2_bot = nn.ModuleList([_make_fuse(2) for _ in range(3 * self.repeats)])\n",
        "        self.fuse3_bot = nn.ModuleList([_make_fuse(3) for _ in range(1 * self.repeats)])\n",
        "\n",
        "        # ---  3 up/edge blocks and shared between repeats ---\n",
        "        self.up_dw, self.up_na = nn.ModuleList(), nn.ModuleList()\n",
        "        self.edge_dw, self.edge_ls = nn.ModuleList(), nn.ModuleList()\n",
        "        for _ in range(3):  # only for 3 items\n",
        "            dw = nn.Conv2d(out, out, 3, 1, 1, groups=out, bias=False)\n",
        "            nn.init.kaiming_normal_(dw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            self.up_dw.append(dw)\n",
        "\n",
        "            nf_layer = self.norm_factory(out)\n",
        "            if isinstance(nf_layer, LayerNormProxy):\n",
        "                from DAT.models.dat_blocks import LNAct as _LNAct\n",
        "                self.up_na.append(_LNAct(out))\n",
        "            else:\n",
        "                self.up_na.append(nn.Sequential(nf_layer, nn.SiLU(inplace=True)))\n",
        "\n",
        "            edw = nn.Conv2d(out, out, 3, 1, 1, groups=out, bias=False)\n",
        "            _init_laplacian_dw(edw)\n",
        "            self.edge_dw.append(edw)\n",
        "            self.edge_ls.append(LayerScale(out, init_values=edge_ls_init))\n",
        "\n",
        "        # High-level passthrough LS (p5, p4, p3)\n",
        "        self.keep_top = nn.ModuleList([LayerScale(out, init_values=0.10) for _ in range(3)])\n",
        "\n",
        "        # Input balancing\n",
        "        self.balance = RMSNorm2d(eps=1e-6) if self.prenorm == \"rms\" else nn.Identity()\n",
        "\n",
        "    # --- helpers ---\n",
        "    def _up_refine_shared(self, x: torch.Tensor, size_hw: tuple[int,int], step_id: int, rep_idx: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        step_id: [0,1,2] → corresponds to each top-down step, independent of repeat\n",
        "        \"\"\"\n",
        "        x = F.interpolate(x, size=size_hw, mode=\"bilinear\", align_corners=False)\n",
        "        if self.no_blur_first and rep_idx == 0:\n",
        "            # no-blur first repeat: only norm/act\n",
        "            x = self.up_na[step_id](x)\n",
        "        else:\n",
        "            x = self.up_dw[step_id](x)\n",
        "            if self.edge_enhance:\n",
        "                x = x + self.edge_ls[step_id](self.edge_dw[step_id](x))\n",
        "            x = self.up_na[step_id](x)\n",
        "        return x\n",
        "\n",
        "    def _bal(self, *xs):\n",
        "        return (tuple(self.balance(x) for x in xs) if self.prenorm != \"none\" else xs)\n",
        "\n",
        "    def forward(self, p3, p4, p5, p6):\n",
        "        # Proj\n",
        "        p3, p4, p5, p6 = self.p3_in(p3), self.p4_in(p4), self.p5_in(p5), self.p6_in(p6)\n",
        "\n",
        "        # Low-level grad-boost\n",
        "        if self.training and self.grad_boost_low > 0:\n",
        "            p3 = _ste_boost(p3, self.grad_boost_low)\n",
        "\n",
        "        tconv = bconv = f2t = f2b = f3b = 0\n",
        "\n",
        "        for rep in range(self.repeats):\n",
        "            # ---------- Top‑down ----------\n",
        "            # step_id = 0: p6→p5\n",
        "            u5 = self._up_refine_shared(p6, p5.shape[-2:], step_id=0, rep_idx=rep)\n",
        "            p5_b, u5_b = self._bal(p5, u5)\n",
        "            p5_td = self.fuse2_top[f2t](p5_b, u5_b); f2t += 1\n",
        "            p5_td = self.top_convs[tconv](p5_td); tconv += 1\n",
        "            p5_td = p5_td + self.keep_top[0](p5)\n",
        "\n",
        "            # step_id = 1: p5_td→p4\n",
        "            u4 = self._up_refine_shared(p5_td, p4.shape[-2:], step_id=1, rep_idx=rep)\n",
        "            p4_b, u4_b = self._bal(p4, u4)\n",
        "            p4_td = self.fuse2_top[f2t](p4_b, u4_b); f2t += 1\n",
        "            p4_td = self.top_convs[tconv](p4_td); tconv += 1\n",
        "            p4_td = p4_td + self.keep_top[1](p4)\n",
        "\n",
        "            # step_id = 2: p4_td→p3\n",
        "            u3 = self._up_refine_shared(p4_td, p3.shape[-2:], step_id=2, rep_idx=rep)\n",
        "            p3_b, u3_b = self._bal(p3, u3)\n",
        "            p3_td = self.fuse2_top[f2t](p3_b, u3_b); f2t += 1\n",
        "            p3_td = self.top_convs[tconv](p3_td); tconv += 1\n",
        "            p3_td = p3_td + self.keep_top[2](p3)\n",
        "\n",
        "            # ---------- Bottom‑up ----------\n",
        "            p3 = self.fuse2_bot[f2b](*self._bal(p3, p3_td)); f2b += 1\n",
        "            p3 = self.bot_convs[bconv](p3); bconv += 1\n",
        "\n",
        "            p4 = self.fuse2_bot[f2b](*self._bal(p4_td, F.max_pool2d(p3, 2))); f2b += 1\n",
        "            p4 = self.bot_convs[bconv](p4); bconv += 1\n",
        "\n",
        "            p5 = self.fuse3_bot[f3b](*self._bal(p5, F.max_pool2d(p4, 2), p5_td)); f3b += 1\n",
        "            p5 = self.bot_convs[bconv](p5); bconv += 1\n",
        "\n",
        "            p6 = self.fuse2_bot[f2b](*self._bal(p6, F.max_pool2d(p5, 2))); f2b += 1\n",
        "            p6 = self.bot_convs[bconv](p6); bconv += 1\n",
        "\n",
        "        return p3, p4, p5, p6\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Positional Encoding\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class Pos2d(nn.Module):\n",
        "    \"\"\"2D sinusoidal positional encoding\"\"\"\n",
        "    def __init__(self, c: int = 320, max_h: int = 640, max_w: int = 640):\n",
        "        super().__init__()\n",
        "        assert c % 4 == 0\n",
        "        self.cq = c // 4\n",
        "        pos = self._build(max_h, max_w, c)\n",
        "        self.register_buffer(\"pos_table\", pos, persistent=False)\n",
        "\n",
        "    def _build(self, H: int, W: int, C: int) -> torch.Tensor:\n",
        "        yv, xv = torch.meshgrid(\n",
        "            torch.linspace(0, 1, H), torch.linspace(0, 1, W), indexing=\"ij\"\n",
        "        )\n",
        "        div = torch.exp(torch.arange(0, self.cq) * (-math.log(10000.0) / self.cq))\n",
        "        pos_x = (xv[..., None] * div).reshape(H, W, -1)\n",
        "        pos_y = (yv[..., None] * div).reshape(H, W, -1)\n",
        "        pos = torch.cat([pos_y.sin(), pos_y.cos(), pos_x.sin(), pos_x.cos()], dim=2)\n",
        "        return pos.permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        _, _, H, W = x.shape\n",
        "        return self.pos_table[:, :, :H, :W]\n",
        "\n",
        "\n",
        "\n",
        "import inspect\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# DATBlock –  TransformerStage wrapper\n",
        "# -----------------------------------------------------------------------------\n",
        "class DATBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                channels: int,\n",
        "                depth: int = 2,\n",
        "                heads: int | None = None,\n",
        "                window_size: int = 8,\n",
        "                drop: float = 0.0):\n",
        "        super().__init__()\n",
        "        heads = heads or max(4, channels // 32)\n",
        "\n",
        "        cfg = dict(\n",
        "            fmap_size           = (window_size, window_size),\n",
        "            window_size         = window_size,\n",
        "            ns_per_pt           = 4,\n",
        "            dim_in              = channels,\n",
        "            dim_embed           = channels,\n",
        "            depths              = depth,               # INT\n",
        "            stage_spec          = 'N' * depth,\n",
        "            n_groups            = 1,\n",
        "            use_pe              = True,\n",
        "            sr_ratio            = 1,\n",
        "            heads               = heads,               # INT\n",
        "            heads_q             = [heads] * depth,     # LIST\n",
        "            stride              = 1,\n",
        "            offset_range_factor = 2,\n",
        "            dwc_pe              = True,\n",
        "            no_off              = False,\n",
        "            fixed_pe            = False,\n",
        "            attn_drop           = drop,\n",
        "            proj_drop           = drop,\n",
        "            expansion           = 4,\n",
        "            drop                = drop,\n",
        "            drop_path_rate      = [drop] * depth,      # LIST\n",
        "            use_dwc_mlp         = False,\n",
        "            ksize               = 3,\n",
        "            nat_ksize           = 7,\n",
        "            k_qna               = 8,\n",
        "            nq_qna              = 9,\n",
        "            qna_activation      = \"relu\",\n",
        "            layer_scale_value   = 0.3,\n",
        "            use_lpu             = False,\n",
        "            log_cpb             = True,\n",
        "        )\n",
        "\n",
        "        from DAT.models.dat import TransformerStage\n",
        "        self.stage = TransformerStage(**cfg)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x = (B,C,H,W) –\n",
        "        if hasattr(self.stage, 'fmap_size'):\n",
        "            self.stage.fmap_size = x.shape[-2:]\n",
        "        try:\n",
        "            return self.stage(x)\n",
        "        except TypeError:                           # some versions (x,H,W) require\n",
        "            H, W = x.shape[-2:]\n",
        "            return self.stage(x, H, W)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Deformable Encoder (production-ready)\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class GLUFFN(nn.Module):\n",
        "    \"\"\"\n",
        "    GLU-based FFN:\n",
        "      - First linear: d_model -> 2*inner\n",
        "      - Gate: SwiGLU (SiLU) or GEGLU (GELU)\n",
        "      - Second linear: inner -> d_model\n",
        "\n",
        "    inner width:\n",
        "      • if ffn_dim is given: inner = ffn_dim // 2  (2*inner = ffn_dim)\n",
        "      • otherwise: inner = round(d_model * ffn_mult)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        ffn_dim: Optional[int] = None,\n",
        "        ffn_mult: float = 2.0,\n",
        "        act: str = \"swiglu\",\n",
        "        drop: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if ffn_dim is not None:\n",
        "            inner = max(32, int(ffn_dim) // 2)\n",
        "        else:\n",
        "            inner = max(32, int(round(d_model * float(ffn_mult))))\n",
        "        self.fc1 = nn.Linear(d_model, inner * 2, bias=True)\n",
        "        self.fc2 = nn.Linear(inner, d_model, bias=True)\n",
        "        self.drop = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "\n",
        "        act = act.lower()\n",
        "        if act not in (\"swiglu\", \"geglu\"):\n",
        "            raise ValueError(\"GLUFFN.act must be 'swiglu' or 'geglu'\")\n",
        "        self.act_kind = act\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        xh = self.fc1(x)\n",
        "        x_lin, x_gate = xh.chunk(2, dim=-1)\n",
        "        if self.act_kind == \"swiglu\":\n",
        "            gated = F.silu(x_gate) * x_lin\n",
        "        else:  # 'geglu'\n",
        "            gated = F.gelu(x_gate) * x_lin\n",
        "        return self.drop(self.fc2(gated))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DeformEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    MS-Deformable self-attention + GLU-FFN, Pre-Norm + LayerScale + DropPath.\n",
        "\n",
        "    Args (backward-compatible):\n",
        "        d_model, n_heads, n_levels, n_points\n",
        "        ffn_dim:   optional (if given, GLU inner width is ffn_dim//2)\n",
        "        ffn_mult:  used if ffn_dim is not specified (default 2.0)\n",
        "        ffn_act:   ‘swiglu’ (default) or ‘geglu’\n",
        "        drop, drop_path\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int = 320,\n",
        "        n_heads: int = 10,\n",
        "        n_levels: int = 4,\n",
        "        n_points: int = 4,\n",
        "        *,\n",
        "        ffn_dim: Optional[int] = None,\n",
        "        ffn_mult: float = 2.0,\n",
        "        ffn_act: str = \"swiglu\",\n",
        "        drop: float = 0.0,\n",
        "        drop_path: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn = MultiScaleDeformableAttention(\n",
        "            embed_dims=d_model, num_heads=n_heads,\n",
        "            num_levels=n_levels, num_points=n_points,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ls1   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop1 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp1   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        self.ffn   = GLUFFN(d_model, ffn_dim=ffn_dim, ffn_mult=ffn_mult, act=ffn_act, drop=drop)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ls2   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop2 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp2   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_encoder_reference_points(\n",
        "        spatial_shapes: torch.Tensor, B: int, device, dtype\n",
        "    ) -> torch.Tensor:\n",
        "        ref_list = []\n",
        "        for (H, W) in spatial_shapes.tolist():\n",
        "            ref_y, ref_x = torch.meshgrid(\n",
        "                torch.linspace(0.5, H - 0.5, H, device=device, dtype=dtype) / H,\n",
        "                torch.linspace(0.5, W - 0.5, W, device=device, dtype=dtype) / W,\n",
        "                indexing=\"ij\",\n",
        "            )\n",
        "            ref = torch.stack((ref_x, ref_y), dim=-1).reshape(-1, 2)  # (HW,2)\n",
        "            ref_list.append(ref)\n",
        "        return torch.cat(ref_list, dim=0)[None, :, None, :].repeat(B, 1, spatial_shapes.size(0), 1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.Tensor,             # (B, sumHW, C)\n",
        "        pos: torch.Tensor,             # (B, sumHW, C)\n",
        "        spatial_shapes: torch.Tensor,  # (L, 2)\n",
        "        lvl_start_idx: torch.Tensor,   # (L,)\n",
        "        key_padding_mask: Optional[torch.Tensor] = None,\n",
        "    ) -> torch.Tensor:\n",
        "\n",
        "        B, N, C = src.shape\n",
        "        ref_pts = self._make_encoder_reference_points(\n",
        "            spatial_shapes=spatial_shapes, B=B, device=src.device, dtype=src.dtype\n",
        "        )  # (B,N,L,2)\n",
        "\n",
        "        x = src\n",
        "        q = self.norm1(x)\n",
        "        attn_out = self.self_attn(\n",
        "            query=q, value=q,\n",
        "            reference_points=ref_pts,\n",
        "            spatial_shapes=spatial_shapes,\n",
        "            level_start_index=lvl_start_idx,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            query_pos=pos,\n",
        "        )\n",
        "        x = x + self.dp1(self.ls1(self.drop1(attn_out)))\n",
        "\n",
        "        y = self.ffn(self.norm2(x))\n",
        "        x = x + self.dp2(self.ls2(self.drop2(y)))\n",
        "        return x\n",
        "\n",
        "class TinyDeformEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Two layers are recommended (lightweight and effective).\n",
        "    get_drop_path_rates() is available;\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_layers: int = 1,\n",
        "                 d_model: int = 320,\n",
        "                 n_heads: int = 10,\n",
        "                 n_levels: int = 4,\n",
        "                 n_points: int = 4,\n",
        "                 ffn_dim: int = 1024,\n",
        "                 drop: float = 0.0,\n",
        "                 drop_path_max: float = 0.1):\n",
        "        super().__init__()\n",
        "        dpr = get_drop_path_rates(num_layers, drop_path_max)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DeformEncoderLayer(d_model=d_model,\n",
        "                               n_heads=n_heads,\n",
        "                               n_levels=n_levels,\n",
        "                               n_points=n_points,\n",
        "                               ffn_dim=ffn_dim,\n",
        "                               drop=drop,\n",
        "                               drop_path=dpr[i])\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self,\n",
        "                src: torch.Tensor,             # (B, sumHW, C)\n",
        "                pos: torch.Tensor,             # (B, sumHW, C)\n",
        "                spatial_shapes: torch.Tensor,  # (L,2)\n",
        "                lvl_start_idx: torch.Tensor,   # (L,)\n",
        "                key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        for ly in self.layers:\n",
        "            src = ly(src, pos, spatial_shapes, lvl_start_idx, key_padding_mask)\n",
        "        return src\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# RT-Deform Decoder (Fixed IoU-aware padding)\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DeformDecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder:\n",
        "      Pre-Norm + LayerScale + DropPath + GLU-FFN + tanh-bounded ref update.\n",
        "      ✔ Optional self-attention mask support (for DN↔MATCH isolation)\n",
        "\n",
        "    Args:\n",
        "        d_model, n_heads, n_levels, n_points\n",
        "        ffn_dim:   optional (for GLU inner width, 2*inner = ffn_dim)\n",
        "        ffn_mult:  used if ffn_dim is absent\n",
        "        ffn_act:   ‘swiglu’ | ‘geglu’\n",
        "        drop, drop_path\n",
        "        refine_scale: float or (lo,hi) — schedule with layer-id\n",
        "        grad_eps: very small nudg\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int = 320,\n",
        "        n_heads: int = 10,\n",
        "        n_levels: int = 4,\n",
        "        n_points: int = 4,\n",
        "        *,\n",
        "        ffn_dim: Optional[int] = None,\n",
        "        ffn_mult: float = 2.0,\n",
        "        ffn_act: str = \"swiglu\",\n",
        "        drop: float = 0.0,\n",
        "        drop_path: float = 0.1,\n",
        "        refine_scale: Union[float, Tuple[float, float]] = 0.5,\n",
        "        layer_id: Optional[int] = None,\n",
        "        num_layers: Optional[int] = None,\n",
        "        grad_eps: float = 1e-4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.grad_eps = float(grad_eps)\n",
        "\n",
        "        # refine scale\n",
        "        if isinstance(refine_scale, (tuple, list)):\n",
        "            lo, hi = float(refine_scale[0]), float(refine_scale[1])\n",
        "            if (num_layers is not None) and (layer_id is not None) and (num_layers > 1):\n",
        "                t = float(layer_id) / float(num_layers - 1)\n",
        "                self.refine_scale = lo + (hi - lo) * t\n",
        "            else:\n",
        "                self.refine_scale = 0.5 * (lo + hi)\n",
        "        else:\n",
        "            self.refine_scale = float(refine_scale)\n",
        "\n",
        "        # Self-Attn (MHA) + Pre-Norm\n",
        "        self.self_attn = nn.MultiheadAttention(\n",
        "            embed_dim=d_model, num_heads=n_heads, batch_first=True\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ls1   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop1 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp1   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # Cross-Attn (MS-Deformable) + Pre-Norm\n",
        "        self.cross_attn = MultiScaleDeformableAttention(\n",
        "            embed_dims=d_model, num_heads=n_heads,\n",
        "            num_levels=n_levels, num_points=n_points,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ls2   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop2 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp2   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # FFN (GLU) + Pre-Norm\n",
        "        self.ffn   = GLUFFN(d_model, ffn_dim=ffn_dim, ffn_mult=ffn_mult, act=ffn_act, drop=drop)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.ls3   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop3 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp3   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # Ref delta head (tanh clamp)\n",
        "        self.ref_mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model, 2),\n",
        "        )\n",
        "        for m in self.ref_mlp:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
        "\n",
        "    @staticmethod\n",
        "    def _inv_sigmoid(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "        x = x.clamp(eps, 1.0 - eps)\n",
        "        return torch.log(x) - torch.log(1.0 - x)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt: torch.Tensor,                 # (B, N, C)\n",
        "        ref_pts: torch.Tensor,             # (B, N, L, 2)\n",
        "        src: torch.Tensor,                 # (B, S, C)\n",
        "        query_pos: torch.Tensor,           # (B, N, C)\n",
        "        spatial_shapes: torch.Tensor,      # (L, 2)\n",
        "        lvl_start_idx: torch.Tensor,       # (L,)\n",
        "        self_attn_mask: Optional[torch.Tensor] = None  # (N, N) bool ya da (B*h, N, N)\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        x = tgt\n",
        "        q = self.norm1(x)\n",
        "        # ✔ MHA with mask for DN↔MATCH isolation\n",
        "        sa_out = self.self_attn(q, q, q, need_weights=False, attn_mask=self_attn_mask)[0]\n",
        "        x = x + self.dp1(self.ls1(self.drop1(sa_out)))\n",
        "\n",
        "        q = self.norm2(x)\n",
        "        ca_out = self.cross_attn(\n",
        "            query=q, value=src,\n",
        "            reference_points=ref_pts,\n",
        "            spatial_shapes=spatial_shapes,\n",
        "            level_start_index=lvl_start_idx,\n",
        "            query_pos=query_pos,\n",
        "        )\n",
        "        x = x + self.dp2(self.ls2(self.drop2(ca_out)))\n",
        "\n",
        "        y = self.ffn(self.norm3(x))\n",
        "        x = x + self.dp3(self.ls3(self.drop3(y)))\n",
        "\n",
        "        # tanh-bounded iterative ref update\n",
        "        delta = torch.tanh(self.ref_mlp(x)) * self.refine_scale      # (B,N,2)\n",
        "        new_ref = torch.sigmoid(self._inv_sigmoid(ref_pts) + delta.unsqueeze(2).to(ref_pts.dtype))\n",
        "\n",
        "        # tiny nudge\n",
        "        x = x + 1e-4 * delta.mean(dim=2, keepdim=True)\n",
        "        return x, new_ref\n",
        "\n",
        "\n",
        "class HeadPrep(nn.Module):\n",
        "    def __init__(self, d_model: int, ls_init: float = 0.80, ln_eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(d_model, eps=ln_eps)\n",
        "        self.ls   = LayerScale(d_model, init_values=ls_init)\n",
        "    def forward(self, x):  # x: (B, N, C)\n",
        "        return self.ls(self.norm(x))\n",
        "\n",
        "class BoxPosEnc(nn.Module):\n",
        "    \"\"\"Sin/cos box positional encoding for (cx, cy, w, h) in [0,1] -> (B,Q,C)\"\"\"\n",
        "    def __init__(self, c: int = 320, *, use_2pi: bool = True):\n",
        "        super().__init__()\n",
        "        assert c % 4 == 0, \"BoxPosEnc: c, 4'e bölünebilmeli\"\n",
        "        self.c = c\n",
        "        self.c_per = c // 4           # her (cx,cy,w,h) için kanal sayısı\n",
        "        assert self.c_per % 2 == 0, \"c/4 çift olmalı (sin+cos için)\"\n",
        "        half = self.c_per // 2\n",
        "        inv_freq = 10000 ** (-torch.arange(0, half).float() / max(1, half))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=True)\n",
        "        self.use_2pi = use_2pi\n",
        "\n",
        "    def _pe1d(self, v: torch.Tensor) -> torch.Tensor:\n",
        "        # v: (B,Q) -> (B,Q,c_per)\n",
        "        x = v.unsqueeze(-1)  # (B,Q,1)\n",
        "        if self.use_2pi:\n",
        "            x = x * (2.0 * math.pi)\n",
        "        x = x * self.inv_freq  # (B,Q,half)\n",
        "        return torch.cat([x.sin(), x.cos()], dim=-1)  # (B,Q,c_per)\n",
        "\n",
        "    def forward(self, boxes: torch.Tensor) -> torch.Tensor:\n",
        "        # boxes: (B,Q,4) in [0,1]\n",
        "        cx = self._pe1d(boxes[..., 0])\n",
        "        cy = self._pe1d(boxes[..., 1])\n",
        "        w  = self._pe1d(boxes[..., 2])\n",
        "        h  = self._pe1d(boxes[..., 3])\n",
        "        return torch.cat([cx, cy, w, h], dim=-1)  # (B,Q,C)\n",
        "\n",
        "class RTDeformDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    RT-Deform Decoder (  DN + Self-Attn Mask)\n",
        "      • Denoising (group-based POS/NEG, relative noise)\n",
        "      • DN ↔ MATCH self-attention isolation (attn mask)\n",
        "      • Proposal + MQS + learned query distribution (same as original API)\n",
        "      • No IoU‑aware header (use_iou_aware always False; API preserved)\n",
        "\n",
        "    Output dictionary:\n",
        "      - ‘pred_logits’, ‘pred_boxes’, ‘final_ref_pts’, 'query_selection_mask'\n",
        "      - in training: ‘aux_outputs’, ‘dn_meta’ (includes dn_len & dn_queries)\n",
        "      - optional: ‘pred_obj_logits’ (if train or emit_obj_logits_in_eval=True)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_obj_classes: int = 20,\n",
        "                 include_background: bool = True,\n",
        "                 *,\n",
        "                 num_queries: int = 300,\n",
        "                 num_layers: int = 4,\n",
        "                 d_model: int = 320,\n",
        "                 attn_n_heads: int = 10,\n",
        "                 dn_queries: int = 100,\n",
        "                 use_iou_aware: bool = False,\n",
        "                 iou_k_ratio: float = 0.75,\n",
        "\n",
        "                 # Encoder\n",
        "                 use_encoder: bool = True,\n",
        "                 encoder_layers: int = 1,\n",
        "                 encoder_drop_path_max: float = 0.1,\n",
        "\n",
        "                 # MQS\n",
        "                 mqs_enable: bool = True,\n",
        "                 mqs_obj_ratio: float = 0.30,\n",
        "                 mqs_grid_ratio: float = 0.20,\n",
        "                 mqs_levels: Tuple[int, ...] = (1, 2, 3),\n",
        "                 mqs_local_max_kernel: int = 3,\n",
        "                 mqs_train_only: bool = False,\n",
        "\n",
        "                 # Seed\n",
        "                 seed_enable: bool = True,\n",
        "                 seed_alpha_obj_init: float = 0.55,\n",
        "                 seed_alpha_grid_init: float = 0.4,\n",
        "                 seed_mlp_expansion: float = 1.0,\n",
        "\n",
        "                 # Two‑Stage Proposals\n",
        "                 use_proposals: bool = True,\n",
        "                 proposal_topk: Optional[int] = None,\n",
        "                 proposal_ratio: float = 0.70,\n",
        "                 min_mqs: int = 60,\n",
        "                 min_left_queries: int = 16,\n",
        "                 emit_obj_logits_in_eval: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        # --- Class/header parameters ---\n",
        "        self.num_obj_classes = num_obj_classes\n",
        "        self.include_background = include_background\n",
        "        self.num_pred = num_obj_classes + int(include_background)\n",
        "\n",
        "        self.num_queries = int(num_queries)\n",
        "        self.num_layers = int(num_layers)\n",
        "        self.dn_queries = int(dn_queries)\n",
        "\n",
        "        # IoU closed\n",
        "        self.use_iou_aware = False\n",
        "        self.emit_obj_logits_in_eval = bool(emit_obj_logits_in_eval)\n",
        "\n",
        "        self.d_model = int(d_model)\n",
        "        self.use_encoder = bool(use_encoder)\n",
        "        self.head_prep = HeadPrep(self.d_model, ls_init=0.80, ln_eps=1e-6)\n",
        "\n",
        "        # --- DN\n",
        "        self.dn_groups = 5\n",
        "        self.pos_box_noise = 0.4\n",
        "        self.neg_box_noise = 1.0\n",
        "        self.dn_box_pe = BoxPosEnc(self.d_model)  # ya da sin/cos box PE\n",
        "        self.dn_box_proj = nn.Linear(self.d_model, self.d_model)\n",
        "        self.dn_box_logit = nn.Parameter(torch.tensor(0.0))\n",
        "        nn.init.xavier_uniform_(self.dn_box_proj.weight)\n",
        "        nn.init.zeros_(self.dn_box_proj.bias)\n",
        "\n",
        "        # --- Proposal  ---\n",
        "        self.use_proposals = bool(use_proposals and use_encoder)\n",
        "        self.proposal_topk = proposal_topk\n",
        "        self.proposal_ratio = float(proposal_ratio)\n",
        "        self.min_mqs = int(min_mqs)\n",
        "        self.min_left_queries = int(min_left_queries)\n",
        "\n",
        "        # --- Embeddings  ---\n",
        "        self.level_embed_seed = nn.Parameter(torch.randn(4, self.d_model))\n",
        "        self.query_obj_head = nn.Linear(self.d_model, 1)  # objness (seed/proposal )\n",
        "\n",
        "        nn.init.xavier_uniform_(self.query_obj_head.weight)\n",
        "        nn.init.zeros_(self.query_obj_head.bias)\n",
        "\n",
        "        self.query_pos  = nn.Embedding(self.num_queries + self.dn_queries, self.d_model)\n",
        "        self.query_feat = nn.Embedding(self.num_queries + self.dn_queries, self.d_model)\n",
        "        self.label_enc  = nn.Embedding(self.num_obj_classes, self.d_model)\n",
        "\n",
        "        self.pos_embed   = Pos2d(self.d_model)\n",
        "        self.level_embed = nn.Parameter(torch.randn(4, self.d_model))\n",
        "\n",
        "        # DN noise  (label & box) – poz/neg\n",
        "        self.dn_label_noise_ratio = 0.5\n",
        "        self.dn_box_noise_scale   = 0.4  # (backward compatibility only; using relative noise)\n",
        "\n",
        "        # init ref(cx,cy)\n",
        "        self.ref_init = nn.Linear(self.d_model, 2)\n",
        "\n",
        "        # Encoder (opt)\n",
        "        if self.use_encoder:\n",
        "            self.encoder = TinyDeformEncoder(\n",
        "                num_layers=int(encoder_layers),\n",
        "                d_model=self.d_model, n_heads=int(attn_n_heads),\n",
        "                n_levels=4, n_points=4,\n",
        "                ffn_dim=self.d_model * 4,\n",
        "                drop=0.0,\n",
        "                drop_path_max=float(encoder_drop_path_max)\n",
        "            )\n",
        "\n",
        "        # Decoder\n",
        "        n_points_list = [4] * max(0, self.num_layers - 2) + [2, 2] if self.num_layers >= 2 else [4] * self.num_layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            DeformDecoderLayer(d_model=self.d_model,\n",
        "                               n_heads=int(attn_n_heads),\n",
        "                               n_levels=4,\n",
        "                               n_points=n_points_list[i] if i < len(n_points_list) else 4,\n",
        "                               ffn_dim=self.d_model * 4,\n",
        "                               refine_scale=(0.15, 0.45),\n",
        "                               layer_id=i, num_layers=self.num_layers)\n",
        "            for i in range(self.num_layers)\n",
        "        ])\n",
        "\n",
        "        # Heads\n",
        "        self.cls_head = nn.Linear(self.d_model, self.num_pred)\n",
        "        self.box_head = nn.Linear(self.d_model, 4)\n",
        "\n",
        "        self.aux_cls_heads = nn.ModuleList(\n",
        "            nn.Linear(self.d_model, self.num_pred) for _ in range(self.num_layers - 1))\n",
        "        self.aux_box_heads = nn.ModuleList(\n",
        "            nn.Linear(self.d_model, 4) for _ in range(self.num_layers - 1))\n",
        "\n",
        "        # MQS\n",
        "        self.mqs_enable = bool(mqs_enable)\n",
        "        self.mqs_obj_ratio = float(mqs_obj_ratio)\n",
        "        self.mqs_grid_ratio = float(mqs_grid_ratio)\n",
        "        self.mqs_levels = tuple(int(i) for i in mqs_levels)\n",
        "        self.mqs_local_max_kernel = int(mqs_local_max_kernel)\n",
        "        self.mqs_train_only = bool(mqs_train_only)\n",
        "\n",
        "        # Seed\n",
        "        self.seed_enable = bool(seed_enable)\n",
        "        hid = max(int(self.d_model * float(seed_mlp_expansion)), self.d_model)\n",
        "        self.seed_mlp = nn.Sequential(\n",
        "            nn.LayerNorm(self.d_model),\n",
        "            nn.Linear(self.d_model, hid),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hid, self.d_model)\n",
        "        )\n",
        "\n",
        "        def _logit(p: float) -> float:\n",
        "            p = min(max(float(p), 1e-4), 1.0 - 1e-4)\n",
        "            return math.log(p / (1.0 - p))\n",
        "\n",
        "        self.seed_logit_obj  = nn.Parameter(torch.tensor(_logit(float(seed_alpha_obj_init)), dtype=torch.float))\n",
        "        self.seed_logit_grid = nn.Parameter(torch.tensor(_logit(float(seed_alpha_grid_init)), dtype=torch.float))\n",
        "\n",
        "        # 2‑Stage Proposals (from encoder memory)\n",
        "        if self.use_proposals:\n",
        "            self.prop_obj_head = nn.Linear(self.d_model, 1)\n",
        "            self.prop_box_head = nn.Linear(self.d_model, 4)\n",
        "            self.prop_content_proj = nn.Sequential(\n",
        "                nn.LayerNorm(self.d_model),\n",
        "                nn.Linear(self.d_model, self.d_model),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(self.d_model, self.d_model)\n",
        "            )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    # ------------ init helpers ------------\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.query_pos.weight,  std=0.02)\n",
        "        nn.init.normal_(self.query_feat.weight, std=0.02)\n",
        "        nn.init.normal_(self.level_embed,       std=0.02)\n",
        "        nn.init.normal_(self.label_enc.weight,  std=0.02)\n",
        "        nn.init.normal_(self.level_embed_seed,  std=0.02)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.ref_init.weight)\n",
        "        nn.init.zeros_(self.ref_init.bias)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.box_head.weight, gain=1.0); nn.init.zeros_(self.box_head.bias)\n",
        "        nn.init.xavier_uniform_(self.cls_head.weight, gain=1.0)\n",
        "\n",
        "        for c, b in zip(self.aux_cls_heads, self.aux_box_heads):\n",
        "            nn.init.xavier_uniform_(c.weight, gain=1.0)\n",
        "            nn.init.xavier_uniform_(b.weight, gain=1.0); nn.init.zeros_(b.bias)\n",
        "\n",
        "        for m in self.seed_mlp:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "        if self.use_proposals:\n",
        "            nn.init.xavier_uniform_(self.prop_obj_head.weight)\n",
        "            nn.init.constant_(self.prop_obj_head.bias, -2.0)\n",
        "            nn.init.xavier_uniform_(self.prop_box_head.weight)\n",
        "            nn.init.zeros_(self.prop_box_head.bias)\n",
        "            for m in self.prop_content_proj:\n",
        "                if isinstance(m, nn.Linear):\n",
        "                    nn.init.xavier_uniform_(m.weight)\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "        self.reset_class_bias()\n",
        "\n",
        "    def _add_level_embed(self, seed: torch.Tensor, lvls: torch.Tensor) -> torch.Tensor:\n",
        "        lvls_clamped = lvls.clamp(0, self.level_embed_seed.size(0) - 1)\n",
        "        emb = F.embedding(lvls_clamped, self.level_embed_seed)  # (B,K,C) ya da (K,C)\n",
        "        emb = emb.to(dtype=seed.dtype, device=seed.device)\n",
        "        return seed + emb\n",
        "\n",
        "    def reset_class_bias(self, object_prior: float = .2, no_object_bias: float = -2.):\n",
        "        obj_bias = -math.log((1. - float(object_prior)) / float(object_prior))\n",
        "        with torch.no_grad():\n",
        "            self.cls_head.bias[:] = 0.\n",
        "            self.cls_head.bias[: self.num_obj_classes] = obj_bias\n",
        "            if self.include_background:\n",
        "                self.cls_head.bias[-1] = float(no_object_bias)\n",
        "            for aux in self.aux_cls_heads:\n",
        "                aux.bias[:] = self.cls_head.bias\n",
        "\n",
        "    # ------------ MQS helpers ------------\n",
        "    @staticmethod\n",
        "    def _allocate_per_level(k_total: int, shapes: List[Tuple[int, int]], sel_levels: Tuple[int, ...]) -> List[int]:\n",
        "        L = len(shapes)\n",
        "        if k_total <= 0:\n",
        "            return [0] * L\n",
        "        areas = [shapes[i][0] * shapes[i][1] if i in sel_levels else 0 for i in range(L)]\n",
        "        S = sum(areas)\n",
        "        if S == 0:\n",
        "            out = [0] * L\n",
        "            if sel_levels:\n",
        "                out[sel_levels[-1]] = k_total\n",
        "            return out\n",
        "        floats = [k_total * (a / S) for a in areas]\n",
        "        floors = [int(math.floor(x)) for x in floats]\n",
        "        remain = k_total - sum(floors)\n",
        "        fracs = [(floats[i] - floors[i]) if areas[i] > 0 else -1 for i in range(L)]\n",
        "        order = sorted([i for i in range(L) if areas[i] > 0], key=lambda i: fracs[i], reverse=True)\n",
        "        for i in range(remain):\n",
        "            floors[order[i % len(order)]] += 1\n",
        "        return [floors[i] if i in sel_levels else 0 for i in range(L)]\n",
        "\n",
        "    @staticmethod\n",
        "    def _energy_map(feat: torch.Tensor) -> torch.Tensor:\n",
        "        return F.relu(feat, inplace=False).mean(dim=1, keepdim=True)\n",
        "\n",
        "    def _mqs_objectness_points(self, feats: List[torch.Tensor], k_obj: int, sel_lvls: Tuple[int, ...]):\n",
        "        B = feats[0].size(0)\n",
        "        device = feats[0].device\n",
        "        if k_obj <= 0:\n",
        "            return torch.zeros(B, 0, 2, device=device), torch.zeros(B, 0, dtype=torch.long, device=device)\n",
        "\n",
        "        shapes = [(f.shape[2], f.shape[3]) for f in feats]  # (H,W)\n",
        "        per_level = self._allocate_per_level(k_obj, shapes, sel_lvls)\n",
        "\n",
        "        coords_list, lvl_list = [], []\n",
        "        ksize = self.mqs_local_max_kernel; pad = ksize // 2\n",
        "        with torch.no_grad():\n",
        "            for li, k_l in enumerate(per_level):\n",
        "                if k_l <= 0:\n",
        "                    continue\n",
        "                f = feats[li]\n",
        "                B_, C, H, W = f.shape\n",
        "                en = self._energy_map(f)\n",
        "                if ksize >= 3:\n",
        "                    mp = F.max_pool2d(en, kernel_size=ksize, stride=1, padding=pad)\n",
        "                    keep = (en >= mp - 1e-6)\n",
        "                    en = en * keep\n",
        "                en_flat = en.view(B, -1)\n",
        "                k_sel = min(k_l, H * W)\n",
        "                vals, idxs = torch.topk(en_flat, k_sel, dim=1)\n",
        "                ys = (idxs // W).float() + 0.5\n",
        "                xs = (idxs %  W).float() + 0.5\n",
        "                cx = (xs / W).clamp(0., 1.)\n",
        "                cy = (ys / H).clamp(0., 1.)\n",
        "                coords = torch.stack([cx, cy], dim=-1)\n",
        "                coords_list.append(coords)\n",
        "                lvl_list.append(torch.full((B, k_sel), li, dtype=torch.long, device=device))\n",
        "        if not coords_list:\n",
        "            return torch.zeros(B, 0, 2, device=device), torch.zeros(B, 0, dtype=torch.long, device=device)\n",
        "        coords_cat = torch.cat(coords_list, dim=1)\n",
        "        lvls_cat  = torch.cat(lvl_list,   dim=1)\n",
        "        return coords_cat, lvls_cat\n",
        "\n",
        "    def _mqs_grid_points(self, spatial_shapes: torch.Tensor, k_grid: int, sel_lvls: Tuple[int, ...]):\n",
        "        device = spatial_shapes.device\n",
        "        if k_grid <= 0:\n",
        "            return torch.zeros(1, 0, 2, device=device), torch.zeros(1, 0, dtype=torch.long, device=device)\n",
        "        shapes: List[Tuple[int,int]] = [tuple(map(int, s.tolist())) for s in spatial_shapes]\n",
        "        per_level = self._allocate_per_level(k_grid, shapes, sel_lvls)\n",
        "        pts, lvls = [], []\n",
        "        for li, k_l in enumerate(per_level):\n",
        "            if k_l <= 0: continue\n",
        "            H, W = shapes[li]\n",
        "            ratio = W / max(H, 1)\n",
        "            nx = max(1, int(round(math.sqrt(max(k_l,1) * ratio))))\n",
        "            ny = max(1, int(math.ceil(k_l / nx)))\n",
        "            xs = torch.linspace(0.5 / W, 1. - 0.5 / W, steps=nx, device=device)\n",
        "            ys = torch.linspace(0.5 / H, 1. - 0.5 / H, steps=ny, device=device)\n",
        "            grid_y, grid_x = torch.meshgrid(ys, xs, indexing=\"ij\")\n",
        "            grid = torch.stack([grid_x, grid_y], dim=-1).reshape(-1, 2)[:k_l]\n",
        "            pts.append(grid); lvls.append(torch.full((k_l,), li, dtype=torch.long, device=device))\n",
        "        if not pts:\n",
        "            return torch.zeros(1, 0, 2, device=device), torch.zeros(1, 0, dtype=torch.long, device=device)\n",
        "        pts = torch.cat(pts, dim=0).unsqueeze(0)\n",
        "        lvls = torch.cat(lvls, dim=0).unsqueeze(0)\n",
        "        return pts, lvls\n",
        "\n",
        "    def _to_grid_xy(self, coords: torch.Tensor) -> torch.Tensor:\n",
        "        if coords.dim() == 2:\n",
        "            coords = coords.unsqueeze(0)\n",
        "        gx = coords[..., 0] * 2.0 - 1.0\n",
        "        gy = coords[..., 1] * 2.0 - 1.0\n",
        "        grid = torch.stack([gx, gy], dim=-1)\n",
        "        return grid.unsqueeze(1)  # (B,1,K,2)\n",
        "\n",
        "    def _sample_feats_at_points(self, feats: List[torch.Tensor], pts: torch.Tensor, lvls: torch.Tensor) -> torch.Tensor:\n",
        "        B = feats[0].size(0); C = feats[0].size(1); K = pts.size(1); device = pts.device\n",
        "        if K == 0:\n",
        "            return torch.zeros(B, 0, C, device=device, dtype=feats[0].dtype)\n",
        "        out = torch.zeros(B, K, C, device=device, dtype=feats[0].dtype)\n",
        "        for b in range(B):\n",
        "            for li, f in enumerate(feats):\n",
        "                mask = (lvls[b] == li)\n",
        "                k_b = int(mask.sum().item())\n",
        "                if k_b == 0: continue\n",
        "                idx = torch.nonzero(mask, as_tuple=False).squeeze(-1)\n",
        "                coords_b = pts[b, idx, :]\n",
        "                grid_b = self._to_grid_xy(coords_b)\n",
        "                grid_b = grid_b.to(device=f.device, dtype=f.dtype)\n",
        "                f_b = f[b:b+1]\n",
        "                samp = F.grid_sample(f_b, grid_b, mode=\"bilinear\", align_corners=False)\n",
        "                samp = samp.reshape(C, k_b).permute(1, 0).contiguous()\n",
        "                out[b, idx, :] = samp\n",
        "        return out\n",
        "\n",
        "    # ------------ DN prep  ------------\n",
        "    def prepare_denoising(\n",
        "        self,\n",
        "        targets: List[Dict],\n",
        "        dn_groups: Optional[int] = None,\n",
        "        pos_box_noise: Optional[float] = None,\n",
        "        neg_box_noise: Optional[float] = None,\n",
        "        label_noise_ratio: Optional[float] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Creates DINO-style denoising:\n",
        "          • dn_groups groups; for each group, copies of POS (small deviation, correct/half-wrong label)\n",
        "            and NEG (large deviation and/or wrong label).\n",
        "          • Total DN length is fixed throughout the batch (Q):\n",
        "                Q = min(self.dn_queries, 2 * dn_groups * max(1, max(#GT)))\n",
        "          • Valid samples are marked with ‘mask’ (if GT=0, all masks are False).\n",
        "        Return: dict {in_boxes, in_labels, tgt_boxes, tgt_labels, mask}\n",
        "        \"\"\"\n",
        "        if not self.training or self.dn_queries == 0:\n",
        "            return None\n",
        "\n",
        "        device = targets[0][\"boxes\"].device\n",
        "        B = len(targets)\n",
        "        g_counts = [int(t[\"boxes\"].size(0)) for t in targets]\n",
        "        if sum(g_counts) == 0:\n",
        "            return None\n",
        "\n",
        "        dn_groups = self.dn_groups if dn_groups is None else int(dn_groups)\n",
        "        pos_noise = self.pos_box_noise if pos_box_noise is None else float(pos_box_noise)\n",
        "        neg_noise = self.neg_box_noise if neg_box_noise is None else float(neg_box_noise)\n",
        "        lbl_noise = self.dn_label_noise_ratio if label_noise_ratio is None else float(label_noise_ratio)\n",
        "\n",
        "        max_g = max(1, max(g_counts))\n",
        "        raw_total   = min(self.dn_queries, 2 * dn_groups * max_g)\n",
        "        q_per_group = max(1, math.ceil(raw_total / (2 * dn_groups)))\n",
        "        total_q     = min(self.dn_queries, 2 * dn_groups * q_per_group)\n",
        "        if total_q <= 0:\n",
        "            return None\n",
        "\n",
        "        dn_in_boxes  = torch.zeros(B, total_q, 4, device=device)\n",
        "        dn_in_labels = torch.zeros(B, total_q,   dtype=torch.long, device=device)\n",
        "        dn_tgt_boxes = torch.zeros(B, total_q, 4, device=device)\n",
        "        dn_tgt_labels= torch.zeros(B, total_q,   dtype=torch.long, device=device)\n",
        "        dn_mask      = torch.zeros(B, total_q,   dtype=torch.bool, device=device)\n",
        "        dn_pos_mask  = torch.zeros(B, total_q,   dtype=torch.bool, device=device)\n",
        "\n",
        "        q_per_group = max(1, total_q // (2 * dn_groups))\n",
        "\n",
        "        for b, tgt in enumerate(targets):\n",
        "            g = int(tgt[\"boxes\"].size(0))\n",
        "            if g == 0:\n",
        "                continue\n",
        "\n",
        "            boxes_gt  = tgt[\"boxes\"]   # (g,4) cx,cy,w,h ∈ [0,1]\n",
        "            labels_gt = tgt[\"labels\"]  # (g,)\n",
        "\n",
        "\n",
        "            rep = (q_per_group // max(1, g)) + 1\n",
        "            base_boxes  = boxes_gt.repeat(rep, 1)[:q_per_group]\n",
        "            base_labels = labels_gt.repeat(rep)[:q_per_group]\n",
        "\n",
        "            # --- POS (relatively small deviation) ---\n",
        "\n",
        "            pos_boxes = base_boxes.clone()\n",
        "            pos_boxes[:, :2] += (torch.rand_like(pos_boxes[:, :2]) - 0.5) * pos_noise\n",
        "            scale = 1.0 + (torch.rand_like(pos_boxes[:, 2:]) - 0.5) * pos_noise\n",
        "            pos_boxes[:, 2:] = (pos_boxes[:, 2:] * scale).clamp(1e-4, 1.0)\n",
        "            pos_boxes[:, :2] = pos_boxes[:, :2].clamp(0.0, 1.0)\n",
        "\n",
        "            pos_labels = base_labels.clone()\n",
        "            m = (torch.rand_like(pos_labels.float()) < lbl_noise)\n",
        "            if int(m.sum()) > 0:\n",
        "                pos_labels[m] = torch.randint(0, self.num_obj_classes, (int(m.sum()),), device=device)\n",
        "\n",
        "            # --- NEG  (aggressive deviation + mostly incorrect classification) ---\n",
        "            neg_boxes = base_boxes.clone()\n",
        "            neg_boxes[:, :2] = torch.rand_like(neg_boxes[:, :2])\n",
        "            scale = 0.5 + torch.rand_like(neg_boxes[:, 2:])\n",
        "            neg_boxes[:, 2:] = (neg_boxes[:, 2:] * scale).clamp(1e-4, 1.0)\n",
        "\n",
        "            neg_labels = base_labels.clone()\n",
        "            m = (torch.rand_like(neg_labels.float()) < 1.0)\n",
        "            if int(m.sum()) > 0:\n",
        "                wrong = torch.randint(0, self.num_obj_classes, (int(m.sum()),), device=device)\n",
        "                same = (wrong == base_labels[m])\n",
        "                if int(same.sum()) > 0:\n",
        "                    wrong[same] = (wrong[same] + 1) % self.num_obj_classes\n",
        "                neg_labels[m] = wrong\n",
        "\n",
        "\n",
        "            for gi in range(dn_groups):\n",
        "                pos_base = 2 * gi * q_per_group\n",
        "                neg_base = (2 * gi + 1) * q_per_group\n",
        "\n",
        "                dn_in_boxes[b,  pos_base:pos_base+q_per_group]  = pos_boxes\n",
        "                dn_in_labels[b, pos_base:pos_base+q_per_group]  = pos_labels\n",
        "                dn_tgt_boxes[b, pos_base:pos_base+q_per_group]  = base_boxes\n",
        "                dn_tgt_labels[b,pos_base:pos_base+q_per_group]  = base_labels\n",
        "                dn_mask[b,      pos_base:pos_base+q_per_group]  = True\n",
        "\n",
        "                dn_pos_mask[b,  pos_base:pos_base+q_per_group]  = True\n",
        "\n",
        "                dn_in_boxes[b,  neg_base:neg_base+q_per_group]  = neg_boxes\n",
        "                dn_in_labels[b, neg_base:neg_base+q_per_group]  = neg_labels\n",
        "                dn_tgt_boxes[b, neg_base:neg_base+q_per_group]  = base_boxes\n",
        "                dn_tgt_labels[b,neg_base:neg_base+q_per_group]  = base_labels\n",
        "                dn_mask[b,      neg_base:neg_base+q_per_group]  = True\n",
        "\n",
        "        return {\n",
        "          \"in_boxes\":   dn_in_boxes,\n",
        "          \"in_labels\":  dn_in_labels,\n",
        "          \"tgt_boxes\":  dn_tgt_boxes,\n",
        "          \"tgt_labels\": dn_tgt_labels,\n",
        "          \"mask\":       dn_mask,       # cls : POS+NEG\n",
        "          \"pos_mask\":   dn_pos_mask,   # bbox : only POS\n",
        "      }\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_self_attn_mask(dn_len, total_len, device, dn_groups=None):\n",
        "        if dn_len <= 0: return None\n",
        "        m = torch.zeros(total_len, total_len, dtype=torch.bool, device=device)\n",
        "        m[:dn_len, dn_len:] = True; m[dn_len:, :dn_len] = True\n",
        "        if dn_groups and dn_groups > 1:\n",
        "            qg = dn_len // dn_groups\n",
        "            for g in range(dn_groups):\n",
        "                s, e = g*qg, (g+1)*qg\n",
        "                m[s:e, :s] = True; m[s:e, e:dn_len] = True  # intergroup DN closed\n",
        "        return m\n",
        "\n",
        "    # ------------ forward ------------\n",
        "    def forward(self,\n",
        "                p3: torch.Tensor, p4: torch.Tensor,\n",
        "                p5: torch.Tensor, p6: torch.Tensor,\n",
        "                targets: Optional[List[Dict]] = None\n",
        "                ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        B = p3.size(0)\n",
        "        device = p3.device\n",
        "        feats = [p6, p5, p4, p3]\n",
        "        L = len(feats)\n",
        "\n",
        "        # -------------------- feature flatten + pos --------------------\n",
        "        src_list, pos_list = [], []\n",
        "        spatial_shapes = torch.zeros(L, 2, dtype=torch.long, device=device)\n",
        "\n",
        "        for i, f in enumerate(feats):\n",
        "            B_, C, H, W = f.shape\n",
        "            pos_lvl = self.pos_embed(f).expand(B_, -1, -1, -1) + self.level_embed[i].view(1, -1, 1, 1)\n",
        "            src_list.append(f.flatten(2).transpose(1, 2))          # (B, H*W, C)\n",
        "            pos_list.append(pos_lvl.flatten(2).transpose(1, 2))    # (B, H*W, C)\n",
        "            spatial_shapes[i, 0] = H\n",
        "            spatial_shapes[i, 1] = W\n",
        "\n",
        "        src = torch.cat(src_list, dim=1)                           # (B, S, C)\n",
        "        pos = torch.cat(pos_list, dim=1).to(dtype=src.dtype)       # (B, S, C)\n",
        "\n",
        "        numel_per_level = spatial_shapes[:, 0] * spatial_shapes[:, 1]\n",
        "        lvl_start_idx = torch.cat([numel_per_level.new_zeros(1), numel_per_level.cumsum(0)[:-1]], dim=0)  # (L,)\n",
        "\n",
        "        # -------------------- encoder --------------------\n",
        "        memory = self.encoder(src, pos, spatial_shapes, lvl_start_idx) if self.use_encoder else src\n",
        "        if pos.size(0) != memory.size(0):\n",
        "            pos = pos.expand(memory.size(0), -1, -1)\n",
        "        pos = pos.to(dtype=memory.dtype)\n",
        "\n",
        "        # -------------------- Denoising (DINO style) --------------------\n",
        "        dn = self.prepare_denoising(targets) if (self.training and self.dn_queries > 0) else None\n",
        "        has_dn = (dn is not None)\n",
        "        dn_len = int(dn[\"in_boxes\"].shape[1]) if has_dn else 0\n",
        "\n",
        "\n",
        "        produce_obj_logits = self.training or self.emit_obj_logits_in_eval\n",
        "\n",
        "        # -------------------- Query  (MATCH) --------------------\n",
        "        K_total = int(self.num_queries)\n",
        "        S = int(memory.size(1))\n",
        "\n",
        "        proposal_topk  = self.proposal_topk\n",
        "        proposal_ratio = self.proposal_ratio\n",
        "        min_mqs        = int(self.min_mqs)\n",
        "        min_left       = int(self.min_left_queries)\n",
        "        mqs_enabled    = self.mqs_enable and (not self.mqs_train_only or self.training)\n",
        "\n",
        "        # 1) Proposals\n",
        "        Kp_cap = K_total\n",
        "        if proposal_topk is not None:\n",
        "            Kp_cap = min(Kp_cap, int(proposal_topk))\n",
        "        if proposal_ratio is not None:\n",
        "            Kp_cap = min(Kp_cap, int(round(K_total * float(proposal_ratio))))\n",
        "        Kp_max = max(0, K_total - (min_mqs + min_left))\n",
        "        Kp = min(Kp_cap, Kp_max, S) if self.use_proposals else 0\n",
        "\n",
        "        # 2) remain\n",
        "        K_rem = K_total - Kp\n",
        "\n",
        "        # 3) MQS\n",
        "        K_mqs = max(0, min(min_mqs, K_rem - min_left)) if mqs_enabled else 0\n",
        "        tot = float(self.mqs_obj_ratio) + float(self.mqs_grid_ratio) + 1e-9\n",
        "        k_obj  = int(round(K_mqs * float(self.mqs_obj_ratio) / tot)) if K_mqs > 0 else 0\n",
        "        k_grid = K_mqs - k_obj\n",
        "\n",
        "        # 4) Learned\n",
        "        k_left = max(0, K_rem - (k_obj + k_grid))\n",
        "        assert (Kp + k_obj + k_grid + k_left) == K_total\n",
        "\n",
        "        def bgather(t: torch.Tensor, idx: torch.Tensor) -> torch.Tensor:\n",
        "            idx_exp = idx.unsqueeze(-1).expand(-1, -1, t.size(-1))\n",
        "            return torch.gather(t, 1, idx_exp)\n",
        "\n",
        "        def levels_from_indices(idx: torch.Tensor,\n",
        "                                spatial_shapes: torch.Tensor,\n",
        "                                lvl_start_idx: torch.Tensor) -> torch.Tensor:\n",
        "            starts = lvl_start_idx.view(1, 1, -1)\n",
        "            sizes  = (spatial_shapes[:, 0] * spatial_shapes[:, 1]).view(1, 1, -1)\n",
        "            ends   = starts + sizes\n",
        "            x = idx.unsqueeze(-1)\n",
        "            mask = (x >= starts) & (x < ends)\n",
        "            lvls = mask.long().argmax(dim=-1)\n",
        "            return lvls\n",
        "\n",
        "        # --- Proposals ---\n",
        "        content_prop = pos_prop = ref_prop = None\n",
        "        obj_logits_prop = None\n",
        "        if Kp > 0:\n",
        "            prop_scores = self.prop_obj_head(memory).squeeze(-1)      # (B,S)\n",
        "            prop_boxes  = self.prop_box_head(memory).sigmoid()        # (B,S,4)\n",
        "            scores_sel, idx_sel = torch.topk(prop_scores, Kp, dim=1)  # (B,Kp)\n",
        "            feat_sel = bgather(memory, idx_sel)                       # (B,Kp,C)\n",
        "            pos_sel  = bgather(pos,    idx_sel)                       # (B,Kp,C)\n",
        "            box_sel  = bgather(prop_boxes, idx_sel)                   # (B,Kp,4)\n",
        "\n",
        "            ref_sel  = box_sel[..., :2]\n",
        "            ref_prop = ref_sel.unsqueeze(2).expand(-1, -1, 4, -1)\n",
        "\n",
        "            lvls_sel = levels_from_indices(idx_sel, spatial_shapes, lvl_start_idx)\n",
        "            content_prop = self.prop_content_proj(feat_sel)\n",
        "            content_prop = self._add_level_embed(content_prop, lvls_sel)\n",
        "            pos_prop     = pos_sel\n",
        "\n",
        "            if produce_obj_logits:\n",
        "                obj_logits_prop = scores_sel\n",
        "\n",
        "        # --- MQS (obj+grid) / Seed ---\n",
        "        content_obj = content_grid = None\n",
        "        pos_obj = pos_grid = None\n",
        "        ref_obj = ref_grid = None\n",
        "        obj_logits_obj = obj_logits_grid = None\n",
        "\n",
        "        if (k_obj + k_grid) > 0:\n",
        "            obj_pts = obj_lvls = None\n",
        "            if k_obj > 0:\n",
        "                obj_pts, obj_lvls = self._mqs_objectness_points(feats, k_obj, self.mqs_levels)\n",
        "\n",
        "            grid_pts = grid_lvls = None\n",
        "            if k_grid > 0:\n",
        "                grid_pts_1, grid_lvls_1 = self._mqs_grid_points(spatial_shapes, k_grid, self.mqs_levels)\n",
        "                grid_pts  = grid_pts_1.repeat(B, 1, 1).to(device)\n",
        "                grid_lvls = grid_lvls_1.repeat(B, 1).to(device)\n",
        "\n",
        "            if self.seed_enable:\n",
        "                if k_obj > 0:\n",
        "                    obj_seed = self._sample_feats_at_points(feats, obj_pts, obj_lvls)\n",
        "                    if obj_seed.numel() > 0:\n",
        "                        obj_seed = self._add_level_embed(obj_seed, obj_lvls)\n",
        "                    alpha_obj  = torch.sigmoid(self.seed_logit_obj)\n",
        "                    obj_seed_m = self.seed_mlp(obj_seed) if (obj_seed is not None and obj_seed.numel() > 0) else None\n",
        "                    base_obj = self.query_feat.weight[self.dn_queries : self.dn_queries + k_obj] \\\n",
        "                    .unsqueeze(0).expand(B, -1, -1)\n",
        "                    content_obj = (1.0 - alpha_obj) * base_obj + alpha_obj * obj_seed_m\n",
        "                    pos_obj  = self.query_pos.weight[self.dn_queries : self.dn_queries + k_obj] \\\n",
        "              .unsqueeze(0).expand(B, -1, -1)\n",
        "                    ref_obj     = obj_pts.unsqueeze(2).expand(-1, -1, 4, -1)\n",
        "                    if produce_obj_logits:\n",
        "                        obj_logits_obj = self.query_obj_head(content_obj).squeeze(-1)\n",
        "\n",
        "                if k_grid > 0:\n",
        "                    grid_seed = self._sample_feats_at_points(feats, grid_pts, grid_lvls)\n",
        "                    if grid_seed.numel() > 0:\n",
        "                        grid_seed = self._add_level_embed(grid_seed, grid_lvls)\n",
        "                    alpha_grid  = torch.sigmoid(self.seed_logit_grid)\n",
        "                    grid_seed_m = self.seed_mlp(grid_seed) if (grid_seed is not None and grid_seed.numel() > 0) else None\n",
        "                    base_grid = self.query_feat.weight[self.dn_queries + k_obj : self.dn_queries + k_obj + k_grid] \\\n",
        "               .unsqueeze(0).expand(B, -1, -1)\n",
        "                    content_grid = (1.0 - alpha_grid) * base_grid + alpha_grid * grid_seed_m\n",
        "                    pos_grid  = self.query_pos.weight[self.dn_queries + k_obj : self.dn_queries + k_obj + k_grid] \\\n",
        "               .unsqueeze(0).expand(B, -1, -1)\n",
        "                    ref_grid     = grid_pts.unsqueeze(2).expand(-1, -1, 4, -1)\n",
        "                    if produce_obj_logits:\n",
        "                        obj_logits_grid = self.query_obj_head(content_grid).squeeze(-1)\n",
        "\n",
        "        # --- Learned remain ---\n",
        "        content_left = pos_left = ref_left = None\n",
        "        obj_logits_left = None\n",
        "        if k_left > 0:\n",
        "            base_s   = self.dn_queries + k_obj + k_grid\n",
        "            content_left = self.query_feat.weight[base_s : base_s + k_left] \\\n",
        "                 .unsqueeze(0).expand(B, -1, -1)\n",
        "            pos_left     = self.query_pos.weight[base_s : base_s + k_left] \\\n",
        "                 .unsqueeze(0).expand(B, -1, -1)\n",
        "            init_ref_left = torch.sigmoid(self.ref_init(pos_left))\n",
        "            ref_left = init_ref_left.unsqueeze(2).expand(-1, -1, 4, -1)\n",
        "            if produce_obj_logits:\n",
        "                obj_logits_left = self.query_obj_head(content_left).squeeze(-1)\n",
        "\n",
        "        # ---  combine MATCH set ---\n",
        "        def cat_safe(parts: list, dim: int) -> torch.Tensor:\n",
        "            valid = [p for p in parts if p is not None]\n",
        "            if len(valid) == 0:\n",
        "                raise RuntimeError(\"RTDeformDecoder: boş MATCH seti oluştu.\")\n",
        "            return torch.cat(valid, dim=dim)\n",
        "\n",
        "        parts_content = [content_prop, content_obj, content_grid, content_left]\n",
        "        parts_pos     = [pos_prop,     pos_obj,     pos_grid,     pos_left]\n",
        "        parts_ref     = [ref_prop,     ref_obj,     ref_grid,     ref_left]\n",
        "\n",
        "        content_match = cat_safe(parts_content, dim=1)   # (B, N_match, C)\n",
        "        pos_match     = cat_safe(parts_pos,     dim=1)   # (B, N_match, C)\n",
        "        ref_match     = cat_safe(parts_ref,     dim=1)   # (B, N_match, 4, 2)\n",
        "        content_match = content_match.contiguous()\n",
        "        pos_match     = pos_match.contiguous()\n",
        "        ref_match     = ref_match.contiguous()\n",
        "        obj_logits_parts = [obj_logits_prop, obj_logits_obj, obj_logits_grid, obj_logits_left]\n",
        "        obj_logits_match = torch.cat([p for p in obj_logits_parts if p is not None], dim=1) \\\n",
        "                           if (produce_obj_logits and any(p is not None for p in obj_logits_parts)) else None\n",
        "\n",
        "        # --------------------Combine with DN (prefix) --------------------\n",
        "        if has_dn:\n",
        "            box_pe    = self.dn_box_pe(dn[\"in_boxes\"].detach())\n",
        "            alpha     = torch.sigmoid(self.dn_box_logit)\n",
        "            content_dn = self.label_enc(dn[\"in_labels\"]) + alpha * self.dn_box_proj(box_pe)                               # (B, Q, C)\n",
        "            pos_dn     = self.query_pos.weight[:dn_len].unsqueeze(0).repeat(B, 1, 1)      # (B, Q, C)\n",
        "            ref_dn     = dn[\"in_boxes\"][..., :2].unsqueeze(2).expand(-1, -1, 4, -1).clone()  # (B, Q, 4, 2)\n",
        "\n",
        "            tgt       = torch.cat([content_dn, content_match], 1).contiguous()\n",
        "            query_pos = torch.cat([pos_dn,     pos_match],     1).contiguous()\n",
        "            ref_pts   = torch.cat([ref_dn,     ref_match],     1).contiguous()                              # (B, Q+N_match, 4, 2)\n",
        "        else:\n",
        "            dn_len = 0\n",
        "            tgt, query_pos, ref_pts = content_match, pos_match, ref_match\n",
        "\n",
        "        # -------------------- self-attention mask --------------------\n",
        "        total_len = tgt.size(1)\n",
        "        attn_mask = self._build_self_attn_mask(dn_len, total_len, tgt.device)\n",
        "\n",
        "        # -------------------- decoder --------------------\n",
        "        aux_out = []\n",
        "        for lid, layer in enumerate(self.layers):\n",
        "\n",
        "            try:\n",
        "                tgt, ref_pts = layer(\n",
        "                    tgt=tgt, ref_pts=ref_pts, src=memory,\n",
        "                    query_pos=query_pos, spatial_shapes=spatial_shapes, lvl_start_idx=lvl_start_idx,\n",
        "                    self_attn_mask=attn_mask\n",
        "                )\n",
        "            except TypeError:\n",
        "                tgt, ref_pts = layer(\n",
        "                    tgt=tgt, ref_pts=ref_pts, src=memory,\n",
        "                    query_pos=query_pos, spatial_shapes=spatial_shapes, lvl_start_idx=lvl_start_idx\n",
        "                )\n",
        "\n",
        "            if lid < self.num_layers - 1:\n",
        "                # Aux only from MATCH\n",
        "                aux_in = tgt[:, dn_len:] if dn_len > 0 else tgt\n",
        "                aux_h  = self.head_prep(aux_in)\n",
        "                aux_logits = self.aux_cls_heads[lid](aux_h)\n",
        "                aux_boxes  = self.aux_box_heads[lid](aux_h).sigmoid()\n",
        "                aux_out.append({\"pred_logits\": aux_logits, \"pred_boxes\": aux_boxes})\n",
        "\n",
        "        # -------------------- last heads --------------------\n",
        "        h = self.head_prep(tgt)\n",
        "        if dn_len > 0:\n",
        "            h_dn    = h[:, :dn_len]\n",
        "            h_match = h[:, dn_len:]\n",
        "            dn_logits    = self.cls_head(h_dn)\n",
        "            dn_boxes_out = self.box_head(h_dn).sigmoid()\n",
        "            logits = self.cls_head(h_match)\n",
        "            boxes  = self.box_head(h_match).sigmoid()\n",
        "        else:\n",
        "            logits = self.cls_head(h)\n",
        "            boxes  = self.box_head(h).sigmoid()\n",
        "\n",
        "        out: Dict[str, torch.Tensor] = {\"pred_logits\": logits, \"pred_boxes\": boxes}\n",
        "\n",
        "        if aux_out and self.training:\n",
        "            out[\"aux_outputs\"] = aux_out\n",
        "\n",
        "        if has_dn and self.training:\n",
        "\n",
        "            out[\"dn_meta\"] = {\n",
        "            \"dn_logits\":   dn_logits,\n",
        "            \"dn_boxes\":    dn_boxes_out,\n",
        "            \"dn_labels\":   dn[\"tgt_labels\"],\n",
        "            \"dn_gt_boxes\": dn[\"tgt_boxes\"],\n",
        "            \"dn_mask\":     dn[\"mask\"].bool(),\n",
        "            \"dn_pos_mask\": dn[\"pos_mask\"].bool(),   # <-- YENİ\n",
        "            \"dn_len\":      dn_len,\n",
        "            \"dn_queries\":  dn_len,\n",
        "          }\n",
        "\n",
        "        # MATCH‑only final_ref_pts (center loss vb. için)\n",
        "        ref_match_only = ref_pts[:, dn_len:, ...] if dn_len > 0 else ref_pts\n",
        "        N_match = out[\"pred_boxes\"].size(1)\n",
        "        if ref_match_only.size(1) != N_match:\n",
        "            ref_match_only = ref_match_only[:, -N_match:, ...]\n",
        "        out[\"final_ref_pts\"] = ref_match_only.contiguous()\n",
        "\n",
        "        # Obj logits (only MATCH part)\n",
        "        if produce_obj_logits and (obj_logits_match is not None):\n",
        "            if obj_logits_match.size(1) != N_match:\n",
        "                if obj_logits_match.size(1) > N_match:\n",
        "                    obj_logits_match = obj_logits_match[:, :N_match]\n",
        "            out[\"pred_obj_logits\"] = obj_logits_match.contiguous()\n",
        "\n",
        "        # All MATCH queries are “selected”\n",
        "        out[\"query_selection_mask\"] = torch.ones(B, logits.size(1), dtype=torch.bool, device=logits.device)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# MiniBackbone\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class MiniBackbone(nn.Module):\n",
        "    \"\"\"Lightweight version with CSI and PGI\"\"\"\n",
        "    def __init__(self, depths: List[int] = [6, 6, 12, 6],\n",
        "                drop_path_max: float = 0.2,\n",
        "                num_classes: int = 20):\n",
        "        super().__init__()\n",
        "        self.backbone = StageAwareBackbone(depths, drop_path_max, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, need_aux: bool = False):\n",
        "        # Match the return format of StageAwareBackbone\n",
        "        return self.backbone(x, need_aux)\n",
        "\n",
        "\n",
        "\n",
        "def _looks_like_norm_name(name: str) -> bool:\n",
        "    low = name.lower()\n",
        "    # name-based heuristic; used in conjunction with a module-based set\n",
        "    return any(k in low for k in [\n",
        "        \".norm\", \"bn\", \"groupnorm\", \"layernorm\", \"gn\", \"ln.\", \"ln_\", \"lnact\", \"ln_act\", \"lnproxy\", \"layernormproxy\", \"rmsnorm\"\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "class HybridDCDATRT(nn.Module):\n",
        "    \"\"\"\n",
        "    End‑to‑End Hybrid‑DCDAT‑RT detector (DAT‑backbone + DCNv4 + RT‑Deform Decoder)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_classes:   int = 20,\n",
        "                 num_queries:   int = 100,\n",
        "                 depths:        List[int] = (2, 2, 2, 2),\n",
        "                 drop_path_max: float = 0.0,\n",
        "                 backbone_norm_factory: 'NormFactory' = None,\n",
        "                 neck_norm_factory:     'NormFactory' = None,\n",
        "                 use_layer_scale: bool = True,\n",
        "                 layer_scale_init: float = 1.0,\n",
        "                 # Decoder\n",
        "                 d_model:    int = 320,\n",
        "                 dec_layers: int = 4,\n",
        "                 dn_queries: int = 100,\n",
        "                 emit_obj_logits_in_eval: bool = True,\n",
        "                 # Features\n",
        "                 use_aux_loss: bool = True,\n",
        "                 use_iou_aware: bool = False,\n",
        "                 # LR multipliers\n",
        "                 backbone_lr: float = 0.1,\n",
        "                 head_lr:     float = 1.0,\n",
        "                 # Dataset\n",
        "                 voc_prior: bool = False,\n",
        "                 # Decoder Encoder\n",
        "                 decoder_use_encoder: bool = True,\n",
        "                 decoder_encoder_layers: int = 2,\n",
        "                 decoder_encoder_drop_path_max: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # default NormFactory\n",
        "        backbone_norm_factory = backbone_norm_factory or NormFactory(\"gn\")\n",
        "        neck_norm_factory     = neck_norm_factory     or NormFactory(\"gn\")\n",
        "\n",
        "        self.num_classes   = num_classes\n",
        "        self.use_aux_loss  = use_aux_loss\n",
        "\n",
        "        # ---------- Backbone (DAT + CSI + PGI) ----------\n",
        "        self.backbone = StageAwareBackbone(\n",
        "            depths          = depths,\n",
        "            drop_path_max   = drop_path_max,\n",
        "            num_classes     = num_classes,\n",
        "            voc_prior       = voc_prior,\n",
        "            norm_factory    = backbone_norm_factory,\n",
        "            use_layer_scale = use_layer_scale,\n",
        "            layer_scale_init= layer_scale_init\n",
        "        )\n",
        "\n",
        "        # ---------- Neck (Light‑BiFPN) ----------\n",
        "\n",
        "        self.neck = LightBiFPN(\n",
        "          in_channels=(128,256,640,768),\n",
        "          out=d_model,\n",
        "          repeats=2,\n",
        "          use_spatial_fuse=True,\n",
        "          fuse_groups=4,\n",
        "          fuse_tau_init=1.4,\n",
        "          fuse_learn_tau=True,\n",
        "          fuse_eps=5e-4,\n",
        "          grad_boost_low=0.1,\n",
        "          dp_top_second=0.01, dp_bot_second=0.01,\n",
        "          no_blur_first=True,\n",
        "          edge_enhance=True,\n",
        "          edge_ls_init=0.03,\n",
        "          norm_factory=neck_norm_factory,\n",
        "      )\n",
        "\n",
        "        # ---------- Decoder (RT‑Deform) ----------\n",
        "        self.decoder = RTDeformDecoder(\n",
        "            num_obj_classes = num_classes,\n",
        "            include_background = True,\n",
        "            num_queries     = num_queries,\n",
        "            num_layers      = dec_layers,\n",
        "            d_model         = d_model,\n",
        "            dn_queries      = dn_queries,\n",
        "            use_iou_aware   = use_iou_aware,\n",
        "            use_encoder                 = decoder_use_encoder,\n",
        "            encoder_layers              = decoder_encoder_layers,\n",
        "            encoder_drop_path_max       = decoder_encoder_drop_path_max,\n",
        "            emit_obj_logits_in_eval=emit_obj_logits_in_eval,\n",
        "        )\n",
        "\n",
        "\n",
        "        self._lr_mult = {\n",
        "            \"backbone\": backbone_lr,\n",
        "            \"head\":     head_lr,\n",
        "            \"bias\":     2.0,\n",
        "            \"dcn_bias\": 10.0,\n",
        "        }\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                targets: Optional[List[Dict]] = None) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "         - ‘pred_logits’, ‘pred_boxes’, optional ‘pred_ious’\n",
        "          - in training: ‘aux_outputs’, ‘dn_meta’, ‘query_selection_mask’\n",
        "          - optional: ‘aux_dense’ (auxiliary dense headers from the backbone)\n",
        "        \"\"\"\n",
        "        if not x.is_cuda:\n",
        "            raise RuntimeError(\"HybridDCDATRT expects CUDA tensor input.\")\n",
        "\n",
        "        # Backbone\n",
        "        if self.training and self.use_aux_loss:\n",
        "            (p3, p4, p5, p6), aux_dense = self.backbone(x, need_aux=True)\n",
        "        else:\n",
        "            p3, p4, p5, p6 = self.backbone(x, need_aux=False)\n",
        "            aux_dense      = None\n",
        "\n",
        "        # Neck\n",
        "        p3, p4, p5, p6 = self.neck(p3, p4, p5, p6)\n",
        "\n",
        "        # Decoder\n",
        "        if self.training and targets is not None:\n",
        "            dec_out = self.decoder(p3, p4, p5, p6, targets)\n",
        "        else:\n",
        "            dec_out = self.decoder(p3, p4, p5, p6)\n",
        "\n",
        "        if aux_dense is not None:\n",
        "            dec_out[\"aux_dense\"] = aux_dense\n",
        "        return dec_out\n",
        "\n",
        "\n",
        "\n",
        "    def param_groups(self,\n",
        "                    base_lr: float = 1e-4,\n",
        "                    *,\n",
        "                    weight_decay: float = 0.02,\n",
        "                    bb_mult: float = 0.5,\n",
        "                    dec_mult: float = 2.0,\n",
        "                    bias_mult: float = 1.0,      # Bias boost\n",
        "                    dcn_mult: float = 1.5,       # DCN offset/mask boost\n",
        "                    rpb_mult: float = 5.0,       # RPB boost\n",
        "                    gate_mult: float = 1.0,      # Gate params boost\n",
        "                    ls_mult: float = 0.3,        # LayerScale reduction\n",
        "                    pos_mult: float = 1.5):\n",
        "        \"\"\"\n",
        "        Production-ready parameter grouping with optimized LR scheduling.\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        buckets = {\n",
        "            # Backbone\n",
        "            \"bb_w\": [], \"bb_b\": [], \"bb_norm\": [],\n",
        "            \"dcn_off\": [],  # DCN offset/mask (specific LR)\n",
        "\n",
        "            # Neck / Head\n",
        "            \"hd_w\": [], \"hd_b\": [], \"hd_norm\": [],\n",
        "            \"fuse_gate\": [],  # Fusion gates\n",
        "\n",
        "            # Decoder\n",
        "            \"dec_w\": [], \"dec_b\": [], \"dec_norm\": [],\n",
        "            \"deform_off_b\": [],  # Deformable attention bias\n",
        "\n",
        "            # Special\n",
        "            \"pos_embed\": [], \"rpb_p\": [], \"ls_gamma\": [],\n",
        "            \"dat_scale_p\": [], \"scalars_gain\": [],\n",
        "        }\n",
        "\n",
        "        def is_norm_name(n: str) -> bool:\n",
        "            return any(x in n for x in [\".norm.\", \".bn.\", \".ln.\", \".gn.\"])\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            if not param.requires_grad:\n",
        "                continue\n",
        "\n",
        "            # --- Special cases (prefix-independent) ---\n",
        "\n",
        "            # RPB tables (needs high LR)\n",
        "            if name.endswith(\".rpb\") or \"relative_position_bias\" in name:\n",
        "                buckets[\"rpb_p\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Positional embeddings\n",
        "            if re.search(r\"(level_embed(_seed)?|query_pos|query_feat|label_enc|pos_table)\", name):\n",
        "                buckets[\"pos_embed\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # LayerScale gamma\n",
        "            if re.search(r\"(layer_scales\\.\\d+\\.weight|\\.scale\\.weight$|\\.ls\\.weight$)\", name):\n",
        "                buckets[\"ls_gamma\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Scalar gates\n",
        "            if any(name.endswith(x) for x in [\"residual_scale\", \"_lambda_logit\", \"dc_res_logit\", \"seed_logit_obj\", \"seed_logit_grid\"]):\n",
        "                buckets[\"scalars_gain\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # DAT scale parameters\n",
        "            if \"dat_logit\" in name or \"dat_log_tau\" in name:\n",
        "                buckets[\"dat_scale_p\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Neck fusion gates\n",
        "            if \"neck.fuse\" in name and \"log_tau\" in name:\n",
        "                buckets[\"fuse_gate\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Deformable attention sampling bias\n",
        "            if \"sampling_offsets.bias\" in name:\n",
        "                buckets[\"deform_off_b\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # --- Module-based categorization ---\n",
        "\n",
        "            # Backbone\n",
        "            if name.startswith(\"backbone.\"):\n",
        "                # DCN offset/mask (special treatment)\n",
        "                if \"offset_mask\" in name and \"dcn\" in name:\n",
        "                    buckets[\"dcn_off\"].append(param)\n",
        "                elif name.endswith(\".bias\") and not is_norm_name(name):\n",
        "                    buckets[\"bb_b\"].append(param)\n",
        "                elif is_norm_name(name):\n",
        "                    buckets[\"bb_norm\"].append(param)\n",
        "                else:\n",
        "                    buckets[\"bb_w\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Neck\n",
        "            if name.startswith(\"neck.\"):\n",
        "                if name.endswith(\".bias\") and not is_norm_name(name):\n",
        "                    buckets[\"hd_b\"].append(param)\n",
        "                elif is_norm_name(name):\n",
        "                    buckets[\"hd_norm\"].append(param)\n",
        "                else:\n",
        "                    buckets[\"hd_w\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Decoder (default)\n",
        "            if name.endswith(\".bias\") and not is_norm_name(name):\n",
        "                buckets[\"dec_b\"].append(param)\n",
        "            elif is_norm_name(name):\n",
        "                buckets[\"dec_norm\"].append(param)\n",
        "            else:\n",
        "                buckets[\"dec_w\"].append(param)\n",
        "\n",
        "        # --- Build parameter groups ---\n",
        "        wd = weight_decay\n",
        "        groups = []\n",
        "\n",
        "        # Backbone groups\n",
        "        if buckets[\"bb_w\"]:\n",
        "            groups.append({\"params\": buckets[\"bb_w\"], \"lr\": base_lr*bb_mult, \"weight_decay\": wd, \"name\": \"bb_w\"})\n",
        "        if buckets[\"bb_b\"]:\n",
        "            groups.append({\"params\": buckets[\"bb_b\"], \"lr\": base_lr*bb_mult*bias_mult, \"weight_decay\": 0.0, \"name\": \"bb_b\"})\n",
        "        if buckets[\"bb_norm\"]:\n",
        "            groups.append({\"params\": buckets[\"bb_norm\"], \"lr\": base_lr*bb_mult, \"weight_decay\": 0.0, \"name\": \"bb_norm\"})\n",
        "\n",
        "        # DCN offset/mask (HIGH LR!)\n",
        "        if buckets[\"dcn_off\"]:\n",
        "            groups.append({\"params\": buckets[\"dcn_off\"], \"lr\": base_lr*dcn_mult, \"weight_decay\": 0.0, \"name\": \"dcn_off\"})\n",
        "\n",
        "        # Neck/Head groups\n",
        "        if buckets[\"hd_w\"]:\n",
        "            groups.append({\"params\": buckets[\"hd_w\"], \"lr\": base_lr, \"weight_decay\": wd, \"name\": \"hd_w\"})\n",
        "        if buckets[\"hd_b\"]:\n",
        "            groups.append({\"params\": buckets[\"hd_b\"], \"lr\": base_lr*bias_mult, \"weight_decay\": 0.0, \"name\": \"hd_b\"})\n",
        "        if buckets[\"hd_norm\"]:\n",
        "            groups.append({\"params\": buckets[\"hd_norm\"], \"lr\": base_lr, \"weight_decay\": 0.0, \"name\": \"hd_norm\"})\n",
        "\n",
        "        # Decoder groups\n",
        "        if buckets[\"dec_w\"]:\n",
        "            groups.append({\"params\": buckets[\"dec_w\"], \"lr\": base_lr*dec_mult, \"weight_decay\": wd, \"name\": \"dec_w\"})\n",
        "        if buckets[\"dec_b\"]:\n",
        "            groups.append({\"params\": buckets[\"dec_b\"], \"lr\": base_lr*dec_mult*bias_mult, \"weight_decay\": 0.0, \"name\": \"dec_b\"})\n",
        "        if buckets[\"dec_norm\"]:\n",
        "            groups.append({\"params\": buckets[\"dec_norm\"], \"lr\": base_lr*dec_mult, \"weight_decay\": 0.0, \"name\": \"dec_norm\"})\n",
        "\n",
        "        # Special parameters with custom LR\n",
        "        if buckets[\"deform_off_b\"]:\n",
        "            groups.append({\"params\": buckets[\"deform_off_b\"], \"lr\": base_lr*dcn_mult, \"weight_decay\": 0.0, \"name\": \"deform_off_b\"})\n",
        "        if buckets[\"fuse_gate\"]:\n",
        "            groups.append({\"params\": buckets[\"fuse_gate\"], \"lr\": base_lr*gate_mult, \"weight_decay\": 0.0, \"name\": \"fuse_gate\"})\n",
        "        if buckets[\"rpb_p\"]:\n",
        "            groups.append({\"params\": buckets[\"rpb_p\"], \"lr\": base_lr*rpb_mult, \"weight_decay\": 0.0, \"name\": \"rpb_p\"})\n",
        "        if buckets[\"pos_embed\"]:\n",
        "            groups.append({\"params\": buckets[\"pos_embed\"], \"lr\": base_lr*pos_mult, \"weight_decay\": 0.0, \"name\": \"pos_embed\"})\n",
        "        if buckets[\"ls_gamma\"]:\n",
        "            groups.append({\"params\": buckets[\"ls_gamma\"], \"lr\": base_lr*ls_mult, \"weight_decay\": 0.0, \"name\": \"ls_gamma\"})\n",
        "        if buckets[\"dat_scale_p\"]:\n",
        "            groups.append({\"params\": buckets[\"dat_scale_p\"], \"lr\": base_lr*gate_mult, \"weight_decay\": 0.0, \"name\": \"dat_scale_p\"})\n",
        "        if buckets[\"scalars_gain\"]:\n",
        "            groups.append({\"params\": buckets[\"scalars_gain\"], \"lr\": base_lr*gate_mult, \"weight_decay\": 0.0, \"name\": \"scalars_gain\"})\n",
        "\n",
        "        # Validation\n",
        "        in_groups = {id(p) for g in groups for p in g[\"params\"]}\n",
        "        missing = [n for n,p in self.named_parameters() if p.requires_grad and id(p) not in in_groups]\n",
        "        assert not missing, f\"Missing params in groups: {len(missing)} params (e.g., {missing[:5]})\"\n",
        "\n",
        "        # Debug logging (optional)\n",
        "        if not hasattr(self, '_param_groups_logged'):\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"Parameter Groups Summary\")\n",
        "            print(\"=\"*60)\n",
        "            for g in groups:\n",
        "                n_params = sum(p.numel() for p in g[\"params\"])\n",
        "                if n_params > 0:\n",
        "                    lr_mult = g[\"lr\"] / base_lr\n",
        "                    print(f\"{g['name']:20s}: {n_params:10,} params | LR: {lr_mult:6.1f}x | WD: {g['weight_decay']:.3f}\")\n",
        "            print(\"=\"*60 + \"\\n\")\n",
        "            self._param_groups_logged = True\n",
        "\n",
        "        return groups\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# build_optimizer – returns only Optimizer (scheduler opsiyonel)\n",
        "# -----------------------------------------------------------------------------\n",
        "def build_optimizer(model: nn.Module,\n",
        "                    base_lr: float = 2e-4,\n",
        "                    weight_decay: float = 0.02,\n",
        "                    *,\n",
        "                    return_scheduler: bool = False):\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # ---------- 1) LAZY‑PARAM warmup (only backbone) ----------\n",
        "    was_training = model.training\n",
        "    model.train()  # PGI only train\n",
        "    with torch.no_grad():\n",
        "        for s in (640, 320):\n",
        "            dummy = torch.zeros(1, 3, s, s, device=device)\n",
        "            _ = model.backbone(dummy, need_aux=True)\n",
        "    model.train(was_training)\n",
        "\n",
        "    # ---------- 2) PARAM GROUPS ----------\n",
        "    groups = model.param_groups(base_lr)\n",
        "\n",
        "    # Norm weights WD=0 (GN/LNProxy vb.)\n",
        "    ln_keys = ('.norm', '.ln_act', 'ln_proxy')\n",
        "\n",
        "    for g in groups:\n",
        "        params_in_group = set(map(id, g['params']))\n",
        "        if any(any(k in n for k in ln_keys)\n",
        "               for n, p in model.named_parameters() if id(p) in params_in_group):\n",
        "            g[\"weight_decay\"] = 0.0\n",
        "\n",
        "    # ---------- 3) OPTIMIZER ----------\n",
        "    optim = torch.optim.AdamW(\n",
        "        groups, lr=base_lr,\n",
        "        betas=(0.9, 0.999), eps=1e-6,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    if not return_scheduler:\n",
        "        return optim\n",
        "\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optim, T_max=100, eta_min=base_lr * 0.05)\n",
        "    return optim, sched\n",
        "\n",
        "\n",
        "def build_model(num_classes: int = 20,\n",
        "                norm: str = \"gn\",            # \"gn\", \"lnp\" veya \"bn\"\n",
        "                use_layer_scale: bool = True,\n",
        "                layer_scale_init: float = 1.0,\n",
        "                **kwargs) -> HybridDCDATRT:\n",
        "\n",
        "    defaults = dict(\n",
        "        # Backbone\n",
        "        depths         = (2, 2, 4, 1),\n",
        "        drop_path_max  = 0.2,\n",
        "        # Decoder\n",
        "        d_model        = 320,\n",
        "        dec_layers     = 4,\n",
        "        dn_queries     = 100,\n",
        "        num_queries    = 200,\n",
        "        use_aux_loss   = True,\n",
        "        use_iou_aware  = False,\n",
        "        # Dataset\n",
        "        voc_prior      = False,\n",
        "        # Decoder Encoder\n",
        "        decoder_use_encoder           = True,\n",
        "        decoder_encoder_layers        = 2,\n",
        "        decoder_encoder_drop_path_max = 0.1,\n",
        "    )\n",
        "    defaults.update(kwargs)\n",
        "\n",
        "    nf = NormFactory(norm)\n",
        "    model = HybridDCDATRT(\n",
        "        num_classes           = num_classes,\n",
        "        backbone_norm_factory = nf,\n",
        "        neck_norm_factory     = nf,\n",
        "        use_layer_scale       = use_layer_scale,\n",
        "        layer_scale_init      = layer_scale_init,\n",
        "        **defaults\n",
        "    )\n",
        "    # init_weights_improved(model)\n",
        "    # apply_hybrid_fixup(model)\n",
        "    # boost_relative_position_bias(model, std=0.05)\n",
        "    return model\n",
        "# # -----------------------------------------------------------------------------\n",
        "# # Test code\n",
        "# # -----------------------------------------------------------------------------\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     import time\n",
        "\n",
        "#     assert torch.cuda.is_available(), \"CUDA required for this model.\"\n",
        "#     device = torch.device(\"cuda\")\n",
        "\n",
        "#     print(\"=\"*80)\n",
        "#     print(\"Building Production-Ready Hybrid-DCDAT-RT Model...\")\n",
        "#     print(\"=\"*80)\n",
        "\n",
        "#     model = build_model(\n",
        "#         num_classes=80,\n",
        "#         depths=[2, 2, 2, 2],  # ResNet-50 like depth\n",
        "#         drop_path_max=0,\n",
        "#         num_queries=200,\n",
        "#         voc_prior=False,norm=\"gn\"  # Use COCO prior\n",
        "#     ).to(device)\n",
        "#     optimizer = build_optimizer(model, base_lr=2e-4)\n",
        "#     # Test forward pass\n",
        "#     print(\"\\n🚀 Testing forward pass...\")\n",
        "#     model.eval()\n",
        "#     dummy = torch.randn(2, 3, 640, 640, device=device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         out = model(dummy)\n",
        "#         print(\"✅ Forward pass successful!\")\n",
        "#         print(f\"Output keys: {list(out.keys())}\")\n",
        "#         print(f\"Predictions shape: {out['pred_logits'].shape}, {out['pred_boxes'].shape}\")\n",
        "\n",
        "#     # Test training mode with targets\n",
        "#     print(\"\\n🚀 Testing training mode...\")\n",
        "#     model.train()\n",
        "\n",
        "#     # Create more realistic targets with proper box format\n",
        "#     targets = []\n",
        "#     for _ in range(2):  # batch size 2\n",
        "#         num_objs = torch.randint(1, 10, (1,)).item()\n",
        "#         # Generate boxes in (cx, cy, w, h) format, all normalized to [0, 1]\n",
        "#         centers = torch.rand(num_objs, 2, device=device)\n",
        "#         sizes = torch.rand(num_objs, 2, device=device) * 0.5  # max size 0.5\n",
        "#         boxes = torch.cat([centers, sizes], dim=1)\n",
        "\n",
        "#         # Ensure boxes are valid (centers must be at least half-size from borders)\n",
        "#         half_sizes = boxes[:, 2:4] / 2\n",
        "#         boxes[:, 0] = boxes[:, 0].clamp(min=half_sizes[:, 0], max=1-half_sizes[:, 0])\n",
        "#         boxes[:, 1] = boxes[:, 1].clamp(min=half_sizes[:, 1], max=1-half_sizes[:, 1])\n",
        "\n",
        "#         targets.append({\n",
        "#             'boxes': boxes,\n",
        "#             'labels': torch.randint(0, 80, (num_objs,), device=device)\n",
        "#         })\n",
        "\n",
        "#     out = model(dummy, targets)\n",
        "#     print(\"✅ Training mode successful!\")\n",
        "#     print(f\"Output keys: {list(out.keys())}\")\n",
        "#     if 'aux_dense' in out:\n",
        "#         print(f\"Auxiliary dense outputs: {list(out['aux_dense'].keys())}\")\n",
        "#     if 'dn_meta' in out:\n",
        "#         print(f\"Denoising queries: {out['dn_meta']['dn_queries']}\")\n",
        "\n",
        "#     # Test gradient flow\n",
        "#     print(\"\\n🚀 Testing gradient flow...\")\n",
        "#     loss = out['pred_logits'].sum() + out['pred_boxes'].sum()\n",
        "#     loss.backward()\n",
        "\n",
        "#     # Check that gradients flow through all parts\n",
        "#     has_grad = {\n",
        "#         'backbone': any(p.grad is not None for n, p in model.named_parameters() if 'backbone' in n),\n",
        "#         'neck': any(p.grad is not None for n, p in model.named_parameters() if 'neck' in n),\n",
        "#         'decoder': any(p.grad is not None for n, p in model.named_parameters() if 'decoder' in n),\n",
        "#     }\n",
        "\n",
        "#     for module, has in has_grad.items():\n",
        "#         print(f\"  {module}: {'✅' if has else '❌'} gradients\")\n",
        "\n",
        "#     # Test parameter groups\n",
        "#     print(\"\\n🚀 Testing parameter groups...\")\n",
        "#     param_groups = model.param_groups(base_lr=2e-4)\n",
        "#     print(f\"Number of parameter groups: {len(param_groups)}\")\n",
        "#     for i, group in enumerate(param_groups):\n",
        "#         print(f\"  Group {i}: {len(group['params'])} params, lr={group['lr']:.6f}, wd={group['weight_decay']}\")\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*80)\n",
        "#     print(\"✅ All tests passed! Model is production-ready.\")\n",
        "#     print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeFKDb3X3ZOD"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torchvision.ops import generalized_box_iou, box_convert,box_iou\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Helpers: focal losses\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def softmax_focal_loss_weighted(\n",
        "    logits: torch.Tensor,          # (N, C+1)\n",
        "    targets: torch.Tensor,         # (N,) int64  (‑1 = ignore)\n",
        "    *,\n",
        "    alpha: float = 0.25,\n",
        "    gamma: float = 2.0,\n",
        "    sample_weight: Optional[torch.Tensor] = None,  # (N,) or None\n",
        "    reduction: str = \"mean\",\n",
        "    ignore_index: int = -1,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Multi-class focal loss (softmax) + sample weight.\n",
        "    • targets == ignore_index → not included in loss.\n",
        "    • BG class ID = C (i.e., num_classes).\n",
        "    • sample_weight to weight positive/negative samples separately (e.g., QFL).\n",
        "    \"\"\"\n",
        "    logp = F.log_softmax(logits, dim=-1)\n",
        "    p = logp.exp()  # (N, C+1)\n",
        "\n",
        "    if ignore_index is not None:\n",
        "        keep = targets.ne(ignore_index)\n",
        "        if keep.sum() == 0:\n",
        "            return logits.sum() * 0.0\n",
        "        logits = logits[keep]\n",
        "        logp = logp[keep]\n",
        "        p = p[keep]\n",
        "        targets = targets[keep]\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = sample_weight[keep]\n",
        "\n",
        "    idx = torch.arange(targets.size(0), device=targets.device)\n",
        "    log_pt = logp[idx, targets]\n",
        "    pt = p[idx, targets]\n",
        "\n",
        "    bg_id = logits.size(1) - 1\n",
        "    alpha_t = torch.where(targets == bg_id, 1.0 - alpha, alpha)\n",
        "\n",
        "    loss = -alpha_t * (1.0 - pt).pow(gamma) * log_pt  # (N,)\n",
        "\n",
        "    if sample_weight is not None:\n",
        "        loss = loss * sample_weight\n",
        "\n",
        "    if reduction == \"sum\":\n",
        "        return loss.sum()\n",
        "    if reduction == \"mean\":\n",
        "        return loss.mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def binary_focal_with_logits(\n",
        "    input: torch.Tensor,           # (..., C)\n",
        "    target: torch.Tensor,          # (..., C) in {0,1}\n",
        "    alpha: float = 0.25,\n",
        "    gamma: float = 2.0,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Sigmoid (channel-independent) focal loss. BG masked (target 0).\"\"\"\n",
        "    ce = F.binary_cross_entropy_with_logits(input, target, reduction=\"none\")\n",
        "    p = torch.sigmoid(input)\n",
        "    p_t = p * target + (1 - p) * (1 - target)\n",
        "    alpha_t = alpha * target + (1 - alpha) * (1 - target)\n",
        "    loss = alpha_t * (1 - p_t).pow(gamma) * ce\n",
        "\n",
        "    if reduction == \"sum\":\n",
        "        return loss.sum()\n",
        "    if reduction == \"mean\":\n",
        "        return loss.mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Dense aux (sigmoid focal): BG implicit\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def focal_loss_dense_sigmoid(logits, targets, num_classes, alpha=.5, gamma=1.5, use_or_map=False, radius: int = 1):\n",
        "    B, C, H, W = logits.shape\n",
        "    device = logits.device\n",
        "    assert C == num_classes\n",
        "    tgt = torch.zeros(B, C, H, W, device=device)\n",
        "\n",
        "    for b, t in enumerate(targets):\n",
        "        if len(t[\"boxes\"]) == 0: continue\n",
        "        ctr = t[\"boxes\"][:, :2] * torch.tensor([W, H], device=device)\n",
        "        gx = ctr[:, 0].long().clamp(0, W - 1)\n",
        "        gy = ctr[:, 1].long().clamp(0, H - 1)\n",
        "        cls = t[\"labels\"].clamp(0, C - 1)\n",
        "\n",
        "        for (x, y, c) in zip(gx.tolist(), gy.tolist(), cls.tolist()):\n",
        "            x0, x1 = max(0, x - radius), min(W - 1, x + radius)\n",
        "            y0, y1 = max(0, y - radius), min(H - 1, y + radius)\n",
        "            tgt[b, c, y0:y1+1, x0:x1+1] = 1.0\n",
        "\n",
        "    logits_flat = logits.permute(0, 2, 3, 1).reshape(-1, C)\n",
        "    tgt_flat = tgt.permute(0, 2, 3, 1).reshape(-1, C)\n",
        "    return binary_focal_with_logits(logits_flat, tgt_flat, alpha=alpha, gamma=gamma, reduction=\"mean\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Hungarian matcher (log‑prob, cost scaling)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"One‑to‑one matching between predicted queries and GT boxes.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 cost_class: float = 2.,\n",
        "                 cost_bbox: float = 5.,\n",
        "                 cost_giou: float = 2.):\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self,\n",
        "                outputs: Dict[str, torch.Tensor],\n",
        "                targets: List[Dict]) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        outputs:\n",
        "            - pred_logits: (B, N, C+1)\n",
        "            - pred_boxes : (B, N, 4)  (cx,cy,w,h norm.)\n",
        "        NOTE: query_selection_mask is not used here; we use it to ignore negatives on the loss side.\n",
        "        \"\"\"\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "        device = outputs[\"pred_logits\"].device\n",
        "\n",
        "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)   # (B·N, C+1)\n",
        "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)                # (B·N, 4)\n",
        "\n",
        "        tgt_ids  = torch.cat([v[\"labels\"] for v in targets])          # (ΣT,)\n",
        "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])           # (ΣT, 4)\n",
        "\n",
        "        cost_class = -out_prob[:, tgt_ids].clamp(1e-8).log()          # (B·N, ΣT)\n",
        "        cost_bbox  = torch.cdist(out_bbox, tgt_bbox, p=1)             # L1\n",
        "        cost_giou  = -generalized_box_iou(\n",
        "            box_convert(out_bbox, \"cxcywh\", \"xyxy\"),\n",
        "            box_convert(tgt_bbox, \"cxcywh\", \"xyxy\")\n",
        "        )\n",
        "\n",
        "        C = (self.cost_class * cost_class +\n",
        "             self.cost_bbox  * cost_bbox  +\n",
        "             self.cost_giou  * cost_giou)                             # (B·N, ΣT)\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "\n",
        "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
        "        indices: list[Tuple[torch.Tensor, torch.Tensor]] = []\n",
        "        tgt_ptr = 0\n",
        "        for b in range(bs):\n",
        "            tgt_cnt = sizes[b]\n",
        "            if tgt_cnt == 0:\n",
        "                indices.append((torch.empty(0, dtype=torch.int64, device=device),\n",
        "                                torch.empty(0, dtype=torch.int64, device=device)))\n",
        "                continue\n",
        "            cost_b = C[b, :, tgt_ptr: tgt_ptr + tgt_cnt].numpy()\n",
        "            row_ind, col_ind = linear_sum_assignment(cost_b)\n",
        "            indices.append((torch.as_tensor(row_ind, dtype=torch.int64, device=device),\n",
        "                            torch.as_tensor(col_ind, dtype=torch.int64, device=device)))\n",
        "            tgt_ptr += tgt_cnt\n",
        "\n",
        "        return indices\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Utilities for QFL‑Softmax (quality weights for positives)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def _compute_matched_ious(\n",
        "    pred_boxes: torch.Tensor,  # (B,N,4)\n",
        "    targets: List[Dict],       # list of dict with \"boxes\"\n",
        "    indices: List[Tuple[torch.Tensor, torch.Tensor]],\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculates IoU(pred_box, gt_box) for matched positives.\n",
        "    Returns: (Σ_matches,) vector, [0,1].\n",
        "    \"\"\"\n",
        "    if len(indices) == 0:\n",
        "        return pred_boxes.new_zeros((0,))\n",
        "    b_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "    s_idx = torch.cat([src for (src, _) in indices])\n",
        "    if b_idx.numel() == 0:\n",
        "        return pred_boxes.new_zeros((0,))\n",
        "\n",
        "    src_boxes = pred_boxes[b_idx, s_idx]    # (Σ,4)\n",
        "    tgt_boxes = torch.cat([t[\"boxes\"][J] for t, (_, J) in zip(targets, indices)], 0)  # (Σ,4)\n",
        "\n",
        "    ious = torch.diag(generalized_box_iou(\n",
        "        box_convert(src_boxes, \"cxcywh\", \"xyxy\"),\n",
        "        box_convert(tgt_boxes, \"cxcywh\", \"xyxy\")\n",
        "    )).clamp(min=0., max=1.)\n",
        "    return ious  # (Σ,)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Main SetCriterion (production‑ready)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes all required losses for HybridDCDATRT training (IoU-OFF OPtional).\n",
        "\n",
        "    • Class loss: Softmax‑Focal + QFL‑Softmax (quality weight only with GT IoU).\n",
        "    • Box: L1 + GIoU.\n",
        "    • Dense aux: Sigmoid‑focal (FG channels).\n",
        "    • DN (denoising): class + box (L1+GIoU).\n",
        "    • Aux decoder outputs: labels/boxes, weighted.\n",
        "    • (Optional) Objectness: If outputs[“pred_obj_logits”] exists, BCE loss is added.\n",
        "    • NOTE: IoU regression **removed** (no IoU header in decoder).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes: int,\n",
        "                 matcher: 'HungarianMatcher',\n",
        "                 weight_dict: Dict[str, float],\n",
        "                 eos_coef: float = .1,\n",
        "                 losses: Optional[List[str]] = None,\n",
        "                 aux_weight: float = .4,\n",
        "                 *,\n",
        "                 # QFL‑Softmax\n",
        "                 use_qfl: bool = True,\n",
        "                 qfl_beta: float = 1.0,            # quality ^ beta\n",
        "                 qfl_lambda: float = 0.5,\n",
        "                 qfl_use_pred: bool = False):      # ← IoU‑OFF (opt)\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.aux_weight = aux_weight\n",
        "\n",
        "\n",
        "        self.losses = losses or [\n",
        "            \"labels\", \"boxes\", \"cardinality\",\n",
        "            \"dense_aux\",\n",
        "            \"dn_label\", \"dn_box\",\n",
        "            \"obj\",\n",
        "            \"center\",\n",
        "        ]\n",
        "\n",
        "        # background (no-object) class weight – (used with alpha in softmax focal)\n",
        "        empty_w = torch.ones(self.num_classes + 1)\n",
        "        empty_w[-1] = eos_coef\n",
        "        self.register_buffer(\"empty_weight\", empty_w)\n",
        "\n",
        "        # QFL params\n",
        "        self.use_qfl = use_qfl\n",
        "        self.qfl_beta = float(qfl_beta)\n",
        "        # IoU‑OFF: The following two parameters are retained but are ineffective.\n",
        "        self.qfl_lambda = float(qfl_lambda)\n",
        "        self.qfl_use_pred = False\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  helpers\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        batch_idx = torch.cat([torch.full_like(src, i)\n",
        "                               for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _build_pos_mask(self, outputs, indices):\n",
        "        \"\"\"(B,N) bool; matched (positive) queries True\"\"\"\n",
        "        B, N = outputs[\"pred_logits\"].shape[:2]\n",
        "        pos = outputs[\"pred_logits\"].new_zeros((B, N), dtype=torch.bool)\n",
        "        b_idx, s_idx = self._get_src_permutation_idx(indices)\n",
        "        if b_idx.numel():\n",
        "            pos[b_idx, s_idx] = True\n",
        "        return pos\n",
        "\n",
        "    # -------------------------------- labels (QFL‑Softmax) -------------\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes, **_):\n",
        "        \"\"\"\n",
        "        Softmax‑Focal + QFL (only GT IoU).  Denom: global keep (sel|pos) count.\n",
        "        When DN is added, keep the loss scale constant using ‘mean over supervised queries’.\n",
        "        \"\"\"\n",
        "        src_logits = outputs[\"pred_logits\"]            # (B,N,C+1)\n",
        "        B, N, K = src_logits.shape\n",
        "        C = K - 1\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "\n",
        "        # all unmatched queries → background\n",
        "        tgt_classes = torch.full((B, N), C, dtype=torch.int64, device=src_logits.device)\n",
        "        if idx[0].numel():\n",
        "            tgt_classes[idx] = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)], 0)\n",
        "\n",
        "        # keep = sel | pos (non keep  ignore=-1 )\n",
        "        if \"query_selection_mask\" in outputs:\n",
        "            sel = outputs[\"query_selection_mask\"].to(torch.bool)\n",
        "            pos = self._build_pos_mask(outputs, indices)\n",
        "            keep = (sel | pos)\n",
        "            tgt_classes = tgt_classes.clone()\n",
        "            tgt_classes[~keep] = -1\n",
        "        else:\n",
        "            keep = torch.ones(B, N, dtype=torch.bool, device=src_logits.device)\n",
        "\n",
        "        # BG weight loss (eos_coef)\n",
        "        base_w = torch.ones((B, N), device=src_logits.device, dtype=src_logits.dtype)\n",
        "        bg_id = C\n",
        "        base_w[tgt_classes == bg_id] = self.empty_weight[-1].item()\n",
        "\n",
        "        # QFL quality weight — only for positives GT IoU\n",
        "        sample_weight = base_w\n",
        "        if self.use_qfl and idx[0].numel():\n",
        "            with torch.no_grad():\n",
        "                ious_t = _compute_matched_ious(outputs[\"pred_boxes\"], targets, indices)  # (Σ_pos,)\n",
        "                q = ious_t.clamp(0., 1.).pow(self.qfl_beta)\n",
        "                q = torch.clamp(q, min=0.05)\n",
        "            sample_weight = sample_weight.clone()\n",
        "            sample_weight[idx] = sample_weight[idx] * q  # quality multiplier for positives\n",
        "\n",
        "        #  Normalize with DDP global ‘keep’ count (truly supervised examples)\n",
        "        denom = self._ddp_denom(keep.sum(), device=src_logits.device)\n",
        "\n",
        "        loss = softmax_focal_loss_weighted(\n",
        "            src_logits.reshape(-1, K),\n",
        "            tgt_classes.reshape(-1),\n",
        "            alpha=.25, gamma=2., reduction=\"sum\",\n",
        "            sample_weight=sample_weight.reshape(-1)\n",
        "        ) / denom\n",
        "\n",
        "        return {\"loss_labels\": loss}\n",
        "\n",
        "    # -------------------------------- cardinality ----------------------\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes, **_):\n",
        "        probs = outputs[\"pred_logits\"].softmax(-1)   # (B,N,C+1)\n",
        "        bg = probs[..., -1]\n",
        "\n",
        "        if \"query_selection_mask\" in outputs:\n",
        "            sel = outputs[\"query_selection_mask\"].to(torch.bool)\n",
        "            pos = self._build_pos_mask(outputs, indices)\n",
        "            keep = (sel | pos).float()\n",
        "            denom = keep.sum(1).clamp(min=1).float()    # only keep\n",
        "            card_pred = ((1. - bg) * keep).sum(1) / denom\n",
        "        else:\n",
        "            card_pred = (1. - bg).sum(1)\n",
        "\n",
        "        tgt_lens = torch.as_tensor([len(t[\"labels\"]) for t in targets],\n",
        "                                   dtype=torch.float, device=probs.device)\n",
        "        loss = F.l1_loss(card_pred, tgt_lens)\n",
        "        return {\"loss_cardinality\": loss}\n",
        "\n",
        "    # -------------------------------- boxes ----------------------------\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes, **_):\n",
        "        \"\"\"\n",
        "        If there is no match (no GT box) ⇒ 0 loss is returned.\n",
        "        \"\"\"\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        if idx[0].numel() == 0:                       # zero‑match guard\n",
        "            z = outputs[\"pred_boxes\"].sum() * 0.0\n",
        "            return {\"loss_bbox\": z, \"loss_giou\": z}\n",
        "\n",
        "        src_boxes = outputs[\"pred_boxes\"][idx]\n",
        "        tgt_boxes = torch.cat([t[\"boxes\"][i]\n",
        "                               for t, (_, i) in zip(targets, indices)], 0)\n",
        "\n",
        "        l1 = F.l1_loss(src_boxes, tgt_boxes, reduction=\"none\").sum() / max(num_boxes, 1.0)\n",
        "\n",
        "        giou = 1.0 - torch.diag(generalized_box_iou(\n",
        "            box_convert(src_boxes, \"cxcywh\", \"xyxy\"),\n",
        "            box_convert(tgt_boxes, \"cxcywh\", \"xyxy\")\n",
        "        )).sum() / max(num_boxes, 1.0)\n",
        "\n",
        "        return {\"loss_bbox\": l1, \"loss_giou\": giou}\n",
        "\n",
        "    # -------------------------------- dense aux ------------------------\n",
        "    def loss_dense_aux(self, outputs, targets, *_):\n",
        "        if \"aux_dense\" not in outputs:\n",
        "            return {}\n",
        "        total = 0.0\n",
        "        for pred in outputs[\"aux_dense\"].values():           # s1, s2, s3\n",
        "            logits = pred[:, :self.num_classes]              # (B,C,H,W) — FG channels\n",
        "            total = total + focal_loss_dense_sigmoid(\n",
        "                logits, targets, self.num_classes,\n",
        "                alpha=.25, gamma=2., use_or_map=False\n",
        "            )\n",
        "        return {\"loss_dense_aux\": total}\n",
        "\n",
        "    # ------------------------------- denoising cls ---------------------\n",
        "    @staticmethod\n",
        "    def _ddp_denom(count, device) -> float:\n",
        "        # count: int or tensor\n",
        "        if isinstance(count, torch.Tensor):\n",
        "            c = count.to(device=device, dtype=torch.float)\n",
        "        else:\n",
        "            c = torch.tensor(float(count), device=device, dtype=torch.float)\n",
        "        if dist.is_available() and dist.is_initialized():\n",
        "            dist.all_reduce(c, op=dist.ReduceOp.SUM)\n",
        "        return max(float(c.item()), 1.0)\n",
        "\n",
        "    def loss_dn_label(self, outputs, *_):\n",
        "        if \"dn_meta\" not in outputs:\n",
        "            return {}\n",
        "\n",
        "        m = outputs[\"dn_meta\"]\n",
        "        logits = m[\"dn_logits\"]        # (B,Q,C+1)\n",
        "        labels = m[\"dn_labels\"]        # (B,Q)\n",
        "        mask   = m.get(\"dn_mask\", None)\n",
        "\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.to(torch.bool)\n",
        "            if mask.dim() == 1:\n",
        "                mask = mask[:, None].expand_as(labels)\n",
        "            elif mask.shape != labels.shape:\n",
        "                mask = mask.expand_as(labels)\n",
        "            labels = labels.where(mask, labels.new_full(labels.shape, -1))  # ignore: -1\n",
        "\n",
        "        # Number of valid samples (global)\n",
        "        valid = labels.ge(0)\n",
        "        denom = self._ddp_denom(valid.sum(), device=logits.device)\n",
        "\n",
        "        # 'sum' + global denom → stable scale\n",
        "        loss = softmax_focal_loss_weighted(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            labels.reshape(-1),\n",
        "            alpha=.25, gamma=2., reduction=\"sum\"\n",
        "        ) / denom\n",
        "\n",
        "        return {\"loss_dn_label\": loss}\n",
        "\n",
        "\n",
        "    # ------------------------------- denoising box ---------------------\n",
        "    def loss_dn_box(self, outputs, *_):\n",
        "        if \"dn_meta\" not in outputs:\n",
        "            return {}\n",
        "\n",
        "        m = outputs[\"dn_meta\"]\n",
        "        pred = m[\"dn_boxes\"]     # (B,Q,4)\n",
        "        tgt  = m[\"dn_gt_boxes\"]  # (B,Q,4)\n",
        "\n",
        "        # First try pos_mask, otherwise dn_mask\n",
        "        posm = m.get(\"dn_pos_mask\", None)\n",
        "        if posm is None:\n",
        "            posm = m.get(\"dn_mask\", None)\n",
        "\n",
        "        if posm is not None:\n",
        "            keep = posm.to(torch.bool)\n",
        "            if keep.dim() == 1:\n",
        "                keep = keep[:, None].expand(pred.shape[:2])\n",
        "        else:\n",
        "            keep = torch.ones(pred.shape[:2], dtype=torch.bool, device=pred.device)\n",
        "\n",
        "        if not keep.any():\n",
        "            z = pred.sum() * 0.0\n",
        "            return {\"loss_dn_bbox\": z, \"loss_dn_giou\": z}\n",
        "\n",
        "        pred = pred[keep]\n",
        "        tgt  = tgt[keep]\n",
        "\n",
        "        denom = self._ddp_denom(keep.sum(), device=pred.device)\n",
        "\n",
        "        l1 = F.l1_loss(pred, tgt, reduction=\"sum\") / denom\n",
        "\n",
        "        giou_mat = generalized_box_iou(\n",
        "            box_convert(pred, 'cxcywh', 'xyxy'),\n",
        "            box_convert(tgt,  'cxcywh', 'xyxy')\n",
        "        )\n",
        "        giou = (1.0 - torch.diag(giou_mat)).sum() / denom\n",
        "\n",
        "        return {\"loss_dn_bbox\": l1, \"loss_dn_giou\": giou}\n",
        "    # ------------------------------- Objectness (optional) ------------\n",
        "    def loss_obj(self, outputs, targets, indices, num_boxes, **_):\n",
        "        \"\"\"\n",
        "        If the decoder has produced ‘pred_obj_logits’ (B,N), the simple BCE loss is calculated using the keep mask (sel|pos).\n",
        "        Otherwise, it returns empty.\n",
        "        \"\"\"\n",
        "        if \"pred_obj_logits\" not in outputs:\n",
        "            return {}\n",
        "        obj_logits = outputs[\"pred_obj_logits\"]  # (B,N)\n",
        "\n",
        "        # keep = sel|pos\n",
        "        if \"query_selection_mask\" in outputs:\n",
        "            sel = outputs[\"query_selection_mask\"].to(torch.bool)  # (B,N)\n",
        "        else:\n",
        "            sel = torch.zeros_like(obj_logits, dtype=torch.bool)\n",
        "\n",
        "        pos = self._build_pos_mask(outputs, indices)              # (B,N) True=matched\n",
        "        keep = sel | pos\n",
        "\n",
        "        # target: pos→1, (keep & ~pos) → 0, ignore those that are not kept\n",
        "        target = keep & pos\n",
        "        if keep.sum() == 0:\n",
        "            return {\"loss_obj\": obj_logits.sum() * 0.0}\n",
        "\n",
        "        loss = F.binary_cross_entropy_with_logits(\n",
        "            obj_logits[keep], target[keep].float(), reduction=\"mean\"\n",
        "        )\n",
        "        return {\"loss_obj\": loss}\n",
        "\n",
        "    # ------------------------------- Center alignment ------------------\n",
        "    def loss_center(self, outputs, targets, indices, num_boxes, **_):\n",
        "        if \"final_ref_pts\" not in outputs:\n",
        "            return {}\n",
        "\n",
        "        B, N, _ = outputs[\"pred_boxes\"].shape\n",
        "        device = outputs[\"pred_boxes\"].device\n",
        "\n",
        "        # keep = sel | pos\n",
        "        if \"query_selection_mask\" in outputs:\n",
        "            sel = outputs[\"query_selection_mask\"].to(torch.bool)  # (B,N)\n",
        "        else:\n",
        "            sel = torch.ones(B, N, dtype=torch.bool, device=device)\n",
        "\n",
        "        pos = self._build_pos_mask(outputs, indices)              # (B,N)\n",
        "        keep = sel | pos                                          # (B,N)\n",
        "\n",
        "        if keep.sum() == 0:\n",
        "            z = outputs[\"pred_boxes\"].sum() * 0.0\n",
        "            return {\"loss_center\": z}\n",
        "\n",
        "        # (B,N,4,2) → only keep\n",
        "        ref = outputs[\"final_ref_pts\"][keep]                      # (K,4,2) veya (K,2)\n",
        "        if ref.dim() == 2:\n",
        "            ctr_ref = ref                                         # (K,2)\n",
        "        else:\n",
        "            ctr_ref = ref.mean(dim=1)                             # (K,2)  seviye ort.\n",
        "        ctr_pred = outputs[\"pred_boxes\"][..., :2][keep]           # (K,2)\n",
        "\n",
        "        # DDP global mean\n",
        "        denom = self._ddp_denom(keep.sum(), device=device)\n",
        "        loss = F.l1_loss(ctr_pred, ctr_ref, reduction=\"sum\") / denom\n",
        "        return {\"loss_center\": loss}\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def get_loss(self, name, outputs, targets, indices, num_boxes):\n",
        "        return {\n",
        "            \"labels\":      self.loss_labels,\n",
        "            \"cardinality\": self.loss_cardinality,\n",
        "            \"boxes\":       self.loss_boxes,\n",
        "            \"dense_aux\":   self.loss_dense_aux,\n",
        "            \"dn_label\":    self.loss_dn_label,\n",
        "            \"dn_box\":      self.loss_dn_box,\n",
        "            \"obj\":         self.loss_obj,\n",
        "            \"center\":      self.loss_center,\n",
        "        }[name](outputs, targets, indices, num_boxes)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def forward(self,\n",
        "                outputs: Dict[str, torch.Tensor],\n",
        "                targets: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        # Hungarian matching (main outputs)\n",
        "        indices = self.matcher(outputs, targets)\n",
        "\n",
        "        # num_boxes\n",
        "        device = outputs[\"pred_logits\"].device\n",
        "        nb = torch.as_tensor([sum(len(t[\"labels\"]) for t in targets)],\n",
        "                     dtype=torch.float, device=device)\n",
        "        if dist.is_available() and dist.is_initialized():\n",
        "            dist.all_reduce(nb, op=dist.ReduceOp.SUM)   # ← now global TOTAL\n",
        "        num_boxes = max(nb.item(), 1.0)\n",
        "\n",
        "        # ---- main losses ----\n",
        "        loss_dict: Dict[str, torch.Tensor] = {}\n",
        "        for name in self.losses:\n",
        "            if name.startswith(\"dn_\") and \"dn_meta\" not in outputs:\n",
        "                continue\n",
        "            loss_dict.update(self.get_loss(\n",
        "                name, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        # ---- auxiliary decoder outputs ----\n",
        "        sel_mask = outputs.get(\"query_selection_mask\", None)\n",
        "        if \"aux_outputs\" in outputs:\n",
        "            for i, aux in enumerate(outputs[\"aux_outputs\"]):\n",
        "                # Pass the mask to aux (shape: (B,N))\n",
        "                aux_in = aux if sel_mask is None else {**aux, \"query_selection_mask\": sel_mask}\n",
        "                aux_idx = self.matcher(aux_in, targets)\n",
        "                for l in (\"labels\", \"boxes\"):\n",
        "                    l_dict = self.get_loss(l, aux_in, targets, aux_idx, num_boxes)\n",
        "                    l_dict = {k + f\"_{i}\": v * self.aux_weight for k, v in l_dict.items()}\n",
        "                    loss_dict.update(l_dict)\n",
        "\n",
        "        # ---- apply weight ----\n",
        "        weighted: Dict[str, torch.Tensor] = {}\n",
        "        for k, v in loss_dict.items():\n",
        "            base = None\n",
        "            for b in self.weight_dict:\n",
        "                if k == b or k.startswith(b + \"_\"):\n",
        "                    base = b\n",
        "                    break\n",
        "            weighted[k] = v * self.weight_dict.get(base, 1.0)\n",
        "\n",
        "        weighted[\"loss\"] = sum(weighted.values())\n",
        "        return weighted\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Convenience builder\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def build_criterion(\n",
        "    num_classes: int = 20,\n",
        "    weight_dict: Optional[Dict[str, float]] = None,\n",
        "    matcher_costs: Optional[Dict[str, float]] = None,\n",
        "    *,\n",
        "    use_qfl: bool = True,\n",
        "    qfl_beta: float = 1.0,\n",
        "    qfl_lambda: float = 0.5,   # IoU‑OFF:is not used\n",
        "    qfl_use_pred: bool = False # IoU‑OFF: is not used\n",
        ") -> SetCriterion:\n",
        "\n",
        "    if weight_dict is None:\n",
        "        weight_dict = {\n",
        "          \"loss_labels\":      2.0,\n",
        "          \"loss_bbox\":        5.0,\n",
        "          \"loss_giou\":        2.5,\n",
        "          \"loss_dense_aux\":   0.5,\n",
        "          \"loss_dn_label\":    1.2,\n",
        "          \"loss_dn_bbox\":     0.75,\n",
        "          \"loss_dn_giou\":     0.75,\n",
        "          \"loss_cardinality\": 0.1,\n",
        "          \"loss_obj\":         0.15,   # if pred_obj_logits\n",
        "          \"loss_center\":      0.1,\n",
        "      }\n",
        "        # ‘loss_iou’ IS INTENTIONALLY MISSING (decoder IoU header is closed for ablation)\n",
        "\n",
        "    if matcher_costs is None:\n",
        "        matcher_costs = {\"cost_class\": 2., \"cost_bbox\": 5., \"cost_giou\": 2.}\n",
        "\n",
        "    matcher = HungarianMatcher(**matcher_costs)\n",
        "\n",
        "    return SetCriterion(\n",
        "        num_classes=num_classes,\n",
        "        matcher=matcher,\n",
        "        weight_dict=weight_dict,\n",
        "        eos_coef=.1,\n",
        "        losses=[\n",
        "            \"labels\", \"boxes\", \"cardinality\",\n",
        "            \"dense_aux\", \"dn_label\", \"dn_box\",\n",
        "            \"obj\", \"center\"\n",
        "        ],\n",
        "        aux_weight=.4,\n",
        "        use_qfl=use_qfl,\n",
        "        qfl_beta=qfl_beta,\n",
        "        qfl_lambda=qfl_lambda,\n",
        "        qfl_use_pred=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI6TTzvO87-8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
