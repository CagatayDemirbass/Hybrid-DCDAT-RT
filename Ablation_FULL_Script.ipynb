{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVxcxCvduCru"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97sLTsZ7cbBT"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "\n",
        "!pip install torch==2.3.1+cu121 torchvision --extra-index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb_WGBs8ce5v"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q  pycocotools tqdm torchmetrics lxml scipy\n",
        "!pip install -U \"datasets>=2.17.0\"  \"pyarrow>=14.0\"\n",
        "!pip install -U cmake ninja wheel\n",
        "!pip install --no-binary=mmcv mmcv==2.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSaE0Y33cg0S"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y numpy\n",
        "\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJTDR51TciZP"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install -U cmake ninja wheel\n",
        "\n",
        "git clone --depth 1 --branch v0.14.6 https://github.com/SHI-Labs/NATTEN.git\n",
        "cd NATTEN\n",
        "\n",
        "export FORCE_CUDA=1\n",
        "export TORCH_CUDA_ARCH_LIST=\"8.0\"\n",
        "\n",
        "pip install .\n",
        "\n",
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdcZRLBfge1U"
      },
      "outputs": [],
      "source": [
        "!rm -rf DAT && git clone -q https://github.com/LeapLabTHU/DAT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEQnXyvSggBV"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "git clone --depth 1 https://github.com/OpenGVLab/DCNv4.git\n",
        "cd DCNv4/DCNv4_op\n",
        "export FORCE_CUDA=1\n",
        "export TORCH_CUDA_ARCH_LIST=\"8.0\"\n",
        "python -m pip install . --no-build-isolation -v   # 4-5 dk\n",
        "cd ../..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHPZajM-gpJT"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "!pip -q install opendatasets\n",
        "import opendatasets as od\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/vijayabhaskar96/pascal-voc-2007-and-2012\",\n",
        "    data_dir=\"/content/voc\")          # → /content/voc/VOCdevkit/{VOC2007,VOC2012}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qY6dzNDgqbT",
        "outputId": "95123020-c4de-44ad-f275-4eaf721ea73c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5011 11540 16551 4952\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import VOCDetection\n",
        "from torch.utils.data import ConcatDataset\n",
        "ROOT = \"/content/voc/pascal-voc-2007-and-2012\"\n",
        "train07 = VOCDetection(ROOT, \"2007\", \"trainval\")   # 5011\n",
        "train12 = VOCDetection(ROOT, \"2012\", \"trainval\")   # 11540\n",
        "combined = ConcatDataset([train07, train12])       # 16551\n",
        "test07   = VOCDetection(ROOT, \"2007\", \"test\")      # 4952x\n",
        "\n",
        "print(len(train07), len(train12), len(combined), len(test07))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjaVJeg3rWJ0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import from actual DAT and DCNv4 modules\n",
        "import importlib\n",
        "from DAT.models.dat import DAT, LayerScale, TransformerStage\n",
        "from DAT.models.dat_blocks import LayerNormProxy, TransformerMLP, TransformerMLPWithConv\n",
        "from DAT.models.dat_blocks import LocalAttention, DAttentionBaseline, ShiftWindowAttention, PyramidAttention\n",
        "from DCNv4.modules.dcnv4 import DCNv4\n",
        "dat_mod = importlib.import_module(\"DAT.models.dat\")\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from __future__ import annotations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.layers import DropPath\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from mmcv.ops.multi_scale_deform_attn import MultiScaleDeformableAttention\n",
        "from timm.models.layers import create_act_layer\n",
        "from typing import Optional, List, Dict, Tuple,  Union\n",
        "\n",
        "\n",
        "\n",
        "class NormFactory:\n",
        "    SUPPORTED = {\"bn\", \"gn\", \"lnp\"}\n",
        "\n",
        "    def __init__(self, kind: str = \"gn\", gn_groups: int = 32):\n",
        "        kind = kind.lower()\n",
        "        if kind not in self.SUPPORTED:\n",
        "            raise ValueError(f\"kind must be one of {self.SUPPORTED}\")\n",
        "        self.kind = kind\n",
        "        self.gn_groups = gn_groups\n",
        "\n",
        "    def __call__(self, num_feat: int) -> nn.Module:\n",
        "        if self.kind == \"bn\":\n",
        "            return nn.BatchNorm2d(num_feat)\n",
        "        if self.kind == \"gn\":\n",
        "            g = math.gcd(self.gn_groups, num_feat) or 1\n",
        "            return nn.GroupNorm(g, num_feat)\n",
        "\n",
        "        if self.kind == \"lnp\":\n",
        "            return LayerNormProxy(num_feat)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. CrossScaleInjection\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class CrossScaleInjection(nn.Module):\n",
        "    \"\"\"Cross‑scale feature enjeksiyonu – kanal‑başına gating versiyonu\"\"\"\n",
        "    def __init__(self, low_ch: int, high_ch: int):\n",
        "        super().__init__()\n",
        "        self.align = nn.Conv2d(low_ch, high_ch, 1, bias=False)\n",
        "        self.norm = LayerNormProxy(high_ch)\n",
        "\n",
        "        # Channel based weight; starting 0.1\n",
        "        self.weight = nn.Parameter(torch.ones(1, high_ch, 1, 1) * 0.15)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.align.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, low_res: torch.Tensor, high_res: torch.Tensor) -> torch.Tensor:\n",
        "        low_aligned = self.align(low_res)\n",
        "        low_up = F.interpolate(low_aligned, size=high_res.shape[-2:],\n",
        "                              mode='bilinear', align_corners=False)\n",
        "        low_up = self.norm(low_up)\n",
        "        # sigmoid → [0,1] for per channel λ\n",
        "        return high_res + torch.sigmoid(self.weight) * low_up\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PGI– Programmable Gradient Injection\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class PGIModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Programmable Gradient Injection v2 – DDP‑safe\n",
        "    * If aux_channels is provided, a 1x1 ‘aux_adapter’ is set up during initialization (static, DDP‑safe).\n",
        "    * The auxiliary contribution is only added in training mode.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        reduction: int = 4,\n",
        "        lambda_bounds: tuple[float, float] = (0.2, 0.7),\n",
        "        init_lambda: float = 0.5,\n",
        "        use_bn: bool = True,\n",
        "        drop_path: float = 0.0,\n",
        "        norm_factory: NormFactory = NormFactory(\"gn\"),\n",
        "        aux_channels: int | None = None,   # ◀︎ yeni\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.lambda_min, self.lambda_max = lambda_bounds\n",
        "\n",
        "        # main branch\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, 3, 1, 1,\n",
        "                      groups=max(1, channels // 4), bias=False),\n",
        "            norm_factory(channels) if use_bn else nn.Identity(),\n",
        "        )\n",
        "\n",
        "        mid = max(channels // reduction, 32)\n",
        "        self.aux = nn.Sequential(\n",
        "            nn.Conv2d(channels, mid, 1, bias=False),\n",
        "            norm_factory(mid) if use_bn else nn.Identity(),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Conv2d(mid, channels, 1, bias=False),\n",
        "            norm_factory(channels) if use_bn else nn.Identity(),\n",
        "        )\n",
        "\n",
        "        # Aux channel adapter (static)\n",
        "        if aux_channels is None or aux_channels == channels:\n",
        "            self.aux_adapter = nn.Identity()\n",
        "        else:\n",
        "            self.aux_adapter = nn.Conv2d(aux_channels, channels, 1, bias=False)\n",
        "            nn.init.kaiming_normal_(self.aux_adapter.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "        # λ (learnable)\n",
        "        import math\n",
        "        init_logit = math.log((init_lambda - self.lambda_min) / (self.lambda_max - init_lambda))\n",
        "        self._lambda_logit = nn.Parameter(torch.tensor(init_logit))\n",
        "\n",
        "        # Gate start 0.6\n",
        "        self.gate = nn.Parameter(torch.ones(1, channels, 1, 1) * 0.5)\n",
        "\n",
        "        self.dp = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "        self._init_weights()\n",
        "\n",
        "    @property\n",
        "    def lambda_val(self) -> torch.Tensor:\n",
        "        σ = torch.sigmoid(self._lambda_logit)\n",
        "        return self.lambda_min + (self.lambda_max - self.lambda_min) * σ\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                if hasattr(m, 'weight'): nn.init.ones_(m.weight)\n",
        "                if hasattr(m, 'bias'):   nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _align_spatial(self, feat: torch.Tensor, target_hw: tuple[int, int]) -> torch.Tensor:\n",
        "        if feat.shape[-2:] != target_hw:\n",
        "            feat = F.interpolate(feat, size=target_hw, mode='bilinear', align_corners=False)\n",
        "        return feat\n",
        "\n",
        "    def forward(self, x: torch.Tensor, aux_input: torch.Tensor | None = None):\n",
        "        main_out = self.main(x)\n",
        "\n",
        "        if aux_input is not None and self.training:\n",
        "            aux = self.aux_adapter(aux_input)\n",
        "            aux = self._align_spatial(aux, x.shape[-2:])\n",
        "            aux = self.aux(aux) * self.gate\n",
        "            main_out = main_out + self.lambda_val * aux\n",
        "\n",
        "        return x + self.dp(main_out)\n",
        "\n",
        "def get_drop_path_rates(num_layers: int, max_rate: float) -> List[float]:\n",
        "    \"\"\"Generate drop path rates with cosine scheduling\n",
        "\n",
        "    Args:\n",
        "        num_layers: Total number of layers\n",
        "        max_rate: Maximum drop path rate\n",
        "\n",
        "    Returns:\n",
        "        List of drop path rates for each layer\n",
        "    \"\"\"\n",
        "    if num_layers <= 1:\n",
        "        return [0.0] * num_layers\n",
        "\n",
        "    # Cosine scheduling for smooth progression\n",
        "    rates = [\n",
        "        max_rate * (1.0 - math.cos(math.pi * i / (num_layers - 1))) * 0.5\n",
        "        for i in range(num_layers)\n",
        "    ]\n",
        "\n",
        "    return rates\n",
        "\n",
        "\n",
        "def init_weights_improved(model: nn.Module):\n",
        "    \"\"\"Improved weight initialization for all module types\"\"\"\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            # Special handling for different conv types\n",
        "            if 'dw' in name or module.groups > 1:  # Depthwise\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='linear')\n",
        "            elif 'pw' in name or module.kernel_size == (1, 1):  # Pointwise\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "            else:  # Regular conv\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.BatchNorm2d):\n",
        "            nn.init.ones_(module.weight)\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, LayerNormProxy):\n",
        "            if hasattr(module, 'weight'):\n",
        "                nn.init.ones_(module.weight)\n",
        "            if hasattr(module, 'bias'):\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            nn.init.trunc_normal_(module.weight, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, std=0.02)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Channel-wise Attention (SE)\n",
        "# -----------------------------------------------------------------------------\n",
        "class ChannelAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, channels: int, reduction: int = 8):\n",
        "        super().__init__()\n",
        "        mid = max(channels // reduction, 16)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(channels, mid, 1, bias=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Conv2d(mid, channels, 1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # --- parameters\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_out', nonlinearity='sigmoid')\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "        # ►–– init scale ≈\n",
        "        self.residual_scale = nn.Parameter(torch.tensor(-2.0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        y = self.avgpool(x)\n",
        "        y = self.relu(self.fc1(y))\n",
        "        y = self.sigmoid(self.fc2(y))\n",
        "\n",
        "        scale = torch.sigmoid(self.residual_scale)   # ∈(0,1)\n",
        "        return x * (1 - scale) + x * y * scale\n",
        "# 5. Gradient-clip helpers\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def clip_dcnv4_grads(model: nn.Module, max_norm: float = 0.2) -> None:\n",
        "    \"\"\"Clip gradients for DCNv4 offset/mask parameters - more aggressive\"\"\"\n",
        "    target_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is None:\n",
        "            continue\n",
        "        n_low = name.lower()\n",
        "        if \"offset\" in n_low or \"mask\" in n_low:\n",
        "            target_params.append(param)\n",
        "\n",
        "    if target_params:\n",
        "        torch.nn.utils.clip_grad_norm_(target_params, max_norm=max_norm)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Helper classes\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def fix_zero_init_params(model):\n",
        "    \"\"\"Fix parameters that are initialized to zero\"\"\"\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.dim() > 0 and torch.all(param == 0):\n",
        "            print(f\"Fixing zero-initialized parameter: {name}\")\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.01)\n",
        "            else:\n",
        "                nn.init.normal_(param, std=0.01)\n",
        "\n",
        "def apply_gradient_checkpointing(model):\n",
        "    \"\"\"Apply gradient checkpointing to backbone stages - IMPROVED\"\"\"\n",
        "    if hasattr(model, 'backbone'):\n",
        "        # Apply a checkpoint for each stage\n",
        "        for stage_name in ['st0', 'st1', 'st2', 'st3']:\n",
        "            if hasattr(model.backbone, stage_name):\n",
        "                stage = getattr(model.backbone, stage_name)\n",
        "                # Make the sequential module checkpoint-friendly\n",
        "                class CheckpointedSequential(nn.Module):\n",
        "                    def __init__(self, *layers):\n",
        "                        super().__init__()\n",
        "                        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "                    def forward(self, x):\n",
        "                        for layer in self.layers:\n",
        "                            x = checkpoint(layer, x, use_reentrant=False)\n",
        "                        return x\n",
        "\n",
        "                # Recreate Stage\n",
        "                checkpointed = CheckpointedSequential(*[block for block in stage])\n",
        "                setattr(model.backbone, stage_name, checkpointed)\n",
        "\n",
        "def init_conv(layer: nn.Conv2d, fan_out: bool = True):\n",
        "    \"\"\"Kaiming normal initialization\"\"\"\n",
        "    nn.init.kaiming_normal_(\n",
        "        layer.weight,\n",
        "        mode=\"fan_out\" if fan_out else \"fan_in\",\n",
        "        nonlinearity=\"relu\",\n",
        "    )\n",
        "    if layer.bias is not None:\n",
        "        nn.init.zeros_(layer.bias)\n",
        "\n",
        "\n",
        "class LNAct(nn.Module):\n",
        "    \"\"\"LayerNormProxy + SiLU\"\"\"\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNormProxy(channels)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.norm(x))\n",
        "\n",
        "\n",
        "class ConvLNAct(nn.Sequential):\n",
        "    \"\"\"Conv + LayerNorm + Act fusion\"\"\"\n",
        "    def __init__(self, in_ch: int, out_ch: int, k: int = 3,\n",
        "                s: int = 1, p: int = None, g: int = 1):\n",
        "        p = p if p is not None else k // 2\n",
        "        super().__init__(\n",
        "            nn.Conv2d(in_ch, out_ch, k, s, p, groups=g, bias=False),\n",
        "            LNAct(out_ch)\n",
        "        )\n",
        "        nn.init.kaiming_normal_(self[0].weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# LayerScale\n",
        "# -----------------------------------------------------------------------------\n",
        "class LayerScale(nn.Module):\n",
        "    \"\"\"\n",
        "    Learnable scalar per layer (γ) – Broadcast-Safe version.\n",
        "    Works with both (B,C,H,W) and (B,N,C) tensor layouts.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        inplace: bool = False,\n",
        "        init_values: float = 1.0,\n",
        "        depth: int | None = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.inplace = inplace\n",
        "        if depth is not None:\n",
        "            init_values = min(1.0, 0.5 * depth / 3)\n",
        "        self.weight = nn.Parameter(torch.ones(dim) * init_values)\n",
        "\n",
        "    def _shape_for(self, x: torch.Tensor) -> tuple[int, ...]:\n",
        "        \"\"\"\n",
        "        Determines the shape to broadcast the weight based on x's placement.\n",
        "        Priority: channel-end (… , C) → channel-first (B, C, …) → singular cases.\n",
        "        \"\"\"\n",
        "        C = self.weight.numel()\n",
        "        nd = x.dim()\n",
        "\n",
        "        # 4D  image: (B, C, H, W) veya (B, H, W, C)\n",
        "        if nd == 4:\n",
        "            if x.shape[1] == C:          # (B, C, H, W)\n",
        "                return (1, C, 1, 1)\n",
        "            if x.shape[-1] == C:         # (B, H, W, C) - nadir\n",
        "                return (1, 1, 1, C)\n",
        "\n",
        "        # 3D: (B, N, C) or (B, C, N) or (C, H, W)\n",
        "        if nd == 3:\n",
        "            if x.shape[-1] == C:         # (B, N, C)\n",
        "                return (1, 1, C)\n",
        "            if x.shape[1] == C:          # (B, C, N)\n",
        "                return (1, C, 1)\n",
        "            if x.shape[0] == C:          # (C, H, W) / (C, N, ?)such as extreme cases\n",
        "                return (C, 1, 1)\n",
        "\n",
        "        # 2D: (B, C) veya (C, B)\n",
        "        if nd == 2:\n",
        "            if x.shape[-1] == C:         # (B, C)\n",
        "                return (1, C)\n",
        "            if x.shape[0] == C:          # (C, B)\n",
        "                return (C, 1)\n",
        "\n",
        "\n",
        "        shape = [1] * max(1, nd)\n",
        "        shape[-1] = C\n",
        "        return tuple(shape)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        w = self.weight.view(*self._shape_for(x))\n",
        "        if self.inplace:\n",
        "            return x.mul_(w)\n",
        "        return x * w\n",
        "\n",
        "# Synchronize the LayerScale within DAT with this version (old behavior is preserved)\n",
        "dat_mod.LayerScale = LayerScale\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Stem\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class Stem(nn.Module):\n",
        "    \"\"\"Three 3×3 convolutions with stride pattern (2,1,2)\"\"\"\n",
        "    def __init__(self, out_channels: int = 128):\n",
        "        super().__init__()\n",
        "        c_mid = out_channels // 2\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, c_mid, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(c_mid, c_mid, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(c_mid, out_channels, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "        self.norm = LayerNormProxy(out_channels)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in [self.conv1, self.conv2, self.conv3]:\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.act(self.conv1(x))\n",
        "        x = self.act(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# DCNv4Lite\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class DCNv4Lite(nn.Module):\n",
        "    \"\"\"\n",
        "    • Fully wraps the DCNv4 path (2D→3D→2D), NO baseline conv/residual mix.\n",
        "    • Offset/mask parameters are initialized with small std (for stable fp32 training).\n",
        "    • Silently discards ‘mix_*’ and ‘conv.*’ keys in old checkpoints.\n",
        "\n",
        "    Args:\n",
        "        channels (int)            : Num of channels\n",
        "        group (int)               : DCNv4 group\n",
        "        kernel_size (int)         : Kernel\n",
        "        stride (int)              : Stride\n",
        "        pad (int)                 : Padding\n",
        "        dilation (int)            : Dilation\n",
        "        offset_scale (float)      : DCNv4 offset scale\n",
        "        without_pointwise (bool)  : Enable/disable the internal 1x1 project in DCNv4 (default: True)\n",
        "        init_offset_mask_std (float): offset/mask start std (default: 0.01)\n",
        "        **kwargs                  : It is forwarded to DCNv4 exactly as is.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 channels: int,\n",
        "                 group: int = 8,\n",
        "                 kernel_size: int = 3,\n",
        "                 stride: int = 1,\n",
        "                 pad: int = 1,\n",
        "                 dilation: int = 1,\n",
        "                 offset_scale: float = 0.1,\n",
        "                 *,\n",
        "                 without_pointwise: bool = True,\n",
        "                 init_offset_mask_std: float = 0.01,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        self.channels = int(channels)\n",
        "\n",
        "        #  DCNv4\n",
        "        self.dcn = DCNv4(\n",
        "            channels=channels, kernel_size=kernel_size, stride=stride, pad=pad,\n",
        "            dilation=dilation, group=group, offset_scale=offset_scale,\n",
        "            center_feature_scale=False, remove_center=False,\n",
        "            output_bias=True, without_pointwise=without_pointwise, **kwargs\n",
        "        )\n",
        "\n",
        "        # Offset/Mask secure init\n",
        "        self._init_dcn_params(std=init_offset_mask_std)\n",
        "\n",
        "        # Save geometry for representation (debug)\n",
        "        self._geom = dict(k=kernel_size, s=stride, p=pad, d=dilation, g=group,\n",
        "                          wop=without_pointwise)\n",
        "\n",
        "    # ---------------- init helpers ----------------\n",
        "    def _init_dcn_params(self, std: float = 0.01):\n",
        "        \"\"\"\n",
        "        Offset/mask parameters are initialized with small normals so that\n",
        "        sigmoid(0)=0.5 exits the plateau lock; other parameters remain in DCNv4's\n",
        "        default init.\n",
        "        \"\"\"\n",
        "        std = float(max(std, 1e-5))\n",
        "        for n, p in self.dcn.named_parameters():\n",
        "            n_low = n.lower()\n",
        "            if ('offset' in n_low) or ('mask' in n_low):\n",
        "                nn.init.normal_(p, std=std)\n",
        "\n",
        "    # ---------------- forward ----------------------\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, C, H, W) → y: (B, C, H, W)\n",
        "        The DCNv4 interface (B, HW, C) requires a 2D→3D→2D conversion.\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        x3d = x.permute(0, 2, 3, 1).contiguous().view(B, H * W, C)   # (B, HW, C)\n",
        "        y3d = self.dcn(x3d, shape=(H, W))                            # (B, HW, C)\n",
        "        y   = y3d.view(B, H, W, C).permute(0, 3, 1, 2).contiguous()  # (B, C, H, W)\n",
        "        return y\n",
        "\n",
        "    # --------------- helpers -------------------\n",
        "    @torch.no_grad()\n",
        "    def freeze_offsets(self, flag: bool = True):\n",
        "        \"\"\"Freeze/unfreeze offset/mask parameters (e.g., for warming).\"\"\"\n",
        "        for n, p in self.dcn.named_parameters():\n",
        "            n_low = n.lower()\n",
        "            if ('offset' in n_low) or ('mask' in n_low):\n",
        "                p.requires_grad = (not flag)\n",
        "\n",
        "    def offset_mask_parameters(self):\n",
        "        \"\"\"In the Optimizer, it yields the offset/mask parameters to provide separate LR/clip.\"\"\"\n",
        "        for n, p in self.dcn.named_parameters():\n",
        "            n_low = n.lower()\n",
        "            if ('offset' in n_low) or ('mask' in n_low):\n",
        "                yield p\n",
        "\n",
        "    # Ignore the mix/baseline keys remaining in old checkpoints\n",
        "    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):\n",
        "        obsolete = (\n",
        "            \"mix_weight\", \"mix_logit\", \"_mix_tau\", \"_mix_eps\", \"_mix_clamp\",\n",
        "            \"conv.weight\", \"conv.bias\"\n",
        "        )\n",
        "        for key in list(state_dict.keys()):\n",
        "            if key.startswith(prefix) and any(key.endswith(obs) for obs in obsolete):\n",
        "                state_dict.pop(key)\n",
        "        return super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        g = self._geom\n",
        "        return (f\"channels={self.channels}, k={g['k']}, s={g['s']}, p={g['p']}, \"\n",
        "                f\"d={g['d']}, group={g['g']}, without_pointwise={g['wop']}\")\n",
        "\n",
        "\n",
        "def boost_relative_position_bias(model: nn.Module, std: float = 0.02):\n",
        "    \"\"\"\n",
        "    Resets all `relative_position_bias_table` parameters in all ShiftWindow/LocalAttention modules belonging to the model.\n",
        "    \"\"\"\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"relative_position_bias_table\"):\n",
        "            nn.init.trunc_normal_(m.relative_position_bias_table, std=std)\n",
        "\n",
        "def init_decoder_sampling_offsets(decoder: nn.Module, bias_val: float = 0.1):\n",
        "    \"\"\"\n",
        "    Set the sampling_offsets bias in the deformable decoder layers\n",
        "    to a fixed value; a small shift triggers the gradient.\n",
        "    \"\"\"\n",
        "    for n, p in decoder.named_parameters():\n",
        "        if \"sampling_offsets.bias\" in n:\n",
        "            nn.init.constant_(p, bias_val)\n",
        "\n",
        "\n",
        "def apply_hybrid_fixup(model: nn.Module):\n",
        "    \"\"\"\n",
        "    a) Relative-pos bias table\n",
        "    b) Decoder sampling_offsets bias\n",
        "    c) (Optional) DCNv4 offset/mask gradient LR boost\n",
        "    \"\"\"\n",
        "    boost_relative_position_bias(model)\n",
        "    if hasattr(model, \"decoder\"):\n",
        "        init_decoder_sampling_offsets(model.decoder, 0.1)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Stage-0 Block\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class StageZeroBlock(nn.Module):\n",
        "    def __init__(self, channels=128, drop_path_rate=0.0,\n",
        "                 use_layer_scale=True, layer_scale_init=1.0,\n",
        "                 norm_factory=NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.conv3 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
        "        nn.init.kaiming_normal_(self.conv3.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "        self.se   = ChannelAttention(channels, reduction=8)\n",
        "        self.norm = norm_factory(channels)\n",
        "        self.scale = LayerScale(channels, init_values=layer_scale_init) if use_layer_scale else nn.Identity()\n",
        "        self.dp = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        out = self.conv3(x)\n",
        "        out = self.se(out)\n",
        "        out = self.norm(out)\n",
        "        out = self.scale(out)\n",
        "        out = self.dp(out)\n",
        "        return res + out\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Stage Blocks\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class StageOneBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    DCNv4Lite + SE + (optional) LayerScale + DropPath\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                channels: int = 128,\n",
        "                drop_path: float = 0.0,\n",
        "                use_layer_scale: bool = True,\n",
        "                layer_scale_init: float = 1.0,\n",
        "                norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.dcn = DCNv4Lite(channels, group=8)\n",
        "        self.se  = ChannelAttention(channels, reduction=8)\n",
        "        self.post_norm = norm_factory(channels)\n",
        "\n",
        "        self.scale = (LayerScale(channels, init_values=layer_scale_init)\n",
        "                      if use_layer_scale else nn.Identity())\n",
        "\n",
        "        self.dp = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        out = self.dcn(x)\n",
        "        out = self.se(out)\n",
        "        out = self.post_norm(out)\n",
        "        out = self.scale(out)\n",
        "        out = self.dp(out)\n",
        "        return res + out\n",
        "\n",
        "class DownABlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Depthwise‑Conv (s=2)  +  Pointwise‑Conv  +  Norm + Act\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                in_ch: int = 128,\n",
        "                out_ch: int = 256,\n",
        "                norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1) Depth‑wise strided conv\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1,\n",
        "                            groups=in_ch, bias=False)\n",
        "\n",
        "        # 2) Point‑wise projection\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "\n",
        "        # 3) Normalization + Activation\n",
        "        #    a) If the norm type is LayerNormProxy, use LNAct\n",
        "        #    b) In other cases, use norm + SiLU\n",
        "        nf_layer = norm_factory(out_ch)\n",
        "        if isinstance(nf_layer, LayerNormProxy):\n",
        "            # LNAct = LayerNormProxy + SiLU\n",
        "            from DAT.models.dat_blocks import LNAct as _LNAct\n",
        "            self.norm_act = _LNAct(out_ch)\n",
        "        else:\n",
        "            self.norm_act = nn.Sequential(nf_layer, nn.SiLU(inplace=True))\n",
        "\n",
        "        # --- Kaiming init ---\n",
        "        nn.init.kaiming_normal_(self.dw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.pw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.dw(x)\n",
        "        x = self.pw(x)\n",
        "        return self.norm_act(x)\n",
        "\n",
        "\n",
        "class StageTwoBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Stage‑2: Only DCNv4Lite path (no DAT, no GateFuse).\n",
        "    CSP split is kept for comparable FLOPs/latency.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 channels: int = 256,\n",
        "                 dcn_group: int = 16,\n",
        "                 drop_path: float = 0.1,\n",
        "                 norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        assert channels % 2 == 0, \"StageTwoBlock expects even channel count.\"\n",
        "        self.split_channels = channels // 2\n",
        "\n",
        "        # --- DCNv4 branch (only this one active) ---\n",
        "        self.dcn = DCNv4Lite(self.split_channels, group=max(1, dcn_group // 2))\n",
        "        self.post_norm = norm_factory(self.split_channels)\n",
        "        self.post_act  = nn.SiLU(inplace=True)\n",
        "\n",
        "        # --- Skip proj (for grad out) ---\n",
        "        self.skip_proj = nn.Sequential(\n",
        "            nn.Conv2d(self.split_channels, self.split_channels, 1, bias=False),\n",
        "            norm_factory(self.split_channels)\n",
        "        )\n",
        "\n",
        "        # --- Output merging ---\n",
        "        self.fusion = nn.Conv2d(channels, channels, 1, bias=False)\n",
        "        self.ln     = norm_factory(channels)\n",
        "        self.act    = nn.SiLU(inplace=True)\n",
        "        self.dp     = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "        # Init\n",
        "        nn.init.kaiming_normal_(self.fusion.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.skip_proj[0].weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # CSP split\n",
        "        x1, x2 = torch.split(x, self.split_channels, dim=1)\n",
        "\n",
        "        # Skip branch\n",
        "        x1 = self.skip_proj(x1)\n",
        "\n",
        "        # DCNv4 branch\n",
        "        x2 = self.dcn(x2)\n",
        "        x2 = self.post_act(self.post_norm(x2))\n",
        "\n",
        "        # Fuse & residual\n",
        "        out = torch.cat([x1, x2], dim=1)\n",
        "        out = self.fusion(out)\n",
        "        out = self.act(self.ln(out))\n",
        "        return x + self.dp(out)\n",
        "\n",
        "\n",
        "class DownBBlock(nn.Module):\n",
        "    \"\"\"DW 3×3 s2 → PW 1×1, 256c → 640c\"\"\"\n",
        "    def __init__(self, in_ch: int = 256, out_ch: int = 640,norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1,\n",
        "                          groups=in_ch, bias=False)\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.norm = norm_factory(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "        for m in [self.dw, self.pw]:\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.act(self.norm(self.pw(self.dw(x))))\n",
        "\n",
        "\n",
        "class StageThreeBlock(nn.Module):\n",
        "    \"\"\"Double DATHubLite + ChannelAttention\"\"\"\n",
        "    def __init__(self, in_ch: int = 640, out_ch: int = 640, drop_path: float = 0.15,norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.norm = norm_factory(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "        heads = max(4, out_ch // 64)\n",
        "        self.dat1 = DATBlock(out_ch, heads=heads)\n",
        "        self.dat2 = DATBlock(out_ch, heads=heads)\n",
        "        self.dp1 = DropPath(drop_path)\n",
        "        self.dp2 = DropPath(drop_path)\n",
        "        self.ca = ChannelAttention(out_ch, reduction=8)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.proj.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.act(self.norm(self.proj(x)))\n",
        "        res = x\n",
        "        x = self.dat1(x)\n",
        "        x = self.dp1(x) + res\n",
        "        res = x\n",
        "        x = self.dat2(x)\n",
        "        x = self.dp2(x) + res\n",
        "        x = self.ca(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DownCBlock(nn.Module):\n",
        "    \"\"\"DW 3×3 s2 → PW 1×1, 640c → 768c\"\"\"\n",
        "    def __init__(self, in_ch: int = 640, out_ch: int = 768,norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1,\n",
        "                          groups=in_ch, bias=False)\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.norm = norm_factory(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.dw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.pw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dw(x)\n",
        "        x = self.pw(x)\n",
        "\n",
        "        return self.act(self.norm(x))\n",
        "# -----------------------------------------------------------------------------\n",
        "# StageAwareBackbone\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class StageAwareBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    Four-tier backbone based on DAT + DCN\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                depths: list[int] = (3, 4, 8, 3),\n",
        "                drop_path_max: float = 0.0,\n",
        "                num_classes: int = 80,\n",
        "                voc_prior: bool = False,\n",
        "                norm_factory: NormFactory = NormFactory(\"gn\"),\n",
        "                use_layer_scale: bool = True,\n",
        "                layer_scale_init: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.voc_prior   = voc_prior\n",
        "\n",
        "        # ---- DropPath rates ----\n",
        "        total_blocks = sum(depths)\n",
        "        dpr = get_drop_path_rates(total_blocks, drop_path_max)\n",
        "        for i in range(depths[0] + depths[1]):   # st0, st1\n",
        "            dpr[i] = 0.0\n",
        "\n",
        "        # ---- Stem ----\n",
        "        self.stem = Stem(128)\n",
        "\n",
        "        # ---- Stage‑0 ----\n",
        "        dp_idx = 0\n",
        "        self.st0 = nn.Sequential(*[\n",
        "          StageZeroBlock(\n",
        "              128,\n",
        "              drop_path_rate=dpr[i],\n",
        "              use_layer_scale=use_layer_scale,\n",
        "              layer_scale_init=layer_scale_init,\n",
        "              norm_factory=norm_factory\n",
        "          )\n",
        "          for i in range(depths[0])\n",
        "      ])\n",
        "        dp_idx += depths[0]\n",
        "\n",
        "        # ---- Stage‑1 ----\n",
        "        self.st1 = nn.Sequential(*[\n",
        "            StageOneBlock(\n",
        "                128,\n",
        "                drop_path=dpr[dp_idx + i],\n",
        "                use_layer_scale=use_layer_scale,\n",
        "                layer_scale_init=layer_scale_init,\n",
        "                norm_factory=norm_factory)\n",
        "            for i in range(depths[1])\n",
        "        ])\n",
        "        dp_idx += depths[1]\n",
        "\n",
        "        # ---- PGI + Down/CSI ----\n",
        "        self.pgi_s1 = PGIModule(128, norm_factory=norm_factory, aux_channels=128)   # s0_out → s1_raw\n",
        "        self.da     = DownABlock(128, 256, norm_factory=norm_factory)\n",
        "        self.csi_s1_s2 = CrossScaleInjection(low_ch=128, high_ch=256)\n",
        "        self.pgi_s2 = PGIModule(256, norm_factory=norm_factory, aux_channels=128)\n",
        "\n",
        "        # ---- Stage‑2 ----\n",
        "        self.st2 = nn.Sequential(*[\n",
        "            StageTwoBlock(\n",
        "                256, dcn_group=16,\n",
        "                drop_path=dpr[dp_idx + i],\n",
        "                norm_factory=norm_factory)\n",
        "            for i in range(depths[2])\n",
        "        ])\n",
        "        dp_idx += depths[2]\n",
        "\n",
        "        # ---- DownB / PGI ----\n",
        "        self.db     = DownBBlock(256, 640, norm_factory=norm_factory)\n",
        "        self.pgi_s3 = PGIModule(640, norm_factory=norm_factory, aux_channels=256)\n",
        "\n",
        "        # ---- Stage‑3 ----\n",
        "        self.st3 = nn.Sequential(*[\n",
        "            StageThreeBlock(\n",
        "                640, 640,\n",
        "                drop_path=dpr[dp_idx + i],\n",
        "                norm_factory=norm_factory)\n",
        "            for i in range(depths[3])\n",
        "        ])\n",
        "\n",
        "        # ---- DownC ----\n",
        "        self.dc = DownCBlock(640, 768, norm_factory=norm_factory)\n",
        "\n",
        "        # ---- Auxiliary dense heads ----\n",
        "        self.aux_head_s1 = nn.Conv2d(128, num_classes + 4, 1, bias=True)\n",
        "        self.aux_head_s2 = nn.Conv2d(256, num_classes + 4, 1, bias=True)\n",
        "        self.aux_head_s3 = nn.Conv2d(640, num_classes + 4, 1, bias=True)\n",
        "        self._init_aux_heads()\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def _init_aux_heads(self):\n",
        "        prior_value = 19 if self.voc_prior else 99\n",
        "        for head in (self.aux_head_s1, self.aux_head_s2, self.aux_head_s3):\n",
        "            nn.init.normal_(head.weight, std=0.01)\n",
        "            nn.init.constant_(head.bias[: self.num_classes], -math.log(prior_value))\n",
        "            nn.init.constant_(head.bias[self.num_classes :], 0.0)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def forward(self, x: torch.Tensor, need_aux: bool = False):\n",
        "        x      = self.stem(x)\n",
        "        s0_out = self.st0(x)\n",
        "\n",
        "        s1_raw = self.st1(s0_out)\n",
        "        p3     = self.pgi_s1(s1_raw, aux_input=s0_out)\n",
        "\n",
        "        p3_down = self.da(p3)\n",
        "        s2_in   = self.csi_s1_s2(p3, p3_down)\n",
        "        s2_in   = self.pgi_s2(s2_in, aux_input=p3)\n",
        "\n",
        "        p4 = self.st2(s2_in)\n",
        "        p4_down = self.db(p4)\n",
        "        s3_in   = self.pgi_s3(p4_down, aux_input=p4)\n",
        "\n",
        "        p5 = self.st3(s3_in)\n",
        "        p6 = self.dc(p5)\n",
        "\n",
        "        if self.training and need_aux:\n",
        "            aux = {\n",
        "                's1': self.aux_head_s1(p3),\n",
        "                's2': self.aux_head_s2(p4),\n",
        "                's3': self.aux_head_s3(p5),\n",
        "            }\n",
        "            return (p3, p4, p5, p6), aux\n",
        "        return p3, p4, p5, p6\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# SpatialFuseNode\n",
        "# ----------------------------------------------------------\n",
        "class SpatialFuseNode(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatial-aware fuse (group-wise, production-ready, RPB-friendly)\n",
        "      • Group-average energy map per input (B,G,K,H,W)\n",
        "      • 1×1 projection per group (K→K), softplus-normalized\n",
        "      • Initial fully uniform: proj.weight=0, bias=0  → equal w for each branch\n",
        "      • Learnable temperature τ (with clamp), optional uniform floor (no dead branches)\n",
        "      • No detach anywhere → gradients are unclipped (including RPBs)\n",
        "\n",
        "    Args:\n",
        "        n_inputs (int): K, how many features to combine (>=2)\n",
        "        channels (int): C, number of channels\n",
        "        groups (int): G, number of groups (automatically drops to 1 if C % G ≠ 0)\n",
        "        tau_init (float): τ start (recommended: 0.9)\n",
        "        learnable_tau (bool): Should τ be learned?\n",
        "        eps (float): Numerical epsilon for normalization and splitting\n",
        "        init_noise (float): (backward compatibility parameter; not used with uniform init)\n",
        "        gate_floor (float): w ← (1-α)·w + α·(1/K); α∈[0,0.1] recommended, 0 closed\n",
        "        tau_bounds (tuple): τ lower/upper bounds (e.g., (0.5, 2.0))\n",
        "        uniform_init (bool): True ⇒ weight=0, bias=0 (full uniform initialization)\n",
        "        save_last (bool): If True, last w is saved (for debug/regularizer)\n",
        "        prenorm (str): “none” | “rms”  — RMS prenorm on the K axis (reduces saturation)\n",
        "\n",
        "    Notes:\n",
        "      • Returns extra_loss() for the entropy regularizer (optional, can be added to the loss).\n",
        "      • The last w (B,G,K,H,W) can be inspected with get_last_weights() (if save_last=True).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_inputs: int,\n",
        "                 channels: int,\n",
        "                 groups: int = 4,\n",
        "                 tau_init: float = 0.9,\n",
        "                 learnable_tau: bool = True,\n",
        "                 eps: float = 5e-4,\n",
        "                 init_noise: float = 1e-3,\n",
        "                 *,\n",
        "                 gate_floor: float = 0.0,\n",
        "                 tau_bounds: tuple = (0.5, 2.0),\n",
        "                 uniform_init: bool = True,\n",
        "                 save_last: bool = False,\n",
        "                 prenorm: str = \"none\"):\n",
        "        super().__init__()\n",
        "        assert n_inputs >= 2, \"SpatialFuseNode: should be n_inputs >= 2 .\"\n",
        "        self.K = int(n_inputs)\n",
        "        self.C = int(channels)\n",
        "        self.G = int(groups) if (groups > 0 and channels % groups == 0) else 1\n",
        "        self.gC = self.C // self.G\n",
        "        self.eps = float(eps)\n",
        "        self.gate_floor = float(gate_floor)\n",
        "        self.tau_lo, self.tau_hi = float(tau_bounds[0]), float(tau_bounds[1])\n",
        "        self.save_last = bool(save_last)\n",
        "        self.prenorm = str(prenorm).lower()\n",
        "        self._last_w = None  # debug amaçlı\n",
        "\n",
        "        # Group-wise K→K projection (produces spatially-varying logit)\n",
        "        # Takes input divided into G groups (B, G*K, H, W) → Output (B, G*K, H, W)\n",
        "        self.proj = nn.Conv2d(self.G * self.K, self.G * self.K, kernel_size=1,\n",
        "                              groups=self.G, bias=True)\n",
        "\n",
        "        # --- Uniform start (to prevent early arm saturation) ---\n",
        "        if uniform_init:\n",
        "            nn.init.zeros_(self.proj.bias)\n",
        "            with torch.no_grad():\n",
        "                self.proj.weight.zero_()\n",
        "        else:\n",
        "            nn.init.kaiming_uniform_(self.proj.weight, a=math.sqrt(5))\n",
        "            nn.init.zeros_(self.proj.bias)\n",
        "\n",
        "        # --- temperature τ ---\n",
        "        if learnable_tau:\n",
        "            self.log_tau = nn.Parameter(torch.log(torch.tensor(float(tau_init))))\n",
        "        else:\n",
        "            self.register_buffer(\"log_tau\", torch.log(torch.tensor(float(tau_init))), persistent=False)\n",
        "\n",
        "    # ------------------ helpers ------------------\n",
        "    @torch.no_grad()\n",
        "    def set_tau(self, tau: float):\n",
        "        \"\"\"To adjust τ during heating.\"\"\"\n",
        "        v = max(1e-3, float(tau))\n",
        "        t = torch.log(torch.tensor(v, device=self.log_tau.device, dtype=self.log_tau.dtype))\n",
        "        self.log_tau.copy_(t)\n",
        "\n",
        "    def get_last_weights(self):\n",
        "        \"\"\"The last calculated w (B, G, K, H, W). If save_last=True, it is saved.\"\"\"\n",
        "        return self._last_w\n",
        "\n",
        "    def extra_loss(self) -> dict:\n",
        "        \"\"\"\n",
        "        Optional regulator: gate entropy (higher values result in more balanced branching).\n",
        "        You can add a small coefficient on the loss side (e.g., 1e-4..5e-4).\n",
        "        \"\"\"\n",
        "        if self._last_w is None:\n",
        "            return {}\n",
        "        p = self._last_w.clamp_min(1e-8)\n",
        "        # Entropy: -sum_k p log p / log(K)  → [0,1]\n",
        "        ent = -(p * p.log()).sum(dim=2) / math.log(self.K)   # (B,G,H,W)\n",
        "        return {\"gate_entropy\": ent.mean()}\n",
        "\n",
        "    def _group_pool(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # (B,C,H,W) → (B,G,H,W), average per group\n",
        "        B, C, H, W = x.shape\n",
        "        return x.view(B, self.G, self.gC, H, W).mean(dim=2)\n",
        "\n",
        "    # ------------------ forward ------------------\n",
        "    def forward(self, *features: torch.Tensor) -> torch.Tensor:\n",
        "        # All inputs must be in the same format.\n",
        "        K = len(features)\n",
        "        assert K == self.K, f\"SpatialFuseNode: {self.K} giriş bekleniyordu, {K} geldi.\"\n",
        "        B, C, H, W = features[0].shape\n",
        "        for f in features:\n",
        "            assert f.shape == (B, C, H, W), \"Tüm giriş feature'lar (B,C,H,W) aynı şekil olmalı.\"\n",
        "\n",
        "        # 1) Grup havuzu → (B,G,K,H,W)\n",
        "        gp = [self._group_pool(f) for f in features]\n",
        "        gp_cat = torch.stack(gp, dim=2)  # (B,G,K,H,W)\n",
        "\n",
        "        # 2) (optional) RMS prenorm → logit scale control on the K axis\n",
        "        if self.prenorm == \"rms\":\n",
        "            rms = gp_cat.pow(2).mean(dim=2, keepdim=True).add(1e-6).sqrt()\n",
        "            gp_cat = gp_cat / rms\n",
        "\n",
        "        # 3) Projection and gate logits\n",
        "        x = gp_cat.flatten(1, 2)                      # (B, G*K, H, W)\n",
        "        logits = self.proj(x).view(B, self.G, self.K, H, W)\n",
        "\n",
        "        # 4) Softplus-normalize + τ\n",
        "        tau = self.log_tau.exp().clamp(self.tau_lo, self.tau_hi)\n",
        "        w = F.softplus(logits / tau) + 1e-9           # (B,G,K,H,W), her yerde >0\n",
        "        w = w / (w.sum(dim=2, keepdim=True) + self.eps)\n",
        "\n",
        "        # 5) Uniform floor (no dead leg, gradient flows to every leg)\n",
        "        if self.gate_floor > 0.0:\n",
        "            u = 1.0 / float(self.K)\n",
        "            w = (1.0 - self.gate_floor) * w + self.gate_floor * u  # no need to normalize again\n",
        "\n",
        "        if self.save_last:\n",
        "            self._last_w = w.detach()\n",
        "\n",
        "        # 6) Distribute weights to channels efficiently (no repeats, memory-friendly)\n",
        "        #    w_k: (B,G,1,H,W), f: (B,G,gC,H,W) → contribution (B,C,H,W)\n",
        "        out = None\n",
        "        for k, f in enumerate(features):\n",
        "            wk = w[:, :, k, :, :].unsqueeze(2)                 # (B,G,1,H,W)\n",
        "            contrib = (f.view(B, self.G, self.gC, H, W) * wk).view(B, C, H, W)\n",
        "            out = contrib if out is None else (out + contrib)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class RMSNorm2d(nn.Module):\n",
        "    def __init__(self, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = float(eps)\n",
        "    def forward(self, x):\n",
        "        return x / (x.pow(2).mean(dim=(2,3), keepdim=True).add(self.eps).sqrt())\n",
        "\n",
        "def _init_laplacian_dw(dw: nn.Conv2d):\n",
        "    with torch.no_grad():\n",
        "        k = torch.tensor([[0., 1., 0.],\n",
        "                          [1.,-4., 1.],\n",
        "                          [0., 1., 0.]], dtype=dw.weight.dtype, device=dw.weight.device)\n",
        "        w = torch.zeros_like(dw.weight)\n",
        "        w[:, 0, :, :] = k\n",
        "        dw.weight.copy_(w)\n",
        "\n",
        "def _ste_boost(x: torch.Tensor, gain: float) -> torch.Tensor:\n",
        "    return x if gain <= 0 else (x + gain * (x - x.detach()))\n",
        "\n",
        "class _FPNResBlock(nn.Module):\n",
        "    def __init__(self, channels: int, drop_path: float = 0.0, norm_factory: 'NormFactory' = None):\n",
        "        super().__init__()\n",
        "        nf = norm_factory or NormFactory(\"gn\")\n",
        "        self.conv = ConvLNAct(channels, channels, k=3)\n",
        "        self.ls   = LayerScale(channels, init_values=0.6)\n",
        "        self.dp   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "    def forward(self, x):\n",
        "        return x + self.dp(self.ls(self.conv(x)))\n",
        "\n",
        "class LightBiFPN(nn.Module):\n",
        "    \"\"\"\n",
        "    LightBiFPN v4.1 — spatial‑aware, RPB‑friendly, repeat‑shared up/edge\n",
        "      • SpatialFuseNode (group‑based 2D weights)\n",
        "      • No‑blur in first top‑down (bilinear + norm/act)\n",
        "      • Up-refine: DW 3×3 + (optional) Laplacian residual (small LS)\n",
        "      • RMS prenorm (optional)\n",
        "      • Low DropPath schedule (0.0 → 0.01)\n",
        "      • Light grad-boost for p3\n",
        "      • **New**: 3 up/edge blocks and **shared between iterations**\n",
        "                  → No “NEVER UPDATED”, no parameter waste\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: tuple[int,int,int,int] = (128, 256, 640, 768),\n",
        "        out: int = 256,\n",
        "        repeats: int = 2,\n",
        "        *,\n",
        "        norm_factory: 'NormFactory' | None = None,\n",
        "        # ---- fuse ----\n",
        "        use_spatial_fuse: bool = True,\n",
        "        fuse_groups: int = 4,\n",
        "        fuse_tau_init: float = 0.9,\n",
        "        fuse_learn_tau: bool = True,\n",
        "        fuse_eps: float = 5e-4,\n",
        "        init_noise: float = 1e-4,\n",
        "        # ---- refine & norm ----\n",
        "        prenorm: str = \"rms\",            # \"none\" | \"rms\"\n",
        "        no_blur_first: bool = True,\n",
        "        edge_enhance: bool = True,\n",
        "        edge_ls_init: float = 0.03,\n",
        "        # ---- drop path ----\n",
        "        dp_top_second: float = 0.01,\n",
        "        dp_bot_second: float = 0.01,\n",
        "        # ---- grad boost ----\n",
        "        grad_boost_low: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        in3, in4, in5, in6 = in_channels\n",
        "        self.repeats = int(repeats)\n",
        "        self.out = int(out)\n",
        "        self.norm_factory = norm_factory or NormFactory(\"gn\")\n",
        "        self.prenorm = prenorm.lower()\n",
        "        self.no_blur_first = bool(no_blur_first)\n",
        "        self.edge_enhance = bool(edge_enhance)\n",
        "        self.grad_boost_low = float(grad_boost_low)\n",
        "        self.use_spatial_fuse = bool(use_spatial_fuse)\n",
        "        self.fuse_groups = int(fuse_groups)\n",
        "        self.fuse_tau_init = float(fuse_tau_init)\n",
        "        self.fuse_learn_tau = bool(fuse_learn_tau)\n",
        "        self.fuse_eps = float(fuse_eps)\n",
        "        self.init_noise = float(init_noise)\n",
        "\n",
        "        # 1x1 projections\n",
        "        self.p3_in = ConvLNAct(in3, out, k=1, p=0)\n",
        "        self.p4_in = ConvLNAct(in4, out, k=1, p=0)\n",
        "        self.p5_in = ConvLNAct(in5, out, k=1, p=0)\n",
        "        self.p6_in = ConvLNAct(in6, out, k=1, p=0)\n",
        "\n",
        "        # DropPath schedule\n",
        "        top_dps = [0.0, float(dp_top_second)]\n",
        "        bot_dps = [0.0, float(dp_bot_second)]\n",
        "        self.top_convs = nn.ModuleList([\n",
        "            _FPNResBlock(out, drop_path=top_dps[i // 3], norm_factory=self.norm_factory)\n",
        "            for i in range(3 * self.repeats)\n",
        "        ])\n",
        "        self.bot_convs = nn.ModuleList([\n",
        "            _FPNResBlock(out, drop_path=bot_dps[i // 4], norm_factory=self.norm_factory)\n",
        "            for i in range(4 * self.repeats)\n",
        "        ])\n",
        "\n",
        "        # Fuse nodes\n",
        "        def _make_fuse(K: int):\n",
        "            from typing import Callable\n",
        "\n",
        "            return SpatialFuseNode(\n",
        "                                      K, channels=out, groups=self.fuse_groups,\n",
        "                                      tau_init=1.4, learnable_tau=True,\n",
        "                                      tau_bounds=(0.8, 1.8),\n",
        "                                      prenorm=\"rms\",\n",
        "                                      gate_floor=0.02,          # no death\n",
        "                                      uniform_init=True,\n",
        "                                      eps=5e-4,\n",
        "                                      save_last=True\n",
        "                                  )\n",
        "\n",
        "        self.fuse2_top = nn.ModuleList([_make_fuse(2) for _ in range(3 * self.repeats)])\n",
        "        self.fuse2_bot = nn.ModuleList([_make_fuse(2) for _ in range(3 * self.repeats)])\n",
        "        self.fuse3_bot = nn.ModuleList([_make_fuse(3) for _ in range(1 * self.repeats)])\n",
        "\n",
        "        # ---  3 up/edge blocks and shared between repeats ---\n",
        "        self.up_dw, self.up_na = nn.ModuleList(), nn.ModuleList()\n",
        "        self.edge_dw, self.edge_ls = nn.ModuleList(), nn.ModuleList()\n",
        "        for _ in range(3):  # only for 3 items\n",
        "            dw = nn.Conv2d(out, out, 3, 1, 1, groups=out, bias=False)\n",
        "            nn.init.kaiming_normal_(dw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            self.up_dw.append(dw)\n",
        "\n",
        "            nf_layer = self.norm_factory(out)\n",
        "            if isinstance(nf_layer, LayerNormProxy):\n",
        "                from DAT.models.dat_blocks import LNAct as _LNAct\n",
        "                self.up_na.append(_LNAct(out))\n",
        "            else:\n",
        "                self.up_na.append(nn.Sequential(nf_layer, nn.SiLU(inplace=True)))\n",
        "\n",
        "            edw = nn.Conv2d(out, out, 3, 1, 1, groups=out, bias=False)\n",
        "            _init_laplacian_dw(edw)\n",
        "            self.edge_dw.append(edw)\n",
        "            self.edge_ls.append(LayerScale(out, init_values=edge_ls_init))\n",
        "\n",
        "        # High-level passthrough LS (p5, p4, p3)\n",
        "        self.keep_top = nn.ModuleList([LayerScale(out, init_values=0.10) for _ in range(3)])\n",
        "\n",
        "        # Input balancing\n",
        "        self.balance = RMSNorm2d(eps=1e-6) if self.prenorm == \"rms\" else nn.Identity()\n",
        "\n",
        "    # --- helpers ---\n",
        "    def _up_refine_shared(self, x: torch.Tensor, size_hw: tuple[int,int], step_id: int, rep_idx: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        step_id: [0,1,2] → corresponds to each top-down step, independent of repeat\n",
        "        \"\"\"\n",
        "        x = F.interpolate(x, size=size_hw, mode=\"bilinear\", align_corners=False)\n",
        "        if self.no_blur_first and rep_idx == 0:\n",
        "            # no-blur first repeat: only norm/act\n",
        "            x = self.up_na[step_id](x)\n",
        "        else:\n",
        "            x = self.up_dw[step_id](x)\n",
        "            if self.edge_enhance:\n",
        "                x = x + self.edge_ls[step_id](self.edge_dw[step_id](x))\n",
        "            x = self.up_na[step_id](x)\n",
        "        return x\n",
        "\n",
        "    def _bal(self, *xs):\n",
        "        return (tuple(self.balance(x) for x in xs) if self.prenorm != \"none\" else xs)\n",
        "\n",
        "    def forward(self, p3, p4, p5, p6):\n",
        "        # Proj\n",
        "        p3, p4, p5, p6 = self.p3_in(p3), self.p4_in(p4), self.p5_in(p5), self.p6_in(p6)\n",
        "\n",
        "        # Low-level grad-boost\n",
        "        if self.training and self.grad_boost_low > 0:\n",
        "            p3 = _ste_boost(p3, self.grad_boost_low)\n",
        "\n",
        "        tconv = bconv = f2t = f2b = f3b = 0\n",
        "\n",
        "        for rep in range(self.repeats):\n",
        "            # ---------- Top‑down ----------\n",
        "            # step_id = 0: p6→p5\n",
        "            u5 = self._up_refine_shared(p6, p5.shape[-2:], step_id=0, rep_idx=rep)\n",
        "            p5_b, u5_b = self._bal(p5, u5)\n",
        "            p5_td = self.fuse2_top[f2t](p5_b, u5_b); f2t += 1\n",
        "            p5_td = self.top_convs[tconv](p5_td); tconv += 1\n",
        "            p5_td = p5_td + self.keep_top[0](p5)\n",
        "\n",
        "            # step_id = 1: p5_td→p4\n",
        "            u4 = self._up_refine_shared(p5_td, p4.shape[-2:], step_id=1, rep_idx=rep)\n",
        "            p4_b, u4_b = self._bal(p4, u4)\n",
        "            p4_td = self.fuse2_top[f2t](p4_b, u4_b); f2t += 1\n",
        "            p4_td = self.top_convs[tconv](p4_td); tconv += 1\n",
        "            p4_td = p4_td + self.keep_top[1](p4)\n",
        "\n",
        "            # step_id = 2: p4_td→p3\n",
        "            u3 = self._up_refine_shared(p4_td, p3.shape[-2:], step_id=2, rep_idx=rep)\n",
        "            p3_b, u3_b = self._bal(p3, u3)\n",
        "            p3_td = self.fuse2_top[f2t](p3_b, u3_b); f2t += 1\n",
        "            p3_td = self.top_convs[tconv](p3_td); tconv += 1\n",
        "            p3_td = p3_td + self.keep_top[2](p3)\n",
        "\n",
        "            # ---------- Bottom‑up ----------\n",
        "            p3 = self.fuse2_bot[f2b](*self._bal(p3, p3_td)); f2b += 1\n",
        "            p3 = self.bot_convs[bconv](p3); bconv += 1\n",
        "\n",
        "            p4 = self.fuse2_bot[f2b](*self._bal(p4_td, F.max_pool2d(p3, 2))); f2b += 1\n",
        "            p4 = self.bot_convs[bconv](p4); bconv += 1\n",
        "\n",
        "            p5 = self.fuse3_bot[f3b](*self._bal(p5, F.max_pool2d(p4, 2), p5_td)); f3b += 1\n",
        "            p5 = self.bot_convs[bconv](p5); bconv += 1\n",
        "\n",
        "            p6 = self.fuse2_bot[f2b](*self._bal(p6, F.max_pool2d(p5, 2))); f2b += 1\n",
        "            p6 = self.bot_convs[bconv](p6); bconv += 1\n",
        "\n",
        "        return p3, p4, p5, p6\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Positional Encoding\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class Pos2d(nn.Module):\n",
        "    \"\"\"2D sinusoidal positional encoding\"\"\"\n",
        "    def __init__(self, c: int = 320, max_h: int = 640, max_w: int = 640):\n",
        "        super().__init__()\n",
        "        assert c % 4 == 0\n",
        "        self.cq = c // 4\n",
        "        pos = self._build(max_h, max_w, c)\n",
        "        self.register_buffer(\"pos_table\", pos, persistent=False)\n",
        "\n",
        "    def _build(self, H: int, W: int, C: int) -> torch.Tensor:\n",
        "        yv, xv = torch.meshgrid(\n",
        "            torch.linspace(0, 1, H), torch.linspace(0, 1, W), indexing=\"ij\"\n",
        "        )\n",
        "        div = torch.exp(torch.arange(0, self.cq) * (-math.log(10000.0) / self.cq))\n",
        "        pos_x = (xv[..., None] * div).reshape(H, W, -1)\n",
        "        pos_y = (yv[..., None] * div).reshape(H, W, -1)\n",
        "        pos = torch.cat([pos_y.sin(), pos_y.cos(), pos_x.sin(), pos_x.cos()], dim=2)\n",
        "        return pos.permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        _, _, H, W = x.shape\n",
        "        return self.pos_table[:, :, :H, :W]\n",
        "\n",
        "\n",
        "\n",
        "import inspect\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# DATBlock –  TransformerStage wrapper\n",
        "# -----------------------------------------------------------------------------\n",
        "class DATBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                channels: int,\n",
        "                depth: int = 2,\n",
        "                heads: int | None = None,\n",
        "                window_size: int = 8,\n",
        "                drop: float = 0.0):\n",
        "        super().__init__()\n",
        "        heads = heads or max(4, channels // 32)\n",
        "\n",
        "        cfg = dict(\n",
        "            fmap_size           = (window_size, window_size),\n",
        "            window_size         = window_size,\n",
        "            ns_per_pt           = 4,\n",
        "            dim_in              = channels,\n",
        "            dim_embed           = channels,\n",
        "            depths              = depth,               # INT\n",
        "            stage_spec          = 'N' * depth,\n",
        "            n_groups            = 1,\n",
        "            use_pe              = True,\n",
        "            sr_ratio            = 1,\n",
        "            heads               = heads,               # INT\n",
        "            heads_q             = [heads] * depth,     # LIST\n",
        "            stride              = 1,\n",
        "            offset_range_factor = 2,\n",
        "            dwc_pe              = True,\n",
        "            no_off              = False,\n",
        "            fixed_pe            = False,\n",
        "            attn_drop           = drop,\n",
        "            proj_drop           = drop,\n",
        "            expansion           = 4,\n",
        "            drop                = drop,\n",
        "            drop_path_rate      = [drop] * depth,      # LIST\n",
        "            use_dwc_mlp         = False,\n",
        "            ksize               = 3,\n",
        "            nat_ksize           = 7,\n",
        "            k_qna               = 8,\n",
        "            nq_qna              = 9,\n",
        "            qna_activation      = \"relu\",\n",
        "            layer_scale_value   = 0.3,\n",
        "            use_lpu             = False,\n",
        "            log_cpb             = True,\n",
        "        )\n",
        "\n",
        "        from DAT.models.dat import TransformerStage\n",
        "        self.stage = TransformerStage(**cfg)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x = (B,C,H,W) –\n",
        "        if hasattr(self.stage, 'fmap_size'):\n",
        "            self.stage.fmap_size = x.shape[-2:]\n",
        "        try:\n",
        "            return self.stage(x)\n",
        "        except TypeError:                           # some versions (x,H,W) require\n",
        "            H, W = x.shape[-2:]\n",
        "            return self.stage(x, H, W)\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Deformable Encoder\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class GLUFFN(nn.Module):\n",
        "    \"\"\"\n",
        "    GLU-based FFN:\n",
        "      - First linear: d_model -> 2*inner\n",
        "      - Gate: SwiGLU (SiLU) or GEGLU (GELU)\n",
        "      - Second linear: inner -> d_model\n",
        "\n",
        "    inner width:\n",
        "      • if ffn_dim is given: inner = ffn_dim // 2  (2*inner = ffn_dim)\n",
        "      • otherwise: inner = round(d_model * ffn_mult)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        ffn_dim: Optional[int] = None,\n",
        "        ffn_mult: float = 2.0,\n",
        "        act: str = \"swiglu\",\n",
        "        drop: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if ffn_dim is not None:\n",
        "            inner = max(32, int(ffn_dim) // 2)\n",
        "        else:\n",
        "            inner = max(32, int(round(d_model * float(ffn_mult))))\n",
        "        self.fc1 = nn.Linear(d_model, inner * 2, bias=True)\n",
        "        self.fc2 = nn.Linear(inner, d_model, bias=True)\n",
        "        self.drop = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "\n",
        "        act = act.lower()\n",
        "        if act not in (\"swiglu\", \"geglu\"):\n",
        "            raise ValueError(\"GLUFFN.act must be 'swiglu' or 'geglu'\")\n",
        "        self.act_kind = act\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        xh = self.fc1(x)\n",
        "        x_lin, x_gate = xh.chunk(2, dim=-1)\n",
        "        if self.act_kind == \"swiglu\":\n",
        "            gated = F.silu(x_gate) * x_lin\n",
        "        else:  # 'geglu'\n",
        "            gated = F.gelu(x_gate) * x_lin\n",
        "        return self.drop(self.fc2(gated))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DeformEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    MS-Deformable self-attention + GLU-FFN, Pre-Norm + LayerScale + DropPath.\n",
        "\n",
        "    Args (backward-compatible):\n",
        "        d_model, n_heads, n_levels, n_points\n",
        "        ffn_dim:   optional (if given, GLU inner width is ffn_dim//2)\n",
        "        ffn_mult:  used if ffn_dim is not specified (default 2.0)\n",
        "        ffn_act:   ‘swiglu’ (default) or ‘geglu’\n",
        "        drop, drop_path\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int = 320,\n",
        "        n_heads: int = 10,\n",
        "        n_levels: int = 4,\n",
        "        n_points: int = 4,\n",
        "        *,\n",
        "        ffn_dim: Optional[int] = None,\n",
        "        ffn_mult: float = 2.0,\n",
        "        ffn_act: str = \"swiglu\",\n",
        "        drop: float = 0.0,\n",
        "        drop_path: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn = MultiScaleDeformableAttention(\n",
        "            embed_dims=d_model, num_heads=n_heads,\n",
        "            num_levels=n_levels, num_points=n_points,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ls1   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop1 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp1   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        self.ffn   = GLUFFN(d_model, ffn_dim=ffn_dim, ffn_mult=ffn_mult, act=ffn_act, drop=drop)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ls2   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop2 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp2   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_encoder_reference_points(\n",
        "        spatial_shapes: torch.Tensor, B: int, device, dtype\n",
        "    ) -> torch.Tensor:\n",
        "        ref_list = []\n",
        "        for (H, W) in spatial_shapes.tolist():\n",
        "            ref_y, ref_x = torch.meshgrid(\n",
        "                torch.linspace(0.5, H - 0.5, H, device=device, dtype=dtype) / H,\n",
        "                torch.linspace(0.5, W - 0.5, W, device=device, dtype=dtype) / W,\n",
        "                indexing=\"ij\",\n",
        "            )\n",
        "            ref = torch.stack((ref_x, ref_y), dim=-1).reshape(-1, 2)  # (HW,2)\n",
        "            ref_list.append(ref)\n",
        "        return torch.cat(ref_list, dim=0)[None, :, None, :].repeat(B, 1, spatial_shapes.size(0), 1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.Tensor,             # (B, sumHW, C)\n",
        "        pos: torch.Tensor,             # (B, sumHW, C)\n",
        "        spatial_shapes: torch.Tensor,  # (L, 2)\n",
        "        lvl_start_idx: torch.Tensor,   # (L,)\n",
        "        key_padding_mask: Optional[torch.Tensor] = None,\n",
        "    ) -> torch.Tensor:\n",
        "\n",
        "        B, N, C = src.shape\n",
        "        ref_pts = self._make_encoder_reference_points(\n",
        "            spatial_shapes=spatial_shapes, B=B, device=src.device, dtype=src.dtype\n",
        "        )  # (B,N,L,2)\n",
        "\n",
        "        x = src\n",
        "        q = self.norm1(x)\n",
        "        attn_out = self.self_attn(\n",
        "            query=q, value=q,\n",
        "            reference_points=ref_pts,\n",
        "            spatial_shapes=spatial_shapes,\n",
        "            level_start_index=lvl_start_idx,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            query_pos=pos,\n",
        "        )\n",
        "        x = x + self.dp1(self.ls1(self.drop1(attn_out)))\n",
        "\n",
        "        y = self.ffn(self.norm2(x))\n",
        "        x = x + self.dp2(self.ls2(self.drop2(y)))\n",
        "        return x\n",
        "\n",
        "class TinyDeformEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Two layers are recommended (lightweight and effective).\n",
        "    get_drop_path_rates() is available;\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_layers: int = 1,\n",
        "                 d_model: int = 320,\n",
        "                 n_heads: int = 10,\n",
        "                 n_levels: int = 4,\n",
        "                 n_points: int = 4,\n",
        "                 ffn_dim: int = 1024,\n",
        "                 drop: float = 0.0,\n",
        "                 drop_path_max: float = 0.1):\n",
        "        super().__init__()\n",
        "        dpr = get_drop_path_rates(num_layers, drop_path_max)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DeformEncoderLayer(d_model=d_model,\n",
        "                               n_heads=n_heads,\n",
        "                               n_levels=n_levels,\n",
        "                               n_points=n_points,\n",
        "                               ffn_dim=ffn_dim,\n",
        "                               drop=drop,\n",
        "                               drop_path=dpr[i])\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self,\n",
        "                src: torch.Tensor,             # (B, sumHW, C)\n",
        "                pos: torch.Tensor,             # (B, sumHW, C)\n",
        "                spatial_shapes: torch.Tensor,  # (L,2)\n",
        "                lvl_start_idx: torch.Tensor,   # (L,)\n",
        "                key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        for ly in self.layers:\n",
        "            src = ly(src, pos, spatial_shapes, lvl_start_idx, key_padding_mask)\n",
        "        return src\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# RT-Deform Decoder (Fixed IoU-aware padding)\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class DeformDecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder:\n",
        "      Pre-Norm + LayerScale + DropPath + GLU-FFN + tanh-bounded ref update.\n",
        "\n",
        "    Args (backward-compatible):\n",
        "        d_model, n_heads, n_levels, n_points\n",
        "        ffn_dim:   optional (for GLU inner width, 2*inner = ffn_dim)\n",
        "        ffn_mult:  used if ffn_dim is not specified\n",
        "        ffn_act:   ‘swiglu’ | ‘geglu’\n",
        "        drop, drop_path\n",
        "        refine_scale: float or (lo,hi)  — you can provide layer_id/num_layers for the schedule\n",
        "        grad_eps: very small nudge\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int = 320,\n",
        "        n_heads: int = 10,\n",
        "        n_levels: int = 4,\n",
        "        n_points: int = 4,\n",
        "        *,\n",
        "        ffn_dim: Optional[int] = None,\n",
        "        ffn_mult: float = 2.0,\n",
        "        ffn_act: str = \"swiglu\",\n",
        "        drop: float = 0.0,\n",
        "        drop_path: float = 0.1,\n",
        "        refine_scale: Union[float, Tuple[float, float]] = 0.5,\n",
        "        layer_id: Optional[int] = None,\n",
        "        num_layers: Optional[int] = None,\n",
        "        grad_eps: float = 1e-4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.grad_eps = float(grad_eps)\n",
        "\n",
        "        # refine scale\n",
        "        if isinstance(refine_scale, (tuple, list)):\n",
        "            lo, hi = float(refine_scale[0]), float(refine_scale[1])\n",
        "            if (num_layers is not None) and (layer_id is not None) and (num_layers > 1):\n",
        "                t = float(layer_id) / float(num_layers - 1)\n",
        "                self.refine_scale = lo + (hi - lo) * t\n",
        "            else:\n",
        "                self.refine_scale = 0.5 * (lo + hi)\n",
        "        else:\n",
        "            self.refine_scale = float(refine_scale)\n",
        "\n",
        "        # Self-Attn (MHA) + Pre-Norm\n",
        "        self.self_attn = nn.MultiheadAttention(\n",
        "            embed_dim=d_model, num_heads=n_heads, batch_first=True\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ls1   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop1 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp1   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # Cross-Attn (MS-Deformable) + Pre-Norm\n",
        "        self.cross_attn = MultiScaleDeformableAttention(\n",
        "            embed_dims=d_model, num_heads=n_heads,\n",
        "            num_levels=n_levels, num_points=n_points,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ls2   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop2 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp2   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # FFN (GLU) + Pre-Norm\n",
        "        self.ffn   = GLUFFN(d_model, ffn_dim=ffn_dim, ffn_mult=ffn_mult, act=ffn_act, drop=drop)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.ls3   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop3 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp3   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # Ref delta head (tanh clamp)\n",
        "        self.ref_mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model, 2),\n",
        "        )\n",
        "        for m in self.ref_mlp:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
        "\n",
        "    @staticmethod\n",
        "    def _inv_sigmoid(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "        x = x.clamp(eps, 1.0 - eps)\n",
        "        return torch.log(x) - torch.log(1.0 - x)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt: torch.Tensor,                 # (B, N, C)\n",
        "        ref_pts: torch.Tensor,             # (B, N, L, 2)\n",
        "        src: torch.Tensor,                 # (B, S, C)\n",
        "        query_pos: torch.Tensor,           # (B, N, C)\n",
        "        spatial_shapes: torch.Tensor,      # (L, 2)\n",
        "        lvl_start_idx: torch.Tensor,       # (L,)\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        x = tgt\n",
        "        q = self.norm1(x)\n",
        "        sa_out = self.self_attn(q, q, q, need_weights=False)[0]\n",
        "        x = x + self.dp1(self.ls1(self.drop1(sa_out)))\n",
        "\n",
        "        q = self.norm2(x)\n",
        "        ca_out = self.cross_attn(\n",
        "            query=q, value=src,\n",
        "            reference_points=ref_pts,\n",
        "            spatial_shapes=spatial_shapes,\n",
        "            level_start_index=lvl_start_idx,\n",
        "            query_pos=query_pos,\n",
        "        )\n",
        "        x = x + self.dp2(self.ls2(self.drop2(ca_out)))\n",
        "\n",
        "        y = self.ffn(self.norm3(x))\n",
        "        x = x + self.dp3(self.ls3(self.drop3(y)))\n",
        "\n",
        "        # tanh-bounded iterative ref update\n",
        "        delta = torch.tanh(self.ref_mlp(x)) * self.refine_scale      # (B,N,2)\n",
        "        new_ref = torch.sigmoid(self._inv_sigmoid(ref_pts) + delta.unsqueeze(2).to(ref_pts.dtype))\n",
        "\n",
        "        # tiny nudge\n",
        "        x = x + self.grad_eps * delta.mean(dim=2, keepdim=True)\n",
        "        return x, new_ref\n",
        "\n",
        "\n",
        "class HeadPrep(nn.Module):\n",
        "    def __init__(self, d_model: int, ls_init: float = 0.80, ln_eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(d_model, eps=ln_eps)\n",
        "        self.ls   = LayerScale(d_model, init_values=ls_init)\n",
        "    def forward(self, x):  # x: (B, N, C)\n",
        "        return self.ls(self.norm(x))\n",
        "\n",
        "class RTDeformDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    ABLATION — RT‑Deform Decoder (No IoU head, No Denoising)\n",
        "\n",
        "\n",
        "    * Denoising (DN) queries and the associated label/box noise mechanism have been removed.\n",
        "    * The interface signature is preserved; the dn_queries / use_iou_aware parameters are ignored for ablation.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_obj_classes: int = 20,\n",
        "                 include_background: bool = True,\n",
        "                 *,\n",
        "                 num_queries: int = 300,\n",
        "                 num_layers: int = 4,\n",
        "                 d_model: int = 320,\n",
        "                 attn_n_heads: int = 10,\n",
        "                 dn_queries: int = 0,                 # ignored\n",
        "                 use_iou_aware: bool = False,         # ignored\n",
        "                 # Encoder\n",
        "                 use_encoder: bool = True,\n",
        "                 encoder_layers: int = 1,\n",
        "                 encoder_drop_path_max: float = 0.1,\n",
        "                 # (The following are retained for interface compatibility, not used)\n",
        "                 iou_k_ratio: float = 0.75,\n",
        "                 mqs_enable: bool = True,             # seed/MQS mechanism is not present here\n",
        "                 mqs_obj_ratio: float = 0.30,\n",
        "                 mqs_grid_ratio: float = 0.20,\n",
        "                 mqs_levels: Tuple[int, ...] = (1, 2, 3),\n",
        "                 mqs_local_max_kernel: int = 3,\n",
        "                 mqs_train_only: bool = False,\n",
        "                 seed_enable: bool = True,\n",
        "                 seed_alpha_obj_init: float = 0.55,\n",
        "                 seed_alpha_grid_init: float = 0.4,\n",
        "                 seed_mlp_expansion: float = 1.0,\n",
        "                 use_proposals: bool = True,\n",
        "                 proposal_topk: Optional[int] = None,\n",
        "                 proposal_ratio: float = 0.70,\n",
        "                 min_mqs: int = 60,\n",
        "                 min_left_queries: int = 16,\n",
        "                 emit_obj_logits_in_eval: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        # Number of classes\n",
        "        self.num_obj_classes = num_obj_classes\n",
        "        self.include_background = include_background\n",
        "        self.num_pred = num_obj_classes + int(include_background)\n",
        "\n",
        "        self.num_queries = int(num_queries)\n",
        "        self.num_layers  = int(num_layers)\n",
        "        self.d_model     = int(d_model)\n",
        "        self.use_encoder = bool(use_encoder)\n",
        "\n",
        "        # Position and level embed\n",
        "        self.query_pos  = nn.Embedding(self.num_queries, d_model)  # learned positional (N,C)\n",
        "        self.query_feat = nn.Embedding(self.num_queries, d_model)  # learned content   (N,C)\n",
        "        self.pos_embed   = Pos2d(d_model)\n",
        "        self.level_embed = nn.Parameter(torch.randn(4, d_model))\n",
        "\n",
        "        # Encoder (optional)\n",
        "        if self.use_encoder:\n",
        "            self.encoder = TinyDeformEncoder(\n",
        "                num_layers=encoder_layers,\n",
        "                d_model=d_model, n_heads=attn_n_heads,\n",
        "                n_levels=4, n_points=4,\n",
        "                ffn_dim=d_model * 4,\n",
        "                drop=0.0,\n",
        "                drop_path_max=encoder_drop_path_max\n",
        "            )\n",
        "\n",
        "        # Decoder\n",
        "        n_points_list = [4] * (num_layers - 2) + [2, 2] if num_layers >= 2 else [4]\n",
        "        self.layers = nn.ModuleList([\n",
        "            DeformDecoderLayer(d_model=d_model,\n",
        "                               n_heads=attn_n_heads,\n",
        "                               n_levels=4,\n",
        "                               n_points=n_points_list[i],\n",
        "                               ffn_dim=d_model * 4,\n",
        "                               refine_scale=(0.15, 0.45),\n",
        "                               layer_id=i, num_layers=num_layers)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Head prep + classification / box heads\n",
        "        self.head_prep = HeadPrep(d_model, ls_init=0.80, ln_eps=1e-6)\n",
        "        self.cls_head  = nn.Linear(d_model, self.num_pred)\n",
        "        self.box_head  = nn.Linear(d_model, 4)\n",
        "\n",
        "        # Aux head'ler (except last layer)\n",
        "        if self.num_layers > 1:\n",
        "            self.aux_cls_heads = nn.ModuleList(nn.Linear(d_model, self.num_pred) for _ in range(self.num_layers - 1))\n",
        "            self.aux_box_heads = nn.ModuleList(nn.Linear(d_model, 4)            for _ in range(self.num_layers - 1))\n",
        "        else:\n",
        "            self.aux_cls_heads = nn.ModuleList([])\n",
        "            self.aux_box_heads = nn.ModuleList([])\n",
        "\n",
        "        # Small MLP for initial reference\n",
        "        self.ref_init = nn.Linear(d_model, 2)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    # ------------------- init helpers -------------------\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.query_pos.weight,  std=0.02)\n",
        "        nn.init.normal_(self.query_feat.weight, std=0.02)\n",
        "        nn.init.normal_(self.level_embed,       std=0.02)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.ref_init.weight); nn.init.zeros_(self.ref_init.bias)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.box_head.weight, gain=1.0); nn.init.zeros_(self.box_head.bias)\n",
        "        nn.init.xavier_uniform_(self.cls_head.weight, gain=1.0)\n",
        "\n",
        "        for c, b in zip(self.aux_cls_heads, self.aux_box_heads):\n",
        "            nn.init.xavier_uniform_(c.weight, gain=1.0)\n",
        "            nn.init.xavier_uniform_(b.weight, gain=1.0); nn.init.zeros_(b.bias)\n",
        "\n",
        "        self.reset_class_bias()\n",
        "\n",
        "    def reset_class_bias(self, object_prior: float = .2, no_object_bias: float = -2.):\n",
        "        obj_bias = -math.log((1. - object_prior) / object_prior)\n",
        "        with torch.no_grad():\n",
        "            self.cls_head.bias[:] = 0.\n",
        "            self.cls_head.bias[: self.num_obj_classes] = obj_bias\n",
        "            if self.include_background:\n",
        "                self.cls_head.bias[-1] = no_object_bias\n",
        "            for aux in self.aux_cls_heads:\n",
        "                aux.bias[:] = self.cls_head.bias\n",
        "\n",
        "    # ------------------- forward -------------------\n",
        "    def forward(self,\n",
        "                p3: torch.Tensor, p4: torch.Tensor,\n",
        "                p5: torch.Tensor, p6: torch.Tensor,\n",
        "                targets: Optional[List[Dict]] = None\n",
        "                ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        B = p3.size(0)\n",
        "        device = p3.device\n",
        "        feats = [p6, p5, p4, p3]  # from largest to smallest (B, C, H, W)\n",
        "        L = len(feats)\n",
        "\n",
        "        # Per‑level src & pos (B, ΣHW, C)\n",
        "        src_list, pos_list = [], []\n",
        "        spatial_shapes = torch.zeros(L, 2, dtype=torch.long, device=device)  # (L,2) = (H,W)\n",
        "\n",
        "        for i, f in enumerate(feats):\n",
        "            B_, C, H, W = f.shape\n",
        "            pos_lvl = self.pos_embed(f).expand(B_, -1, -1, -1) + self.level_embed[i].view(1, -1, 1, 1)\n",
        "            src_list.append(f.flatten(2).transpose(1, 2))              # (B, HW, C)\n",
        "            pos_list.append(pos_lvl.flatten(2).transpose(1, 2))         # (B, HW, C)\n",
        "            spatial_shapes[i, 0] = H\n",
        "            spatial_shapes[i, 1] = W\n",
        "\n",
        "        src = torch.cat(src_list, dim=1)                         # (B, ΣHW, C)\n",
        "        pos = torch.cat(pos_list, dim=1).to(dtype=src.dtype)     # (B, ΣHW, C)\n",
        "\n",
        "        # level start index: (L,)\n",
        "        numel_per_level = spatial_shapes[:, 0] * spatial_shapes[:, 1]\n",
        "        lvl_start_idx = torch.cat(\n",
        "            [numel_per_level.new_zeros(1), numel_per_level.cumsum(0)[:-1]],\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        # Encoder (optional)\n",
        "        memory = self.encoder(src, pos, spatial_shapes, lvl_start_idx) if self.use_encoder else src\n",
        "        if pos.size(0) != memory.size(0):\n",
        "            pos = pos.expand(memory.size(0), -1, -1)\n",
        "        pos = pos.to(dtype=memory.dtype)\n",
        "\n",
        "        # Learned queries\n",
        "        content = self.query_feat.weight.unsqueeze(0).expand(B, -1, -1)  # (B,N,C)\n",
        "        qpos    = self.query_pos.weight.unsqueeze(0).expand(B, -1, -1)   # (B,N,C)\n",
        "        init_ref = torch.sigmoid(self.ref_init(qpos))                    # (B,N,2)\n",
        "        ref_pts  = init_ref.unsqueeze(2).expand(-1, -1, 4, -1)           # (B,N,4,2)\n",
        "\n",
        "        tgt = content\n",
        "        aux_out = []\n",
        "\n",
        "        # Decoder\n",
        "        for lid, layer in enumerate(self.layers):\n",
        "            tgt, ref_pts = layer(\n",
        "                tgt=tgt, ref_pts=ref_pts, src=memory,\n",
        "                query_pos=qpos, spatial_shapes=spatial_shapes, lvl_start_idx=lvl_start_idx\n",
        "            )\n",
        "\n",
        "            if self.training and lid < self.num_layers - 1:\n",
        "                h_aux = self.head_prep(tgt)  # LN + LayerScale\n",
        "                aux_logits = self.aux_cls_heads[lid](h_aux)\n",
        "                aux_boxes  = self.aux_box_heads[lid](h_aux).sigmoid()\n",
        "                aux_out.append({\"pred_logits\": aux_logits, \"pred_boxes\": aux_boxes})\n",
        "\n",
        "        # Final heads\n",
        "        h = self.head_prep(tgt)\n",
        "        logits = self.cls_head(h)\n",
        "        boxes  = self.box_head(h).sigmoid()\n",
        "\n",
        "        out: Dict[str, torch.Tensor] = {\"pred_logits\": logits, \"pred_boxes\": boxes,\n",
        "                                        \"final_ref_pts\": ref_pts.contiguous()}\n",
        "        if aux_out and self.training:\n",
        "            out[\"aux_outputs\"] = aux_out\n",
        "        return out\n",
        "# -----------------------------------------------------------------------------\n",
        "# MiniBackbone\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class MiniBackbone(nn.Module):\n",
        "    \"\"\"Lightweight version with CSI and PGI\"\"\"\n",
        "    def __init__(self, depths: List[int] = [6, 6, 12, 6],\n",
        "                drop_path_max: float = 0.2,\n",
        "                num_classes: int = 20):\n",
        "        super().__init__()\n",
        "        self.backbone = StageAwareBackbone(depths, drop_path_max, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, need_aux: bool = False):\n",
        "        # Match the return format of StageAwareBackbone\n",
        "        return self.backbone(x, need_aux)\n",
        "\n",
        "\n",
        "\n",
        "def _looks_like_norm_name(name: str) -> bool:\n",
        "    low = name.lower()\n",
        "\n",
        "    return any(k in low for k in [\n",
        "        \".norm\", \"bn\", \"groupnorm\", \"layernorm\", \"gn\", \"ln.\", \"ln_\", \"lnact\", \"ln_act\", \"lnproxy\", \"layernormproxy\", \"rmsnorm\"\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "class HybridDCDATRT(nn.Module):\n",
        "    \"\"\"\n",
        "    End‑to‑End Hybrid‑DCDAT‑RT detector (DAT + DCNv4 backbone + RT‑Deform Decoder)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_classes:   int = 20,\n",
        "                 num_queries:   int = 100,\n",
        "                 depths:        List[int] = (2, 2, 2, 2),\n",
        "                 drop_path_max: float = 0.0,\n",
        "                 backbone_norm_factory: 'NormFactory' = None,\n",
        "                 neck_norm_factory:     'NormFactory' = None,\n",
        "                 use_layer_scale: bool = True,\n",
        "                 layer_scale_init: float = 1.0,\n",
        "                 # Decoder\n",
        "                 d_model:    int = 320,\n",
        "                 dec_layers: int = 4,\n",
        "                 dn_queries: int = 100,\n",
        "                 # Features\n",
        "                 use_aux_loss: bool = True,\n",
        "                 use_iou_aware: bool = False,\n",
        "                 # LR multipliers\n",
        "                 backbone_lr: float = 0.1,\n",
        "                 head_lr:     float = 1.0,\n",
        "                 # Dataset\n",
        "                 voc_prior: bool = False,\n",
        "                 # Decoder Encoder kontrolü\n",
        "                 decoder_use_encoder: bool = True,\n",
        "                 decoder_encoder_layers: int = 2,\n",
        "                 decoder_encoder_drop_path_max: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # default NormFactory\n",
        "        backbone_norm_factory = backbone_norm_factory or NormFactory(\"gn\")\n",
        "        neck_norm_factory     = neck_norm_factory     or NormFactory(\"gn\")\n",
        "\n",
        "        self.num_classes   = num_classes\n",
        "        self.use_aux_loss  = use_aux_loss\n",
        "\n",
        "        # ---------- Backbone (DAT + CSI + PGI) ----------\n",
        "        self.backbone = StageAwareBackbone(\n",
        "            depths          = depths,\n",
        "            drop_path_max   = drop_path_max,\n",
        "            num_classes     = num_classes,\n",
        "            voc_prior       = voc_prior,\n",
        "            norm_factory    = backbone_norm_factory,\n",
        "            use_layer_scale = use_layer_scale,\n",
        "            layer_scale_init= layer_scale_init\n",
        "        )\n",
        "\n",
        "        # ---------- Neck (Light‑BiFPN) ----------\n",
        "\n",
        "        self.neck = LightBiFPN(\n",
        "          in_channels=(128,256,640,768),\n",
        "          out=d_model,\n",
        "          repeats=2,\n",
        "          use_spatial_fuse=True,\n",
        "          fuse_groups=4,\n",
        "          fuse_tau_init=1.4,\n",
        "          fuse_learn_tau=True,\n",
        "          fuse_eps=5e-4,\n",
        "          grad_boost_low=0.1,     # << 0.5 → 0.3\n",
        "          dp_top_second=0.01, dp_bot_second=0.01,\n",
        "          no_blur_first=True,\n",
        "          edge_enhance=True,\n",
        "          edge_ls_init=0.03,\n",
        "          norm_factory=neck_norm_factory,\n",
        "      )\n",
        "\n",
        "        # ---------- Decoder (RT‑Deform) ----------\n",
        "        self.decoder = RTDeformDecoder(\n",
        "            num_obj_classes = num_classes,\n",
        "            include_background = True,\n",
        "            num_queries     = num_queries,\n",
        "            num_layers      = dec_layers,\n",
        "            d_model         = d_model,\n",
        "            dn_queries      = dn_queries,\n",
        "            use_iou_aware   = use_iou_aware,\n",
        "            use_encoder                 = decoder_use_encoder,\n",
        "            encoder_layers              = decoder_encoder_layers,\n",
        "            encoder_drop_path_max       = decoder_encoder_drop_path_max,\n",
        "        )\n",
        "\n",
        "        # (optional)\n",
        "        self._lr_mult = {\n",
        "            \"backbone\": backbone_lr,\n",
        "            \"head\":     head_lr,\n",
        "            \"bias\":     2.0,\n",
        "            \"dcn_bias\": 10.0,\n",
        "        }\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                targets: Optional[List[Dict]] = None) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        The output dictionary is compatible with RTDeformDecoder outputs:\n",
        "          - ‘pred_logits’, ‘pred_boxes’, optional ‘pred_ious’\n",
        "          - in training: ‘aux_outputs’, ‘dn_meta’, ‘query_selection_mask’\n",
        "          - optional: ‘aux_dense’ (auxiliary dense headers from the backbone)\n",
        "        \"\"\"\n",
        "        if not x.is_cuda:\n",
        "            raise RuntimeError(\"HybridDCDATRT expects CUDA tensor input.\")\n",
        "\n",
        "        # Backbone\n",
        "        if self.training and self.use_aux_loss:\n",
        "            (p3, p4, p5, p6), aux_dense = self.backbone(x, need_aux=True)\n",
        "        else:\n",
        "            p3, p4, p5, p6 = self.backbone(x, need_aux=False)\n",
        "            aux_dense      = None\n",
        "\n",
        "        # Neck\n",
        "        p3, p4, p5, p6 = self.neck(p3, p4, p5, p6)\n",
        "\n",
        "        # Decoder\n",
        "        if self.training and targets is not None:\n",
        "            dec_out = self.decoder(p3, p4, p5, p6, targets)\n",
        "        else:\n",
        "            dec_out = self.decoder(p3, p4, p5, p6)\n",
        "\n",
        "        if aux_dense is not None:\n",
        "            dec_out[\"aux_dense\"] = aux_dense\n",
        "        return dec_out\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "\n",
        "    def param_groups(self,\n",
        "                    base_lr: float = 1e-4,\n",
        "                    *,\n",
        "                    weight_decay: float = 0.02,\n",
        "                    bb_mult: float = 0.5,\n",
        "                    dec_mult: float = 2.0,\n",
        "                    bias_mult: float = 1.0,      # Bias boost\n",
        "                    dcn_mult: float = 1.5,       # DCN offset/mask boost\n",
        "                    rpb_mult: float = 5.0,       # RPB boost\n",
        "                    gate_mult: float = 1.0,      # Gate params boost\n",
        "                    ls_mult: float = 0.3,        # LayerScale reduction\n",
        "                    pos_mult: float = 1.5):\n",
        "        \"\"\"\n",
        "        parameter grouping with optimized LR scheduling.\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        buckets = {\n",
        "            # Backbone\n",
        "            \"bb_w\": [], \"bb_b\": [], \"bb_norm\": [],\n",
        "            \"dcn_off\": [],  # DCN offset/mask\n",
        "\n",
        "            # Neck / Head\n",
        "            \"hd_w\": [], \"hd_b\": [], \"hd_norm\": [],\n",
        "            \"fuse_gate\": [],  # Fusion gates\n",
        "\n",
        "            # Decoder\n",
        "            \"dec_w\": [], \"dec_b\": [], \"dec_norm\": [],\n",
        "            \"deform_off_b\": [],  # Deformable attention bias\n",
        "\n",
        "            # Special\n",
        "            \"pos_embed\": [], \"rpb_p\": [], \"ls_gamma\": [],\n",
        "            \"dat_scale_p\": [], \"scalars_gain\": [],\n",
        "        }\n",
        "\n",
        "        def is_norm_name(n: str) -> bool:\n",
        "            return any(x in n for x in [\".norm.\", \".bn.\", \".ln.\", \".gn.\"])\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            if not param.requires_grad:\n",
        "                continue\n",
        "\n",
        "            # --- Special cases (prefix-independent) ---\n",
        "\n",
        "            # RPB tables (needs high LR)\n",
        "            if name.endswith(\".rpb\") or \"relative_position_bias\" in name:\n",
        "                buckets[\"rpb_p\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Positional embeddings\n",
        "            if re.search(r\"(level_embed(_seed)?|query_pos|query_feat|label_enc|pos_table)\", name):\n",
        "                buckets[\"pos_embed\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # LayerScale gamma\n",
        "            if re.search(r\"(layer_scales\\.\\d+\\.weight|\\.scale\\.weight$|\\.ls\\.weight$)\", name):\n",
        "                buckets[\"ls_gamma\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Scalar gates\n",
        "            if any(name.endswith(x) for x in [\"residual_scale\", \"_lambda_logit\", \"dc_res_logit\", \"seed_logit_obj\", \"seed_logit_grid\"]):\n",
        "                buckets[\"scalars_gain\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # DAT scale parameters\n",
        "            if \"dat_logit\" in name or \"dat_log_tau\" in name:\n",
        "                buckets[\"dat_scale_p\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Neck fusion gates\n",
        "            if \"neck.fuse\" in name and \"log_tau\" in name:\n",
        "                buckets[\"fuse_gate\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Deformable attention sampling bias\n",
        "            if \"sampling_offsets.bias\" in name:\n",
        "                buckets[\"deform_off_b\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # --- Module-based categorization ---\n",
        "\n",
        "            # Backbone\n",
        "            if name.startswith(\"backbone.\"):\n",
        "                # DCN offset/mask (special treatment)\n",
        "                if \"offset_mask\" in name and \"dcn\" in name:\n",
        "                    buckets[\"dcn_off\"].append(param)\n",
        "                elif name.endswith(\".bias\") and not is_norm_name(name):\n",
        "                    buckets[\"bb_b\"].append(param)\n",
        "                elif is_norm_name(name):\n",
        "                    buckets[\"bb_norm\"].append(param)\n",
        "                else:\n",
        "                    buckets[\"bb_w\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Neck\n",
        "            if name.startswith(\"neck.\"):\n",
        "                if name.endswith(\".bias\") and not is_norm_name(name):\n",
        "                    buckets[\"hd_b\"].append(param)\n",
        "                elif is_norm_name(name):\n",
        "                    buckets[\"hd_norm\"].append(param)\n",
        "                else:\n",
        "                    buckets[\"hd_w\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Decoder (default)\n",
        "            if name.endswith(\".bias\") and not is_norm_name(name):\n",
        "                buckets[\"dec_b\"].append(param)\n",
        "            elif is_norm_name(name):\n",
        "                buckets[\"dec_norm\"].append(param)\n",
        "            else:\n",
        "                buckets[\"dec_w\"].append(param)\n",
        "\n",
        "        # --- Build parameter groups ---\n",
        "        wd = weight_decay\n",
        "        groups = []\n",
        "\n",
        "        # Backbone groups\n",
        "        if buckets[\"bb_w\"]:\n",
        "            groups.append({\"params\": buckets[\"bb_w\"], \"lr\": base_lr*bb_mult, \"weight_decay\": wd, \"name\": \"bb_w\"})\n",
        "        if buckets[\"bb_b\"]:\n",
        "            groups.append({\"params\": buckets[\"bb_b\"], \"lr\": base_lr*bb_mult*bias_mult, \"weight_decay\": 0.0, \"name\": \"bb_b\"})\n",
        "        if buckets[\"bb_norm\"]:\n",
        "            groups.append({\"params\": buckets[\"bb_norm\"], \"lr\": base_lr*bb_mult, \"weight_decay\": 0.0, \"name\": \"bb_norm\"})\n",
        "\n",
        "        # DCN offset/mask (HIGH LR!)\n",
        "        if buckets[\"dcn_off\"]:\n",
        "            groups.append({\"params\": buckets[\"dcn_off\"], \"lr\": base_lr*dcn_mult, \"weight_decay\": 0.0, \"name\": \"dcn_off\"})\n",
        "\n",
        "        # Neck/Head groups\n",
        "        if buckets[\"hd_w\"]:\n",
        "            groups.append({\"params\": buckets[\"hd_w\"], \"lr\": base_lr, \"weight_decay\": wd, \"name\": \"hd_w\"})\n",
        "        if buckets[\"hd_b\"]:\n",
        "            groups.append({\"params\": buckets[\"hd_b\"], \"lr\": base_lr*bias_mult, \"weight_decay\": 0.0, \"name\": \"hd_b\"})\n",
        "        if buckets[\"hd_norm\"]:\n",
        "            groups.append({\"params\": buckets[\"hd_norm\"], \"lr\": base_lr, \"weight_decay\": 0.0, \"name\": \"hd_norm\"})\n",
        "\n",
        "        # Decoder groups\n",
        "        if buckets[\"dec_w\"]:\n",
        "            groups.append({\"params\": buckets[\"dec_w\"], \"lr\": base_lr*dec_mult, \"weight_decay\": wd, \"name\": \"dec_w\"})\n",
        "        if buckets[\"dec_b\"]:\n",
        "            groups.append({\"params\": buckets[\"dec_b\"], \"lr\": base_lr*dec_mult*bias_mult, \"weight_decay\": 0.0, \"name\": \"dec_b\"})\n",
        "        if buckets[\"dec_norm\"]:\n",
        "            groups.append({\"params\": buckets[\"dec_norm\"], \"lr\": base_lr*dec_mult, \"weight_decay\": 0.0, \"name\": \"dec_norm\"})\n",
        "\n",
        "        # Special parameters with custom LR\n",
        "        if buckets[\"deform_off_b\"]:\n",
        "            groups.append({\"params\": buckets[\"deform_off_b\"], \"lr\": base_lr*dcn_mult, \"weight_decay\": 0.0, \"name\": \"deform_off_b\"})\n",
        "        if buckets[\"fuse_gate\"]:\n",
        "            groups.append({\"params\": buckets[\"fuse_gate\"], \"lr\": base_lr*gate_mult, \"weight_decay\": 0.0, \"name\": \"fuse_gate\"})\n",
        "        if buckets[\"rpb_p\"]:\n",
        "            groups.append({\"params\": buckets[\"rpb_p\"], \"lr\": base_lr*rpb_mult, \"weight_decay\": 0.0, \"name\": \"rpb_p\"})\n",
        "        if buckets[\"pos_embed\"]:\n",
        "            groups.append({\"params\": buckets[\"pos_embed\"], \"lr\": base_lr*pos_mult, \"weight_decay\": 0.0, \"name\": \"pos_embed\"})\n",
        "        if buckets[\"ls_gamma\"]:\n",
        "            groups.append({\"params\": buckets[\"ls_gamma\"], \"lr\": base_lr*ls_mult, \"weight_decay\": 0.0, \"name\": \"ls_gamma\"})\n",
        "        if buckets[\"dat_scale_p\"]:\n",
        "            groups.append({\"params\": buckets[\"dat_scale_p\"], \"lr\": base_lr*gate_mult, \"weight_decay\": 0.0, \"name\": \"dat_scale_p\"})\n",
        "        if buckets[\"scalars_gain\"]:\n",
        "            groups.append({\"params\": buckets[\"scalars_gain\"], \"lr\": base_lr*gate_mult, \"weight_decay\": 0.0, \"name\": \"scalars_gain\"})\n",
        "\n",
        "        # Validation\n",
        "        in_groups = {id(p) for g in groups for p in g[\"params\"]}\n",
        "        missing = [n for n,p in self.named_parameters() if p.requires_grad and id(p) not in in_groups]\n",
        "        assert not missing, f\"Missing params in groups: {len(missing)} params (e.g., {missing[:5]})\"\n",
        "\n",
        "        # Debug logging (optional)\n",
        "        if not hasattr(self, '_param_groups_logged'):\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"Parameter Groups Summary\")\n",
        "            print(\"=\"*60)\n",
        "            for g in groups:\n",
        "                n_params = sum(p.numel() for p in g[\"params\"])\n",
        "                if n_params > 0:\n",
        "                    lr_mult = g[\"lr\"] / base_lr\n",
        "                    print(f\"{g['name']:20s}: {n_params:10,} params | LR: {lr_mult:6.1f}x | WD: {g['weight_decay']:.3f}\")\n",
        "            print(\"=\"*60 + \"\\n\")\n",
        "            self._param_groups_logged = True\n",
        "\n",
        "        return groups\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# build_optimizer – returns only Optimizer (scheduler opsiyonel)\n",
        "# -----------------------------------------------------------------------------\n",
        "def build_optimizer(model: nn.Module,\n",
        "                    base_lr: float = 2e-4,\n",
        "                    weight_decay: float = 0.02,\n",
        "                    *,\n",
        "                    return_scheduler: bool = False):\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # ---------- 1) LAZY-PARAM WARM-UP (only backbone) ----------\n",
        "    was_training = model.training\n",
        "    model.train()  # PGI routes should be in train mode\n",
        "    with torch.no_grad():\n",
        "        for s in (640, 320):\n",
        "            dummy = torch.zeros(1, 3, s, s, device=device)\n",
        "            _ = model.backbone(dummy, need_aux=True)\n",
        "    model.train(was_training)\n",
        "\n",
        "    # ---------- 2) PARAM GRUPLARI ----------\n",
        "    groups = model.param_groups(base_lr)\n",
        "\n",
        "    # Norm weights WD=0 (GN/LNProxy vb.)\n",
        "    ln_keys = ('.norm', '.ln_act', 'ln_proxy')\n",
        "    #  Note: This check is heuristic by name; it can be customized as needed.\n",
        "    for g in groups:\n",
        "        params_in_group = set(map(id, g['params']))\n",
        "        if any(any(k in n for k in ln_keys)\n",
        "               for n, p in model.named_parameters() if id(p) in params_in_group):\n",
        "            g[\"weight_decay\"] = 0.0\n",
        "\n",
        "    # ---------- 3) OPTIMIZER ----------\n",
        "    optim = torch.optim.AdamW(\n",
        "        groups, lr=base_lr,\n",
        "        betas=(0.9, 0.999), eps=1e-6,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    if not return_scheduler:\n",
        "        return optim\n",
        "\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optim, T_max=100, eta_min=base_lr * 0.05)\n",
        "    return optim, sched\n",
        "\n",
        "\n",
        "def build_model(num_classes: int = 20,\n",
        "                norm: str = \"gn\",            # \"gn\", \"lnp\" veya \"bn\"\n",
        "                use_layer_scale: bool = True,\n",
        "                layer_scale_init: float = 1.0,\n",
        "                **kwargs) -> HybridDCDATRT:\n",
        "    \"\"\"\n",
        "    High-level constructor. The most commonly used settings come with defaults.\n",
        "    HybridDCDATRT __init__ parameters (e.g., dec_layers, dn_queries,\n",
        "    decoder_use_encoder, etc.) can be overridden via `kwargs`.\n",
        "    \"\"\"\n",
        "    defaults = dict(\n",
        "        # Backbone\n",
        "        depths         = (2, 2, 4, 1),\n",
        "        drop_path_max  = 0.2,\n",
        "        # Decoder\n",
        "        d_model        = 320,\n",
        "        dec_layers     = 4,\n",
        "        dn_queries     = 100,\n",
        "        num_queries    = 200,\n",
        "        use_aux_loss   = True,\n",
        "        use_iou_aware  = False,\n",
        "        # Dataset\n",
        "        voc_prior      = False,\n",
        "\n",
        "        decoder_use_encoder           = True,\n",
        "        decoder_encoder_layers        = 2,\n",
        "        decoder_encoder_drop_path_max = 0.1,\n",
        "    )\n",
        "    defaults.update(kwargs)\n",
        "\n",
        "    nf = NormFactory(norm)\n",
        "    model = HybridDCDATRT(\n",
        "        num_classes           = num_classes,\n",
        "        backbone_norm_factory = nf,\n",
        "        neck_norm_factory     = nf,\n",
        "        use_layer_scale       = use_layer_scale,\n",
        "        layer_scale_init      = layer_scale_init,\n",
        "        **defaults\n",
        "    )\n",
        "    # init_weights_improved(model)\n",
        "    # apply_hybrid_fixup(model)\n",
        "    # boost_relative_position_bias(model, std=0.05)\n",
        "    return model\n",
        "# -----------------------------------------------------------------------------\n",
        "# Test dummy code\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     import time\n",
        "\n",
        "#     assert torch.cuda.is_available(), \"CUDA required for this model.\"\n",
        "#     device = torch.device(\"cuda\")\n",
        "\n",
        "#     print(\"=\"*80)\n",
        "#     print(\"Building Hybrid-DCDAT-RT Model...\")\n",
        "#     print(\"=\"*80)\n",
        "\n",
        "#     model = build_model(\n",
        "#         num_classes=80,\n",
        "#         depths=[2, 2, 2, 2],  # ResNet-50 like depth\n",
        "#         drop_path_max=0,\n",
        "#         num_queries=200,\n",
        "#         voc_prior=False,norm=\"gn\"  # Use COCO prior\n",
        "#     ).to(device)\n",
        "#     optimizer = build_optimizer(model, base_lr=2e-4)\n",
        "#     # Test forward pass\n",
        "#     print(\"\\n🚀 Testing forward pass...\")\n",
        "#     model.eval()\n",
        "#     dummy = torch.randn(2, 3, 640, 640, device=device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         out = model(dummy)\n",
        "#         print(\"✅ Forward pass successful!\")\n",
        "#         print(f\"Output keys: {list(out.keys())}\")\n",
        "#         print(f\"Predictions shape: {out['pred_logits'].shape}, {out['pred_boxes'].shape}\")\n",
        "\n",
        "#     # Test training mode with targets\n",
        "#     print(\"\\n🚀 Testing training mode...\")\n",
        "#     model.train()\n",
        "\n",
        "#     # Create more realistic targets with proper box format\n",
        "#     targets = []\n",
        "#     for _ in range(2):  # batch size 2\n",
        "#         num_objs = torch.randint(1, 10, (1,)).item()\n",
        "#         # Generate boxes in (cx, cy, w, h) format, all normalized to [0, 1]\n",
        "#         centers = torch.rand(num_objs, 2, device=device)\n",
        "#         sizes = torch.rand(num_objs, 2, device=device) * 0.5  # max size 0.5\n",
        "#         boxes = torch.cat([centers, sizes], dim=1)\n",
        "\n",
        "#         # Ensure boxes are valid (centers must be at least half-size from borders)\n",
        "#         half_sizes = boxes[:, 2:4] / 2\n",
        "#         boxes[:, 0] = boxes[:, 0].clamp(min=half_sizes[:, 0], max=1-half_sizes[:, 0])\n",
        "#         boxes[:, 1] = boxes[:, 1].clamp(min=half_sizes[:, 1], max=1-half_sizes[:, 1])\n",
        "\n",
        "#         targets.append({\n",
        "#             'boxes': boxes,\n",
        "#             'labels': torch.randint(0, 80, (num_objs,), device=device)\n",
        "#         })\n",
        "\n",
        "#     out = model(dummy, targets)\n",
        "#     print(\"✅ Training mode successful!\")\n",
        "#     print(f\"Output keys: {list(out.keys())}\")\n",
        "#     if 'aux_dense' in out:\n",
        "#         print(f\"Auxiliary dense outputs: {list(out['aux_dense'].keys())}\")\n",
        "#     if 'dn_meta' in out:\n",
        "#         print(f\"Denoising queries: {out['dn_meta']['dn_queries']}\")\n",
        "\n",
        "#     # Test gradient flow\n",
        "#     print(\"\\n🚀 Testing gradient flow...\")\n",
        "#     loss = out['pred_logits'].sum() + out['pred_boxes'].sum()\n",
        "#     loss.backward()\n",
        "\n",
        "#     # Check that gradients flow through all parts\n",
        "#     has_grad = {\n",
        "#         'backbone': any(p.grad is not None for n, p in model.named_parameters() if 'backbone' in n),\n",
        "#         'neck': any(p.grad is not None for n, p in model.named_parameters() if 'neck' in n),\n",
        "#         'decoder': any(p.grad is not None for n, p in model.named_parameters() if 'decoder' in n),\n",
        "#     }\n",
        "\n",
        "#     for module, has in has_grad.items():\n",
        "#         print(f\"  {module}: {'✅' if has else '❌'} gradients\")\n",
        "\n",
        "#     # Test parameter groups\n",
        "#     print(\"\\n🚀 Testing parameter groups...\")\n",
        "#     param_groups = model.param_groups(base_lr=2e-4)\n",
        "#     print(f\"Number of parameter groups: {len(param_groups)}\")\n",
        "#     for i, group in enumerate(param_groups):\n",
        "#         print(f\"  Group {i}: {len(group['params'])} params, lr={group['lr']:.6f}, wd={group['weight_decay']}\")\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*80)\n",
        "#     print(\"✅ All tests passed! Model is ready.\")\n",
        "#     print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp-lOWg5rz7F"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torchvision.ops import generalized_box_iou, box_convert\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Helpers: focal losses\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def softmax_focal_loss_weighted(\n",
        "    logits: torch.Tensor,          # (N, C+1)\n",
        "    targets: torch.Tensor,         # (N,) int64  (‑1 = ignore)\n",
        "    *,\n",
        "    alpha: float = 0.25,\n",
        "    gamma: float = 2.0,\n",
        "    sample_weight: Optional[torch.Tensor] = None,  # (N,) or None\n",
        "    reduction: str = \"mean\",\n",
        "    ignore_index: int = -1,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Multi-class focal loss (softmax) + sample weight.\n",
        "    • targets == ignore_index → not included in loss.\n",
        "    • BG class ID = C (i.e., num_classes).\n",
        "    • sample_weight to weight positive/negative samples separately (e.g., QFL).\n",
        "    \"\"\"\n",
        "    logp = F.log_softmax(logits, dim=-1)\n",
        "    p = logp.exp()  # (N, C+1)\n",
        "\n",
        "    if ignore_index is not None:\n",
        "        keep = targets.ne(ignore_index)\n",
        "        if keep.sum() == 0:\n",
        "            return logits.sum() * 0.0\n",
        "        logits = logits[keep]\n",
        "        logp = logp[keep]\n",
        "        p = p[keep]\n",
        "        targets = targets[keep]\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = sample_weight[keep]\n",
        "\n",
        "    idx = torch.arange(targets.size(0), device=targets.device)\n",
        "    log_pt = logp[idx, targets]\n",
        "    pt = p[idx, targets]\n",
        "\n",
        "    bg_id = logits.size(1) - 1\n",
        "    alpha_t = torch.where(targets == bg_id, 1.0 - alpha, alpha)\n",
        "\n",
        "    loss = -alpha_t * (1.0 - pt).pow(gamma) * log_pt  # (N,)\n",
        "\n",
        "    if sample_weight is not None:\n",
        "        loss = loss * sample_weight\n",
        "\n",
        "    if reduction == \"sum\":\n",
        "        return loss.sum()\n",
        "    if reduction == \"mean\":\n",
        "        return loss.mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def binary_focal_with_logits(\n",
        "    input: torch.Tensor,           # (..., C)\n",
        "    target: torch.Tensor,          # (..., C) in {0,1}\n",
        "    alpha: float = 0.25,\n",
        "    gamma: float = 2.0,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Sigmoid (channel-independent) focal loss. BG masked (target 0).\"\"\"\n",
        "    ce = F.binary_cross_entropy_with_logits(input, target, reduction=\"none\")\n",
        "    p = torch.sigmoid(input)\n",
        "    p_t = p * target + (1 - p) * (1 - target)\n",
        "    alpha_t = alpha * target + (1 - alpha) * (1 - target)\n",
        "    loss = alpha_t * (1 - p_t).pow(gamma) * ce\n",
        "\n",
        "    if reduction == \"sum\":\n",
        "        return loss.sum()\n",
        "    if reduction == \"mean\":\n",
        "        return loss.mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Dense aux (sigmoid focal): BG implicit\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def focal_loss_dense_sigmoid(logits, targets, num_classes, alpha=.5, gamma=1.5, use_or_map=False, radius: int = 1):\n",
        "    B, C, H, W = logits.shape\n",
        "    device = logits.device\n",
        "    assert C == num_classes\n",
        "    tgt = torch.zeros(B, C, H, W, device=device)\n",
        "\n",
        "    for b, t in enumerate(targets):\n",
        "        if len(t[\"boxes\"]) == 0: continue\n",
        "        ctr = t[\"boxes\"][:, :2] * torch.tensor([W, H], device=device)\n",
        "        gx = ctr[:, 0].long().clamp(0, W - 1)\n",
        "        gy = ctr[:, 1].long().clamp(0, H - 1)\n",
        "        cls = t[\"labels\"].clamp(0, C - 1)\n",
        "\n",
        "        for (x, y, c) in zip(gx.tolist(), gy.tolist(), cls.tolist()):\n",
        "            x0, x1 = max(0, x - radius), min(W - 1, x + radius)\n",
        "            y0, y1 = max(0, y - radius), min(H - 1, y + radius)\n",
        "            tgt[b, c, y0:y1+1, x0:x1+1] = 1.0  # a small window\n",
        "\n",
        "    logits_flat = logits.permute(0, 2, 3, 1).reshape(-1, C)\n",
        "    tgt_flat = tgt.permute(0, 2, 3, 1).reshape(-1, C)\n",
        "    return binary_focal_with_logits(logits_flat, tgt_flat, alpha=alpha, gamma=gamma, reduction=\"mean\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Hungarian matcher (log‑prob, cost scaling)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"One‑to‑one matching between predicted queries and GT boxes.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 cost_class: float = 2.,\n",
        "                 cost_bbox: float = 5.,\n",
        "                 cost_giou: float = 2.):\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self,\n",
        "                outputs: Dict[str, torch.Tensor],\n",
        "                targets: List[Dict]) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        outputs:\n",
        "            - pred_logits: (B, N, C+1)\n",
        "            - pred_boxes : (B, N, 4)  (cx,cy,w,h norm.)\n",
        "        NOTE: query_selection_mask is not used here; we use it to ignore negatives on the loss side.\n",
        "        \"\"\"\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "        device = outputs[\"pred_logits\"].device\n",
        "\n",
        "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)   # (B·N, C+1)\n",
        "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)                # (B·N, 4)\n",
        "\n",
        "        tgt_ids  = torch.cat([v[\"labels\"] for v in targets])          # (ΣT,)\n",
        "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])           # (ΣT, 4)\n",
        "\n",
        "        cost_class = -out_prob[:, tgt_ids].clamp(1e-8).log()          # (B·N, ΣT)\n",
        "        cost_bbox  = torch.cdist(out_bbox, tgt_bbox, p=1)             # L1\n",
        "        cost_giou  = -generalized_box_iou(\n",
        "            box_convert(out_bbox, \"cxcywh\", \"xyxy\"),\n",
        "            box_convert(tgt_bbox, \"cxcywh\", \"xyxy\")\n",
        "        )\n",
        "\n",
        "        C = (self.cost_class * cost_class +\n",
        "             self.cost_bbox  * cost_bbox  +\n",
        "             self.cost_giou  * cost_giou)                             # (B·N, ΣT)\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "\n",
        "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
        "        indices: list[Tuple[torch.Tensor, torch.Tensor]] = []\n",
        "        tgt_ptr = 0\n",
        "        for b in range(bs):\n",
        "            tgt_cnt = sizes[b]\n",
        "            if tgt_cnt == 0:\n",
        "                indices.append((torch.empty(0, dtype=torch.int64, device=device),\n",
        "                                torch.empty(0, dtype=torch.int64, device=device)))\n",
        "                continue\n",
        "            cost_b = C[b, :, tgt_ptr: tgt_ptr + tgt_cnt].numpy()\n",
        "            row_ind, col_ind = linear_sum_assignment(cost_b)\n",
        "            indices.append((torch.as_tensor(row_ind, dtype=torch.int64, device=device),\n",
        "                            torch.as_tensor(col_ind, dtype=torch.int64, device=device)))\n",
        "            tgt_ptr += tgt_cnt\n",
        "\n",
        "        return indices\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Utilities for QFL‑Softmax (quality weights for positives)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def _compute_matched_ious(\n",
        "    pred_boxes: torch.Tensor,  # (B,N,4)\n",
        "    targets: List[Dict],       # list of dict with \"boxes\"\n",
        "    indices: List[Tuple[torch.Tensor, torch.Tensor]],\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculates IoU(pred_box, gt_box) for matched positives.\n",
        "    Returns: (Σ_matches,) vector, [0,1].\n",
        "    \"\"\"\n",
        "    if len(indices) == 0:\n",
        "        return pred_boxes.new_zeros((0,))\n",
        "    b_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "    s_idx = torch.cat([src for (src, _) in indices])\n",
        "    if b_idx.numel() == 0:\n",
        "        return pred_boxes.new_zeros((0,))\n",
        "\n",
        "    src_boxes = pred_boxes[b_idx, s_idx]    # (Σ,4)\n",
        "    tgt_boxes = torch.cat([t[\"boxes\"][J] for t, (_, J) in zip(targets, indices)], 0)  # (Σ,4)\n",
        "\n",
        "    ious = torch.diag(generalized_box_iou(\n",
        "        box_convert(src_boxes, \"cxcywh\", \"xyxy\"),\n",
        "        box_convert(tgt_boxes, \"cxcywh\", \"xyxy\")\n",
        "    )).clamp(min=0., max=1.)\n",
        "    return ious  # (Σ,)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Main SetCriterion (production‑ready)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes all required losses for HybridDCDATRT training.\n",
        "\n",
        "    • Class loss: Softmax‑Focal + QFL‑Softmax (positive examples quality‑weighted).\n",
        "    • Box: L1 + GIoU.\n",
        "    • IoU regression: L1( pred_ious , IoU(gt, pred) ).\n",
        "    • Dense aux: Sigmoid‑focal (FG channels).\n",
        "    • DN (denoising): class + box (L1+GIoU).\n",
        "    • Aux decoder outputs: labels/boxes, weighted.\n",
        "    • (Optional) Objectness: If outputs[“pred_obj_logits”] exists, BCE loss is added.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes: int,\n",
        "                 matcher: HungarianMatcher,\n",
        "                 weight_dict: Dict[str, float],\n",
        "                 eos_coef: float = .1,\n",
        "                 losses: Optional[List[str]] = None,\n",
        "                 aux_weight: float = .4,\n",
        "                 *,\n",
        "                 # QFL‑Softmax settings\n",
        "                 use_qfl: bool = True,\n",
        "                 qfl_beta: float = 1.0,            # quality ^ beta\n",
        "                 qfl_lambda: float = 0.5,          # target IoU (1-λ) + sigmoid(pred) * λ\n",
        "                 qfl_use_pred: bool = True):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.aux_weight = aux_weight\n",
        "\n",
        "        self.losses = losses or [\n",
        "            \"labels\", \"boxes\", \"cardinality\",\n",
        "            \"dense_aux\",\n",
        "            \"dn_label\", \"dn_box\",\n",
        "            \"iou\",\n",
        "            \"obj\"      # optional; calculated if pred_obj_logits exists\n",
        "        ]\n",
        "\n",
        "        # background (no-object) class weight – (used with alpha in softmax focal)\n",
        "        empty_w = torch.ones(self.num_classes + 1)\n",
        "        empty_w[-1] = eos_coef\n",
        "        self.register_buffer(\"empty_weight\", empty_w)\n",
        "\n",
        "        # QFL parameters\n",
        "        self.use_qfl = use_qfl\n",
        "        self.qfl_beta = float(qfl_beta)\n",
        "        self.qfl_lambda = float(qfl_lambda)\n",
        "        self.qfl_use_pred = bool(qfl_use_pred)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  helpers\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        batch_idx = torch.cat([torch.full_like(src, i)\n",
        "                               for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _build_pos_mask(self, outputs, indices):\n",
        "        \"\"\"(B,N) bool; eşleşmiş (pozitif) sorgular True.\"\"\"\n",
        "        B, N = outputs[\"pred_logits\"].shape[:2]\n",
        "        pos = outputs[\"pred_logits\"].new_zeros((B, N), dtype=torch.bool)\n",
        "        b_idx, s_idx = self._get_src_permutation_idx(indices)\n",
        "        if b_idx.numel():\n",
        "            pos[b_idx, s_idx] = True\n",
        "        return pos\n",
        "\n",
        "    # -------------------------------- labels (QFL‑Softmax) -------------\n",
        "    # ---- SetCriterion.loss_labels  ----\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes, **_):\n",
        "        src_logits = outputs[\"pred_logits\"]            # (B,N,C+1)\n",
        "        B, N = src_logits.shape[:2]\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "\n",
        "        # all unmatched queries → background\n",
        "        tgt_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                dtype=torch.int64, device=src_logits.device)\n",
        "        if idx[0].numel():\n",
        "            tgt_classes[idx] = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)], 0)\n",
        "\n",
        "        # keep = sel | pos (ignore those that are not keep)\n",
        "        if \"query_selection_mask\" in outputs:\n",
        "            sel = outputs[\"query_selection_mask\"].to(torch.bool)\n",
        "            pos = self._build_pos_mask(outputs, indices)\n",
        "            keep = (sel | pos)\n",
        "            tgt_classes = tgt_classes.clone()\n",
        "            tgt_classes[~keep] = -1\n",
        "        else:\n",
        "            keep = torch.ones(B, N, dtype=torch.bool, device=src_logits.device)\n",
        "\n",
        "        # --- BG weighting: BG → self.empty_weight[-1] (e.g., 0.1), FG → 1.0\n",
        "        base_w = torch.ones(B, N, device=src_logits.device, dtype=src_logits.dtype)\n",
        "        bg_id = self.num_classes\n",
        "        base_w[tgt_classes == bg_id] = self.empty_weight[-1].item()\n",
        "\n",
        "        # --- QFL quality weight (apply only to positives)\n",
        "        sample_weight = base_w\n",
        "        if self.use_qfl and idx[0].numel():\n",
        "            with torch.no_grad():\n",
        "                ious_t = _compute_matched_ious(outputs[\"pred_boxes\"], targets, indices)  # (Σ_pos,)\n",
        "                if self.qfl_use_pred and (\"pred_ious\" in outputs):\n",
        "                    q_pred = torch.sigmoid(outputs[\"pred_ious\"][idx])\n",
        "                    q = ((1.0 - self.qfl_lambda) * ious_t + self.qfl_lambda * q_pred).clamp(0., 1.)\n",
        "                else:\n",
        "                    q = ious_t.clamp(0., 1.)\n",
        "\n",
        "                q = q.pow(self.qfl_beta)\n",
        "            sample_weight = sample_weight.clone()\n",
        "            sample_weight[idx] = sample_weight[idx] * q  # quality multiplier for positives\n",
        "\n",
        "        loss = softmax_focal_loss_weighted(\n",
        "            src_logits.flatten(0, 1),\n",
        "            tgt_classes.flatten(),\n",
        "            alpha=.25, gamma=2., reduction=\"sum\",\n",
        "            sample_weight=sample_weight.flatten(0, 1)\n",
        "        ) / max(num_boxes, 1.0)\n",
        "\n",
        "        return {\"loss_labels\": loss}\n",
        "\n",
        "    # -------------------------------- cardinality ----------------------\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes, **_):\n",
        "        probs = outputs[\"pred_logits\"].softmax(-1)   # (B,N,C+1)\n",
        "        bg = probs[..., -1]\n",
        "\n",
        "        if \"query_selection_mask\" in outputs:\n",
        "            sel = outputs[\"query_selection_mask\"].to(torch.bool)\n",
        "            pos = self._build_pos_mask(outputs, indices)\n",
        "            keep = (sel | pos).float()\n",
        "            card_pred = ((1. - bg) * keep).sum(1)    # only keep\n",
        "        else:\n",
        "            card_pred = (1. - bg).sum(1)\n",
        "\n",
        "        tgt_lens = torch.as_tensor([len(t[\"labels\"]) for t in targets],\n",
        "                                   dtype=torch.float, device=probs.device)\n",
        "        loss = F.l1_loss(card_pred, tgt_lens)\n",
        "        return {\"loss_cardinality\": loss}\n",
        "\n",
        "    # -------------------------------- boxes ----------------------------\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes, **_):\n",
        "        \"\"\"\n",
        "        If there is no match (no GT box) ⇒ 0 loss is returned.\n",
        "        \"\"\"\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        if idx[0].numel() == 0:                       # zero‑match guard\n",
        "            z = outputs[\"pred_boxes\"].sum() * 0.0\n",
        "            return {\"loss_bbox\": z, \"loss_giou\": z}\n",
        "\n",
        "        src_boxes = outputs[\"pred_boxes\"][idx]\n",
        "        tgt_boxes = torch.cat([t[\"boxes\"][i]\n",
        "                              for t, (_, i) in zip(targets, indices)], 0)\n",
        "\n",
        "        l1 = F.l1_loss(src_boxes, tgt_boxes, reduction=\"none\").sum() / max(num_boxes, 1.0)\n",
        "\n",
        "        giou = 1.0 - torch.diag(generalized_box_iou(\n",
        "            box_convert(src_boxes, \"cxcywh\", \"xyxy\"),\n",
        "            box_convert(tgt_boxes, \"cxcywh\", \"xyxy\")\n",
        "        )).sum() / max(num_boxes, 1.0)\n",
        "\n",
        "        return {\"loss_bbox\": l1, \"loss_giou\": giou}\n",
        "\n",
        "    # -------------------------------- dense aux ------------------------\n",
        "    def loss_dense_aux(self, outputs, targets, *_):\n",
        "        if \"aux_dense\" not in outputs:\n",
        "            return {}\n",
        "\n",
        "        total = 0.0\n",
        "        for pred in outputs[\"aux_dense\"].values():           # s1, s2, s3\n",
        "            logits = pred[:, :self.num_classes]              # (B,C,H,W) — FG channels\n",
        "            total = total + focal_loss_dense_sigmoid(\n",
        "                logits, targets, self.num_classes,\n",
        "                alpha=.25, gamma=2., use_or_map=False\n",
        "            )\n",
        "\n",
        "        return {\"loss_dense_aux\": total}\n",
        "\n",
        "    # ------------------------------- denoising cls ---------------------\n",
        "    def loss_dn_label(self, outputs, *_):\n",
        "        if \"dn_meta\" not in outputs:\n",
        "            return {}\n",
        "        meta = outputs[\"dn_meta\"]\n",
        "        loss = softmax_focal_loss_weighted(\n",
        "            meta[\"dn_logits\"].flatten(0, 1),\n",
        "            meta[\"dn_labels\"].flatten(),\n",
        "            alpha=.25, gamma=2., reduction=\"mean\",\n",
        "            sample_weight=None\n",
        "        )\n",
        "        return {\"loss_dn_label\": loss}\n",
        "\n",
        "    # ------------------------------- denoising box ---------------------\n",
        "    def loss_dn_box(self, outputs, *_):\n",
        "        \"\"\"\n",
        "         Denoisy box loss – memory-friendly (B-looped) GIoU.\n",
        "        \"\"\"\n",
        "        if \"dn_meta\" not in outputs:\n",
        "            return {}\n",
        "        meta = outputs[\"dn_meta\"]\n",
        "        dn_boxes, dn_gt = meta[\"dn_boxes\"], meta[\"dn_gt_boxes\"]   # (B,Q,4)\n",
        "\n",
        "        # L1\n",
        "        l1 = F.l1_loss(dn_boxes, dn_gt, reduction=\"none\").sum(-1).mean()\n",
        "\n",
        "        # GIoU (batch‑wise)\n",
        "        giou_acc = 0.0\n",
        "        for db, dg in zip(dn_boxes, dn_gt):\n",
        "            giou_acc += 1. - torch.diag(generalized_box_iou(\n",
        "                box_convert(db, 'cxcywh', 'xyxy'),\n",
        "                box_convert(dg, 'cxcywh', 'xyxy')\n",
        "            )).mean()\n",
        "        giou = giou_acc / dn_boxes.size(0)\n",
        "\n",
        "        return {\"loss_dn_bbox\": l1, \"loss_dn_giou\": giou}\n",
        "\n",
        "    # ------------------------------- IoU reg ---------------------------\n",
        "    def loss_iou(self, outputs, targets, indices, *_):\n",
        "        if \"pred_ious\" not in outputs:\n",
        "            return {}\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        if idx[0].numel() == 0:\n",
        "            return {\"loss_iou\": outputs[\"pred_ious\"].sum() * 0.0}\n",
        "\n",
        "        pred_ious = torch.sigmoid(outputs[\"pred_ious\"])[idx]  # <<< EK\n",
        "        src_boxes = outputs[\"pred_boxes\"][idx]\n",
        "        tgt_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], 0)\n",
        "\n",
        "        ious = torch.diag(generalized_box_iou(\n",
        "            box_convert(src_boxes, \"cxcywh\", \"xyxy\"),\n",
        "            box_convert(tgt_boxes, \"cxcywh\", \"xyxy\")\n",
        "        )).clamp(min=0., max=1.)\n",
        "\n",
        "        return {\"loss_iou\": F.l1_loss(pred_ious, ious)}\n",
        "\n",
        "    # ------------------------------- Objectness (opsiyonel) ------------\n",
        "    def loss_obj(self, outputs, targets, indices, num_boxes, **_):\n",
        "        \"\"\"\n",
        "        If the decoder has produced ‘pred_obj_logits’ (B,N), the simple BCE loss is calculated using the keep mask (sel|pos).\n",
        "        Otherwise, it returns empty.\n",
        "        \"\"\"\n",
        "        if \"pred_obj_logits\" not in outputs:\n",
        "            return {}\n",
        "        obj_logits = outputs[\"pred_obj_logits\"]  # (B,N)\n",
        "\n",
        "        # keep = sel|pos\n",
        "        if \"query_selection_mask\" in outputs:\n",
        "            sel = outputs[\"query_selection_mask\"].to(torch.bool)  # (B,N)\n",
        "        else:\n",
        "            # If there is no sel, assume all are negative; a positive mask is required to preserve the positives.\n",
        "            sel = torch.zeros_like(obj_logits, dtype=torch.bool)\n",
        "\n",
        "        pos = self._build_pos_mask(outputs, indices)              # (B,N) True=matched\n",
        "        keep = sel | pos\n",
        "\n",
        "        # target: pos→1, (keep & ~pos) → 0, ignore non keep\n",
        "        target = keep & pos\n",
        "        # ignore: keep=False ⇒ maskle\n",
        "        if keep.sum() == 0:\n",
        "            return {\"loss_obj\": obj_logits.sum() * 0.0}\n",
        "\n",
        "        loss = F.binary_cross_entropy_with_logits(\n",
        "            obj_logits[keep], target[keep].float(), reduction=\"mean\"\n",
        "        )\n",
        "        # fixed weight for object calibration\n",
        "        return {\"loss_obj\": loss}\n",
        "\n",
        "\n",
        "    def loss_center(self, outputs, targets, indices, num_boxes, **_):\n",
        "        if \"final_ref_pts\" not in outputs:\n",
        "            return {}\n",
        "\n",
        "        # Only POSITIVE matches\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        if idx[0].numel() == 0:\n",
        "            z = outputs[\"pred_boxes\"].sum() * 0.0\n",
        "            return {\"loss_center\": z}\n",
        "\n",
        "        # (Σ_pos, 4, 2) veya (Σ_pos, 2)\n",
        "        ref = outputs[\"final_ref_pts\"][idx[0], idx[1]]\n",
        "        ctr_ref = ref if ref.dim() == 2 else ref.mean(dim=1)  # (Σ_pos, 2)\n",
        "\n",
        "        ctr_pred = outputs[\"pred_boxes\"][idx][..., :2]        # (Σ_pos, 2)\n",
        "        loss = F.l1_loss(ctr_pred, ctr_ref, reduction=\"sum\") / max(num_boxes, 1.0)\n",
        "        return {\"loss_center\": loss}\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def get_loss(self, name, outputs, targets, indices, num_boxes):\n",
        "        return {\n",
        "            \"labels\":      self.loss_labels,\n",
        "            \"cardinality\": self.loss_cardinality,\n",
        "            \"boxes\":       self.loss_boxes,\n",
        "            \"dense_aux\":   self.loss_dense_aux,\n",
        "            \"dn_label\":    self.loss_dn_label,\n",
        "            \"dn_box\":      self.loss_dn_box,\n",
        "            \"iou\":         self.loss_iou,\n",
        "            \"obj\":         self.loss_obj,\n",
        "            \"center\":      self.loss_center,   # <<< YENİ\n",
        "        }[name](outputs, targets, indices, num_boxes)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def forward(self,\n",
        "                outputs: Dict[str, torch.Tensor],\n",
        "                targets: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        # Hungarian matching (main outputs)\n",
        "        indices = self.matcher(outputs, targets)\n",
        "\n",
        "        # num_boxes (DDP senkronlu)\n",
        "        device = outputs[\"pred_logits\"].device\n",
        "        nb = torch.as_tensor([sum(len(t[\"labels\"]) for t in targets)],\n",
        "                     dtype=torch.float, device=device)\n",
        "        if dist.is_available() and dist.is_initialized():\n",
        "            # Sum all processes → then divide by the world size (average)\n",
        "            dist.all_reduce(nb, op=dist.ReduceOp.SUM)\n",
        "            world_size = dist.get_world_size()\n",
        "            nb = nb / max(world_size, 1)\n",
        "        num_boxes = max(nb.item(), 1.0)\n",
        "\n",
        "        # ---- actual losses ----\n",
        "        loss_dict: Dict[str, torch.Tensor] = {}\n",
        "        for name in self.losses:\n",
        "            if name.startswith(\"dn_\") and \"dn_meta\" not in outputs:\n",
        "                continue\n",
        "            loss_dict.update(self.get_loss(\n",
        "                name, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        # ---- helper decoder outs ----\n",
        "        sel_mask = outputs.get(\"query_selection_mask\", None)\n",
        "        if \"aux_outputs\" in outputs:\n",
        "            for i, aux in enumerate(outputs[\"aux_outputs\"]):\n",
        "                # mask'i aux'a geçir (shape: (B,N))\n",
        "                aux_in = aux if sel_mask is None else {**aux, \"query_selection_mask\": sel_mask}\n",
        "                aux_idx = self.matcher(aux_in, targets)\n",
        "                for l in (\"labels\", \"boxes\"):\n",
        "                    l_dict = self.get_loss(l, aux_in, targets, aux_idx, num_boxes)\n",
        "                    l_dict = {k + f\"_{i}\": v * self.aux_weight for k, v in l_dict.items()}\n",
        "                    loss_dict.update(l_dict)\n",
        "\n",
        "        # ---- apply weight ----\n",
        "        weighted: Dict[str, torch.Tensor] = {}\n",
        "        for k, v in loss_dict.items():\n",
        "            base = None\n",
        "            for b in self.weight_dict:\n",
        "                if k == b or k.startswith(b + \"_\"):\n",
        "                    base = b\n",
        "                    break\n",
        "            weighted[k] = v * self.weight_dict.get(base, 1.0)\n",
        "\n",
        "        weighted[\"loss\"] = sum(weighted.values())\n",
        "        return weighted\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Builder\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def build_criterion(\n",
        "    num_classes: int = 20,\n",
        "    weight_dict: Optional[Dict[str, float]] = None,\n",
        "    matcher_costs: Optional[Dict[str, float]] = None,\n",
        ") -> SetCriterion:\n",
        "\n",
        "    if weight_dict is None:\n",
        "        weight_dict = {\n",
        "            \"loss_labels\":      2.0,\n",
        "            \"loss_bbox\":        5.0,\n",
        "            \"loss_giou\":        2.5,\n",
        "            \"loss_center\":      0.1,  # small regulator\n",
        "            \"loss_cardinality\": 0.1,\n",
        "            \"loss_dense_aux\":   0.5,\n",
        "        }\n",
        "\n",
        "    if matcher_costs is None:\n",
        "        matcher_costs = {\"cost_class\": 2., \"cost_bbox\": 5., \"cost_giou\": 2.}\n",
        "\n",
        "    matcher = HungarianMatcher(**matcher_costs)\n",
        "\n",
        "    return SetCriterion(\n",
        "        num_classes=num_classes,\n",
        "        matcher=matcher,\n",
        "        weight_dict=weight_dict,\n",
        "        eos_coef=.1,\n",
        "        losses=[\n",
        "            \"labels\", \"boxes\", \"center\",\n",
        "            \"cardinality\", \"dense_aux\",\n",
        "        ],\n",
        "        aux_weight=.4,\n",
        "        # QFL only with matched IoU (no pred_ious)\n",
        "        use_qfl=True, qfl_beta=1.0, qfl_lambda=0.5, qfl_use_pred=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq2on0y0t_1E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from collections import defaultdict, OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torchvision.ops import batched_nms\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def clip_dcnv4_grads(model: nn.Module, max_norm: float = 1.0) -> int:\n",
        "    \"\"\"\n",
        "    Clip gradients for DCNv4 offset/mask parameters\n",
        "    Returns: number of parameters clipped (always returns int, never None)\n",
        "    \"\"\"\n",
        "    target_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is None:\n",
        "            continue\n",
        "        n_low = name.lower()\n",
        "        if \"offset\" in n_low or \"mask\" in n_low:\n",
        "            target_params.append(param)\n",
        "\n",
        "    if target_params:\n",
        "        torch.nn.utils.clip_grad_norm_(target_params, max_norm=max_norm)\n",
        "        return len(target_params)  # Kaç parametre clip edildi\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Resume (full checkpoint) Loading Function\n",
        "# =========================================================\n",
        "def load_resume_checkpoint(\n",
        "    model: nn.Module,\n",
        "    optimizer: Optional[torch.optim.Optimizer],\n",
        "    scheduler: Optional[torch.optim.lr_scheduler._LRScheduler],\n",
        "    resume_path: str,\n",
        "    device: str = \"cpu\",\n",
        "    strict: bool = True,\n",
        "    verbose: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Resumes from where it left off at the full checkpoint:\n",
        "      - Loads the model/optimizer/scheduler states\n",
        "      - Returns start_epoch, best_ap, histoy\n",
        "    \"\"\"\n",
        "    if not resume_path or not os.path.isfile(resume_path):\n",
        "        if verbose:\n",
        "            print(f\"⚠️ Resume file not found: {resume_path}\")\n",
        "        return None\n",
        "\n",
        "    ckpt = torch.load(resume_path, map_location=device)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n=== RESUME META ===\")\n",
        "        print(f\"Path              : {resume_path}\")\n",
        "        print(f\"Kaydedilen epoch  : {ckpt.get('epoch', 'NA')}\")\n",
        "        print(f\"Best AP           : {ckpt.get('best_ap', 'NA')}\")\n",
        "\n",
        "    msg = model.load_state_dict(ckpt[\"model\"], strict=strict)\n",
        "    if verbose:\n",
        "        try:\n",
        "            miss = len(msg.missing_keys)\n",
        "            unexp = len(msg.unexpected_keys)\n",
        "        except:\n",
        "            miss = unexp = 0\n",
        "        print(f\"Model loaded (strict={strict}) → missing={miss}, unexpected={unexp}\")\n",
        "\n",
        "    if optimizer is not None and \"optimizer\" in ckpt:\n",
        "        try:\n",
        "            optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Optimizer state couldnt load: {e}\")\n",
        "\n",
        "    if scheduler is not None and \"scheduler\" in ckpt:\n",
        "        try:\n",
        "            scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Scheduler  state couldnt load: {e}\")\n",
        "\n",
        "    start_epoch = int(ckpt.get(\"epoch\", 0)) + 1\n",
        "    best_ap = float(ckpt.get(\"best_ap\", -1.0))\n",
        "    history = ckpt.get(\"history\", {\n",
        "        \"train_loss\": [], \"train_mAP_50_95\": [], \"train_mAP_50\": [],\n",
        "        \"val_mAP_50_95\": [], \"val_mAP_50\": []\n",
        "    })\n",
        "\n",
        "    return {\"start_epoch\": start_epoch, \"best_ap\": best_ap, \"history\": history}\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Pretrained Loading Functions\n",
        "# =========================================================\n",
        "def get_submodule(root: nn.Module, dotted: str) -> nn.Module:\n",
        "    \"\"\"Safely capture the ‘backbone’ or ‘neck’ submodule (DDP supported).\"\"\"\n",
        "    m = root.module if hasattr(root, \"module\") else root\n",
        "    cur = m\n",
        "    for part in dotted.split(\".\"):\n",
        "        if not hasattr(cur, part):\n",
        "            return None\n",
        "        cur = getattr(cur, part)\n",
        "    return cur\n",
        "\n",
        "def partition_state_dict_by_match(\n",
        "    module: nn.Module,\n",
        "    ckpt_sd: Dict[str, torch.Tensor],\n",
        "    auto_cast: bool = True\n",
        ") -> Tuple[OrderedDict, Dict]:\n",
        "    \"\"\"\n",
        "    Compare the module and ckpt state_dict's:\n",
        "    - Only put those with exactly matching keys and shapes into ‘to_load’\n",
        "    - Report the mismatched ones\n",
        "    \"\"\"\n",
        "    mod_sd = module.state_dict()  # params + buffers\n",
        "    mod_param_names = set(n for n, _ in module.named_parameters(recurse=True))\n",
        "    mod_buffer_names = set(n for n, _ in module.named_buffers(recurse=True))\n",
        "\n",
        "    to_load = OrderedDict()\n",
        "    size_mismatch: List[Tuple[str, Tuple[int, ...], Tuple[int, ...]]] = []\n",
        "    dtype_mismatch: List[Tuple[str, str, str]] = []\n",
        "    unexpected: List[str] = []  # It exists in ckpt, but not in the module.\n",
        "    missing: List[str] = []     # It is in the module, but not in ckpt.\n",
        "    casted: List[str] = []\n",
        "\n",
        "\n",
        "    for k, v in ckpt_sd.items():\n",
        "        if k not in mod_sd:\n",
        "            unexpected.append(k)\n",
        "            continue\n",
        "\n",
        "        tgt = mod_sd[k]\n",
        "        if v.shape != tgt.shape:\n",
        "            size_mismatch.append((k, tuple(v.shape), tuple(tgt.shape)))\n",
        "            continue\n",
        "\n",
        "        if v.dtype != tgt.dtype:\n",
        "\n",
        "            if auto_cast and v.dtype.is_floating_point and tgt.dtype.is_floating_point:\n",
        "                v = v.to(dtype=tgt.dtype)\n",
        "                casted.append(k)\n",
        "            else:\n",
        "                dtype_mismatch.append((k, str(v.dtype), str(tgt.dtype)))\n",
        "                continue\n",
        "\n",
        "        to_load[k] = v\n",
        "\n",
        "\n",
        "    for k in mod_sd.keys():\n",
        "        if k not in ckpt_sd:\n",
        "            missing.append(k)\n",
        "\n",
        "\n",
        "    loaded_param_cnt = sum(1 for k in to_load if k in mod_param_names)\n",
        "    loaded_buffer_cnt = sum(1 for k in to_load if k in mod_buffer_names)\n",
        "    loaded_param_numel = sum(mod_sd[k].numel() for k in to_load if k in mod_param_names)\n",
        "    loaded_buffer_numel = sum(mod_sd[k].numel() for k in to_load if k in mod_buffer_names)\n",
        "\n",
        "    stats = dict(\n",
        "        ckpt_total=len(ckpt_sd),\n",
        "        module_total=len(mod_sd),\n",
        "        loaded_total=len(to_load),\n",
        "        loaded_param_cnt=loaded_param_cnt,\n",
        "        loaded_buffer_cnt=loaded_buffer_cnt,\n",
        "        loaded_param_numel=loaded_param_numel,\n",
        "        loaded_buffer_numel=loaded_buffer_numel,\n",
        "        unexpected=unexpected,\n",
        "        missing=missing,\n",
        "        size_mismatch=size_mismatch,\n",
        "        dtype_mismatch=dtype_mismatch,\n",
        "        casted=casted,\n",
        "    )\n",
        "\n",
        "    return to_load, stats\n",
        "\n",
        "def print_report(title: str, stats: Dict, show_first: int = 20):\n",
        "    line = \"=\" * 64\n",
        "    print(f\"\\n{line}\\n{title}\\n{line}\")\n",
        "    print(f\"CKPT number of tensors         : {stats['ckpt_total']}\")\n",
        "    print(f\"Module number of tensors        : {stats['module_total']}\")\n",
        "    print(f\"LOADED number of tensors     : {stats['loaded_total']} \"\n",
        "          f\"(params: {stats['loaded_param_cnt']}, buffers: {stats['loaded_buffer_cnt']})\")\n",
        "    print(f\"Loaded total     : params={stats['loaded_param_numel']:,} \"\n",
        "          f\"buffers={stats['loaded_buffer_numel']:,}\")\n",
        "    print(f\"Auto dtype cast nums : {len(stats['casted'])}\")\n",
        "\n",
        "    if stats[\"size_mismatch\"]:\n",
        "        print(f\"\\nSize_mismatch (First {show_first}): [{len(stats['size_mismatch'])}]\")\n",
        "        for k, s1, s2 in stats[\"size_mismatch\"][:show_first]:\n",
        "            print(f\"  - {k}: ckpt{tuple(s1)} → mod{tuple(s2)}\")\n",
        "\n",
        "    if stats[\"dtype_mismatch\"]:\n",
        "        print(f\"\\nDtype_mismatch (First {show_first}): [{len(stats['dtype_mismatch'])}]\")\n",
        "        for k, d1, d2 in stats[\"dtype_mismatch\"][:show_first]:\n",
        "            print(f\"  - {k}: ckpt {d1} → mod {d2}\")\n",
        "\n",
        "    if stats[\"missing\"]:\n",
        "        print(f\"\\nMissing (First {show_first}): [{len(stats['missing'])}]\")\n",
        "        for k in stats[\"missing\"][:show_first]:\n",
        "            print(f\"  - {k}\")\n",
        "\n",
        "    if stats[\"unexpected\"]:\n",
        "        print(f\"\\nUnexpected available in ckpt, not in the module) (ilk {show_first}): [{len(stats['unexpected'])}]\")\n",
        "        for k in stats[\"unexpected\"][:show_first]:\n",
        "            print(f\"  - {k}\")\n",
        "\n",
        "def load_pretrained_backbone_neck(\n",
        "    det_model: nn.Module,\n",
        "    pretrained_path: str,\n",
        "    device: str = \"cpu\",\n",
        "    backbone_attr: str = \"backbone\",\n",
        "    neck_attr: str = \"neck\",  # If you give None, the neck won't load. (optional neck)\n",
        "    freeze_backbone: bool = False,\n",
        "    freeze_neck: bool = False,\n",
        "    verbose: bool = True,\n",
        "    show_first: int = 20,\n",
        "    auto_cast: bool = True,\n",
        ") -> Dict[str, Dict]:\n",
        "    \"\"\"\n",
        "    Loads the pretrained_backbone_neck.pth file into the detection model and returns a detailed report.\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(pretrained_path):\n",
        "        print(f\"\\n⚠️  No pretrained file found at {pretrained_path}\")\n",
        "        print(\"  Training from scratch...\")\n",
        "        return {}\n",
        "\n",
        "    ckpt = torch.load(pretrained_path, map_location=device)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n=== PRETRAINED META ===\")\n",
        "        print(f\"Path              : {pretrained_path}\")\n",
        "        print(f\"EMA mı?           : {ckpt.get('is_ema', False)}\")\n",
        "        print(f\"Pretrain Acc      : {ckpt.get('accuracy', 0.0):.2f}%\")\n",
        "        arch = ckpt.get(\"architecture_config\", {})\n",
        "        print(f\"Arch cfg          : {arch}\")\n",
        "\n",
        "    reports = {}\n",
        "\n",
        "    # ---------------- BACKBONE ----------------\n",
        "    bb_mod = get_submodule(det_model, backbone_attr)\n",
        "    bb_sd = ckpt.get(\"backbone_state_dict\", None)\n",
        "\n",
        "    if bb_mod is None:\n",
        "        print(f\"\\n⚠️ Warning:  '{backbone_attr}' couldnt find, The backbone has not been loaded.\")\n",
        "        reports[\"backbone\"] = {\"loaded\": False}\n",
        "    elif bb_sd is None:\n",
        "        print(\"\\n⚠️ Warning: 'backbone_state_dict'couldnt find , The backbone has not been loaded.\")\n",
        "        reports[\"backbone\"] = {\"loaded\": False}\n",
        "    else:\n",
        "        to_load, stats = partition_state_dict_by_match(bb_mod, bb_sd, auto_cast=auto_cast)\n",
        "\n",
        "        msg = bb_mod.load_state_dict(to_load, strict=False)\n",
        "\n",
        "        if verbose:\n",
        "            print_report(\"BB Loading report\", stats, show_first=show_first)\n",
        "\n",
        "        reports[\"backbone\"] = {\"loaded\": True, \"stats\": stats, \"load_msg\": msg}\n",
        "\n",
        "        if freeze_backbone:\n",
        "            for p in bb_mod.parameters():\n",
        "                p.requires_grad = False\n",
        "            if verbose:\n",
        "                print(\"→ Backbone frozen (requires_grad=False)\")\n",
        "\n",
        "    # ---------------- NECK (opsiyonel) ----------------\n",
        "    if neck_attr is not None:\n",
        "        nk_mod = get_submodule(det_model, neck_attr)\n",
        "        nk_sd = ckpt.get(\"neck_state_dict\", None)\n",
        "\n",
        "        if nk_mod is None:\n",
        "            print(f\"\\nℹ️ Info:  '{neck_attr}' not in the model; The neck is not loaded.\")\n",
        "            reports[\"neck\"] = {\"loaded\": False}\n",
        "        elif nk_sd is None:\n",
        "            print(\"\\nℹ️ Info:'neck_state_dict' not in ckpt; The neck is not loaded.\")\n",
        "            reports[\"neck\"] = {\"loaded\": False}\n",
        "        else:\n",
        "            to_load, stats = partition_state_dict_by_match(nk_mod, nk_sd, auto_cast=auto_cast)\n",
        "            msg = nk_mod.load_state_dict(to_load, strict=False)\n",
        "\n",
        "            if verbose:\n",
        "                print_report(\"Neck Loading info\", stats, show_first=show_first)\n",
        "\n",
        "            reports[\"neck\"] = {\"loaded\": True, \"stats\": stats, \"load_msg\": msg}\n",
        "\n",
        "            if freeze_neck:\n",
        "                for p in nk_mod.parameters():\n",
        "                    p.requires_grad = False\n",
        "                if verbose:\n",
        "                    print(\"→ Neck Frozen (requires_grad=False)\")\n",
        "\n",
        "    # Toplam özet\n",
        "    if verbose and reports:\n",
        "        print(\"\\n=== Summary===\")\n",
        "        for part in (\"backbone\", \"neck\"):\n",
        "            if part in reports and reports[part].get(\"loaded\"):\n",
        "                s = reports[part][\"stats\"]\n",
        "                print(f\"- {part.upper():8s}: loaded={s['loaded_total']:4d} \"\n",
        "                      f\"(params={s['loaded_param_cnt']}, buffers={s['loaded_buffer_cnt']}), \"\n",
        "                      f\"size_mismatch={len(s['size_mismatch'])}, \"\n",
        "                      f\"missing={len(s['missing'])}, unexpected={len(s['unexpected'])}, \"\n",
        "                      f\"casted={len(s['casted'])}\")\n",
        "\n",
        "    return reports\n",
        "\n",
        "\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Google Drive already mounted!\")\n",
        "# =========================================================\n",
        "# Config\n",
        "# =========================================================\n",
        "VOC_ROOT = \"/content/voc/pascal-voc-2007-and-2012\"\n",
        "DRIVE_DIR = Path(\"/content/drive/MyDrive/voc_hybrid_yeni\")\n",
        "DRIVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "SAVE_LATEST = DRIVE_DIR / \"latest.pth\"\n",
        "SAVE_BEST = DRIVE_DIR / \"best.pth\"\n",
        "HIST_PATH = DRIVE_DIR / \"history.json\"\n",
        "RESUME_PATH = \"/content/latest_coco.pth\"   #  (Drive ise: \"/content/drive/MyDrive/.../latest.pth\")\n",
        "RESUME_MODE = \"full\"\n",
        "PRETRAIN_PATH = \"/content/pretrained_backbone_ablation.pth\"\n",
        "\n",
        "\n",
        "\n",
        "NUM_CLASSES = 20\n",
        "NUM_CLS = NUM_CLASSES\n",
        "NUM_QUERIES = 200\n",
        "IMG_SIZE = 480\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 100\n",
        "BASE_LR = 1e-4\n",
        "NUM_WORKERS = 8\n",
        "CONF_THRES = 0.05\n",
        "GRAD_CLIP = 10.0\n",
        "EVAL_EVERY = 1\n",
        "SEED = 42\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ACCUM_STEPS = 3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "CLASS2IDX = {\n",
        "    \"aeroplane\": 0, \"bicycle\": 1, \"bird\": 2, \"boat\": 3, \"bottle\": 4,\n",
        "    \"bus\": 5, \"car\": 6, \"cat\": 7, \"chair\": 8, \"cow\": 9,\n",
        "    \"diningtable\": 10, \"dog\": 11, \"horse\": 12, \"motorbike\": 13, \"person\": 14,\n",
        "    \"pottedplant\": 15, \"sheep\": 16, \"sofa\": 17, \"train\": 18, \"tvmonitor\": 19\n",
        "}\n",
        "\n",
        "def set_seed(seed=SEED):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def cxcywh_to_xyxy(boxes: torch.Tensor) -> torch.Tensor:\n",
        "    x_c, y_c, w, h = boxes.unbind(-1)\n",
        "    x1 = (x_c - w / 2).clamp(0, 1)\n",
        "    y1 = (y_c - h / 2).clamp(0, 1)\n",
        "    x2 = (x_c + w / 2).clamp(0, 1)\n",
        "    y2 = (y_c + h / 2).clamp(0, 1)\n",
        "    return torch.stack([x1, y1, x2, y2], dim=-1)\n",
        "\n",
        "def decode_outputs(\n",
        "    outputs: Dict[str, torch.Tensor],\n",
        "    conf_thresh: float = 0.0,\n",
        "    topk: Optional[int] = 100,\n",
        "    *,\n",
        "    use_bg_gate: bool = True,       # scores (1 - P(bg)) with doors\n",
        "    honor_query_mask: bool = True,  # Apply query_selection_mask if available\n",
        "    use_iou_head: bool = False,      # Quality-aware score if IoU exists\n",
        "    gamma: float = 1.0              # score *= sigmoid(iou)^gamma\n",
        "):\n",
        "\n",
        "    logits = outputs[\"pred_logits\"]   # (B,N,C+1)\n",
        "    boxes  = outputs[\"pred_boxes\"]    # (B,N,4)  cx,cy,w,h (0..1)\n",
        "    B, N, K = logits.shape\n",
        "    C = K - 1\n",
        "\n",
        "    probs = logits.softmax(-1)\n",
        "    fg = probs[..., :C]               # (B,N,C)\n",
        "    bg = probs[..., -1]               # (B,N)\n",
        "\n",
        "\n",
        "    if honor_query_mask and (\"query_selection_mask\" in outputs):\n",
        "        sel = outputs[\"query_selection_mask\"].to(torch.bool)  # (B,N)\n",
        "    else:\n",
        "        sel = torch.ones(B, N, dtype=torch.bool, device=logits.device)\n",
        "\n",
        "    # Objectness gate: (1 - P(bg))\n",
        "    if use_bg_gate:\n",
        "        fg = fg * (1.0 - bg).unsqueeze(-1)\n",
        "\n",
        "    boxes_xyxy = cxcywh_to_xyxy(boxes)  # (B,N,4)\n",
        "\n",
        "    # IoU head\n",
        "    has_iou = use_iou_head and ((\"pred_iou\" in outputs) or (\"iou_pred\" in outputs))\n",
        "    if has_iou:\n",
        "        iou_tensor = outputs.get(\"pred_iou\", outputs.get(\"iou_pred\"))  # (B,N) veya (B,N,1)\n",
        "        if iou_tensor.dim() == 3:\n",
        "            iou_tensor = iou_tensor.squeeze(-1)                        # (B,N)\n",
        "\n",
        "    results = []\n",
        "    for b in range(B):\n",
        "        # (N,C) → (N*C,)\n",
        "        scores = fg[b].reshape(-1)\n",
        "        labels = torch.arange(C, device=fg.device).view(1, C).expand(N, C).reshape(-1)\n",
        "        boxes_b = boxes_xyxy[b].unsqueeze(1).expand(N, C, 4).reshape(-1, 4)\n",
        "\n",
        "\n",
        "        keep = scores > conf_thresh\n",
        "        if keep.sum() == 0:\n",
        "            results.append({\n",
        "                \"boxes\":  boxes_b.new_zeros((0, 4)),\n",
        "                \"scores\": scores.new_zeros((0,)),\n",
        "                \"labels\": labels.new_zeros((0,), dtype=torch.long),\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        scores, labels, boxes_b = scores[keep], labels[keep], boxes_b[keep]\n",
        "\n",
        "        # IoU-aware\n",
        "        if has_iou:\n",
        "            qual = iou_tensor[b].unsqueeze(1).expand(N, C).reshape(-1)[keep]\n",
        "            scores = scores * qual.sigmoid().pow(gamma)\n",
        "\n",
        "        # Top-K (global)\n",
        "        if (topk is not None) and (scores.numel() > topk):\n",
        "            v, idx = torch.topk(scores, topk)\n",
        "            scores, labels, boxes_b = v, labels[idx], boxes_b[idx]\n",
        "\n",
        "        results.append({\n",
        "            \"boxes\":  boxes_b,\n",
        "            \"scores\": scores,\n",
        "            \"labels\": labels,\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "class VOCDataset(Dataset):\n",
        "    def __init__(self, root: str, year: str, split: str, size: int = IMG_SIZE):\n",
        "        super().__init__()\n",
        "        self.ds = VOCDetection(root=root, year=year, image_set=split, download=False)\n",
        "        self.size = int(size)\n",
        "        self.train = (\"train\" in split)\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_rgb(img):\n",
        "        t = TF.to_tensor(img)\n",
        "        if t.shape[0] == 1:\n",
        "            t = t.repeat(3, 1, 1)\n",
        "        elif t.shape[0] == 4:\n",
        "            t = t[:3]\n",
        "        return t\n",
        "\n",
        "    def _resize_and_pad(self, img: torch.Tensor, boxes_xyxy_norm: torch.Tensor):\n",
        "        H, W = img.shape[-2:]\n",
        "        S = self.size\n",
        "        scale = S / max(H, W)\n",
        "        new_h, new_w = int(H * scale), int(W * scale)\n",
        "\n",
        "        img = TF.resize(img, (new_h, new_w), antialias=True)\n",
        "\n",
        "        pad_h = (S - new_h) // 2\n",
        "        pad_w = (S - new_w) // 2\n",
        "        img = TF.pad(img, [pad_w, pad_h, S - new_w - pad_w, S - new_h - pad_h], fill=0.5)\n",
        "\n",
        "        x1 = boxes_xyxy_norm[:, 0] * W\n",
        "        y1 = boxes_xyxy_norm[:, 1] * H\n",
        "        x2 = boxes_xyxy_norm[:, 2] * W\n",
        "        y2 = boxes_xyxy_norm[:, 3] * H\n",
        "\n",
        "        x1 = x1 * scale + pad_w\n",
        "        x2 = x2 * scale + pad_w\n",
        "        y1 = y1 * scale + pad_h\n",
        "        y2 = y2 * scale + pad_h\n",
        "\n",
        "        x1 /= S; x2 /= S; y1 /= S; y2 /= S\n",
        "\n",
        "        cx = (x1 + x2) / 2\n",
        "        cy = (y1 + y2) / 2\n",
        "        w = (x2 - x1).clamp_min(1e-4)\n",
        "        h = (y2 - y1).clamp_min(1e-4)\n",
        "\n",
        "        boxes_cxcywh = torch.stack([cx, cy, w, h], dim=1).clamp(0., 1.)\n",
        "        return img, boxes_cxcywh\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_pil, ann = self.ds[idx]\n",
        "        w, h = img_pil.width, img_pil.height\n",
        "        img = self._to_rgb(img_pil)\n",
        "\n",
        "        boxes, labels = [], []\n",
        "        if isinstance(ann, dict) and \"annotation\" in ann:\n",
        "            objs = ann[\"annotation\"].get(\"object\", [])\n",
        "            objs = objs if isinstance(objs, list) else [objs]\n",
        "\n",
        "            for o in objs:\n",
        "                cls = o[\"name\"].lower().strip()\n",
        "                if cls not in CLASS2IDX:\n",
        "                    continue\n",
        "\n",
        "                bnd = o[\"bndbox\"]\n",
        "                xmin = (float(bnd[\"xmin\"]) - 1) / w\n",
        "                ymin = (float(bnd[\"ymin\"]) - 1) / h\n",
        "                xmax = (float(bnd[\"xmax\"]) - 1) / w\n",
        "                ymax = (float(bnd[\"ymax\"]) - 1) / h\n",
        "\n",
        "                if xmax <= xmin or ymax <= ymin:\n",
        "                    continue\n",
        "                if (xmax - xmin) < 0.01 or (ymax - ymin) < 0.01:\n",
        "                    continue\n",
        "\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "                labels.append(CLASS2IDX[cls])\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        if boxes.numel():\n",
        "            img, boxes = self._resize_and_pad(img, boxes)\n",
        "        else:\n",
        "            img = TF.resize(img, (self.size, self.size), antialias=True)\n",
        "\n",
        "        img = TF.normalize(img, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "\n",
        "        if self.train and torch.rand(()) < 0.5 and boxes.numel():\n",
        "            img = TF.hflip(img)\n",
        "            boxes[:, 0] = 1.0 - boxes[:, 0]\n",
        "\n",
        "        if boxes.numel():\n",
        "            boxes[:, 2:4] = boxes[:, 2:4].clamp(min=0.01)\n",
        "            half_w = boxes[:, 2] / 2\n",
        "            half_h = boxes[:, 3] / 2\n",
        "            boxes[:, 0] = boxes[:, 0].clamp(min=half_w, max=1 - half_w)\n",
        "            boxes[:, 1] = boxes[:, 1].clamp(min=half_h, max=1 - half_h)\n",
        "\n",
        "        return img, {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "def collate_fn(batch: List[Tuple[torch.Tensor, Dict]]):\n",
        "    imgs, targets = zip(*batch)\n",
        "    return torch.stack(imgs, 0), list(targets)\n",
        "\n",
        "def box_iou_xyxy_np(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
        "    N, M = a.shape[0], b.shape[0]\n",
        "    if N == 0 or M == 0:\n",
        "        return np.zeros((N, M), dtype=np.float32)\n",
        "\n",
        "    x11, y11, x12, y12 = a[:, 0][:, None], a[:, 1][:, None], a[:, 2][:, None], a[:, 3][:, None]\n",
        "    x21, y21, x22, y22 = b[:, 0][None, :], b[:, 1][None, :], b[:, 2][None, :], b[:, 3][None, :]\n",
        "\n",
        "    inter_x1 = np.maximum(x11, x21)\n",
        "    inter_y1 = np.maximum(y11, y21)\n",
        "    inter_x2 = np.minimum(x12, x22)\n",
        "    inter_y2 = np.minimum(y12, y22)\n",
        "\n",
        "    inter_w = np.clip(inter_x2 - inter_x1, 0, 1)\n",
        "    inter_h = np.clip(inter_y2 - inter_y1, 0, 1)\n",
        "    inter = inter_w * inter_h\n",
        "\n",
        "    area_a = np.clip((x12 - x11), 0, 1) * np.clip((y12 - y11), 0, 1)\n",
        "    area_b = np.clip((x22 - x21), 0, 1) * np.clip((y22 - y21), 0, 1)\n",
        "    union = area_a + area_b - inter\n",
        "\n",
        "    iou = inter / np.clip(union, 1e-9, None)\n",
        "    return iou.astype(np.float32)\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
        "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
        "\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "    rs = np.linspace(0, 1, 101)\n",
        "    ap = np.mean([np.max(mpre[mrec >= r]) for r in rs])\n",
        "    return ap\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_map(model: nn.Module, data_loader: DataLoader, device=DEVICE, iou_thresholds=np.arange(0.5, 0.96, 0.05)):\n",
        "    model.eval()\n",
        "\n",
        "    preds = {c: [] for c in range(NUM_CLASSES)}\n",
        "    gts = {c: {} for c in range(NUM_CLASSES)}\n",
        "    img_index = 0\n",
        "\n",
        "    for imgs, targets in tqdm(data_loader, desc=\"Computing mAP\", leave=False):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        out = model(imgs)\n",
        "        dec = decode_outputs(out, conf_thresh=CONF_THRES, topk=100)\n",
        "\n",
        "        B = imgs.size(0)\n",
        "        for b in range(B):\n",
        "            gt_xyxy = cxcywh_to_xyxy(targets[b][\"boxes\"]).cpu().numpy()\n",
        "            gt_lbls = targets[b][\"labels\"].cpu().numpy().tolist()\n",
        "\n",
        "            for c in range(NUM_CLASSES):\n",
        "                gts[c].setdefault(img_index + b, [])\n",
        "\n",
        "            for bb, ll in zip(gt_xyxy, gt_lbls):\n",
        "                gts[ll].setdefault(img_index + b, [])\n",
        "                gts[ll][img_index + b].append(bb)\n",
        "\n",
        "            pb = dec[b]\n",
        "            p_boxes = pb[\"boxes\"].cpu().numpy()\n",
        "            p_scores = pb[\"scores\"].cpu().numpy()\n",
        "            p_labels = pb[\"labels\"].cpu().numpy()\n",
        "\n",
        "            for bb, sc, ll in zip(p_boxes, p_scores, p_labels):\n",
        "                preds[ll].append((img_index + b, float(sc), bb))\n",
        "\n",
        "        img_index += B\n",
        "\n",
        "    aps_iou = []\n",
        "    ap50_list = []\n",
        "\n",
        "    total_gts_per_class = [sum(len(v) for v in gts[c].values()) for c in range(NUM_CLASSES)]\n",
        "\n",
        "    for c in range(NUM_CLASSES):\n",
        "        gt_per_img = {}\n",
        "        for img_id, boxes in gts[c].items():\n",
        "            arr = np.array(boxes, dtype=np.float32) if len(boxes) else np.zeros((0, 4), np.float32)\n",
        "            gt_per_img[img_id] = {\"boxes\": arr, \"matched\": np.zeros((arr.shape[0],), dtype=bool)}\n",
        "\n",
        "        pc = preds[c]\n",
        "        if len(pc):\n",
        "            pc = sorted(pc, key=lambda x: -x[1])\n",
        "\n",
        "        aps_this_class = []\n",
        "\n",
        "        for iou_thr in iou_thresholds:\n",
        "            if total_gts_per_class[c] == 0:\n",
        "                aps_this_class.append(0.0)\n",
        "                continue\n",
        "\n",
        "            for img_id in gt_per_img:\n",
        "                gt_per_img[img_id][\"matched\"] = np.zeros((gt_per_img[img_id][\"boxes\"].shape[0],), dtype=bool)\n",
        "\n",
        "            tp = np.zeros((len(pc),), dtype=np.float32)\n",
        "            fp = np.zeros((len(pc),), dtype=np.float32)\n",
        "\n",
        "            for i, (img_id, score, box) in enumerate(pc):\n",
        "                gt_info = gt_per_img.get(img_id, {\"boxes\": np.zeros((0, 4), np.float32), \"matched\": np.zeros((0,), dtype=bool)})\n",
        "                gt_boxes = gt_info[\"boxes\"]\n",
        "                matched = gt_info[\"matched\"]\n",
        "\n",
        "                if gt_boxes.shape[0] == 0:\n",
        "                    fp[i] = 1.0\n",
        "                    continue\n",
        "\n",
        "                ious = box_iou_xyxy_np(np.asarray([box], dtype=np.float32), gt_boxes)[0]\n",
        "                jmax = ious.argmax()\n",
        "                iou_max = ious[jmax]\n",
        "\n",
        "                if iou_max >= iou_thr and not matched[jmax]:\n",
        "                    tp[i] = 1.0\n",
        "                    matched[jmax] = True\n",
        "                else:\n",
        "                    fp[i] = 1.0\n",
        "\n",
        "            tp_cum = np.cumsum(tp)\n",
        "            fp_cum = np.cumsum(fp)\n",
        "            recall = tp_cum / (total_gts_per_class[c] + 1e-9)\n",
        "            precision = tp_cum / np.clip(tp_cum + fp_cum, 1e-9, None)\n",
        "\n",
        "            ap = compute_ap(recall, precision)\n",
        "            aps_this_class.append(ap)\n",
        "\n",
        "            if abs(iou_thr - 0.5) < 1e-9:\n",
        "                ap50_list.append(ap)\n",
        "\n",
        "        aps_iou.extend(aps_this_class)\n",
        "\n",
        "    mAP_50_95 = float(np.mean(aps_iou)) if len(aps_iou) else 0.0\n",
        "    mAP_50 = float(np.mean(ap50_list)) if len(ap50_list) else 0.0\n",
        "\n",
        "    return {\"mAP_50_95\": mAP_50_95, \"mAP_50\": mAP_50}\n",
        "\n",
        "def get_gradient_norm(model):\n",
        "    total_norm = 0.0\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            param_norm = p.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "    return total_norm ** 0.5\n",
        "\n",
        "from contextlib import nullcontext\n",
        "import math\n",
        "\n",
        "def train_one_epoch(model, criterion, optimizer, loader, epoch: int):\n",
        "    model.train()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "\n",
        "    grad_clip_value = 20.0 if epoch <= 50 else GRAD_CLIP\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_grad_norm = 0.0\n",
        "    step_count = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch:02d}\")\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "\n",
        "    if epoch == 1:\n",
        "        has_dcn = any((\"offset\" in n.lower() or \"mask\" in n.lower())\n",
        "                      for n, p in model.named_parameters() if p.requires_grad)\n",
        "        if has_dcn:\n",
        "            print(\"  ✓ DCNv4 offset/mask parameters found (clip will be applied at step time)\")\n",
        "        else:\n",
        "            print(\"  ℹ DCNv4 parameter not found\")\n",
        "\n",
        "    for it, (imgs, targets) in enumerate(pbar, 1):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        outputs = model(imgs, targets)\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        loss = loss_dict[\"loss\"]\n",
        "\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # GA: Divide the loss to mimic the large batch average\n",
        "        loss = loss / max(1, ACCUM_STEPS)\n",
        "\n",
        "\n",
        "        use_no_sync = hasattr(model, \"no_sync\") and (ACCUM_STEPS > 1) and (it % ACCUM_STEPS != 0)\n",
        "        ctx = model.no_sync() if use_no_sync else nullcontext()\n",
        "\n",
        "        with ctx:\n",
        "            optimizer.zero_grad(set_to_none=True) if it == 1 else None\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "        do_step = (it % ACCUM_STEPS == 0) or (it == len(loader))\n",
        "        if do_step:\n",
        "            # Apply the clip in GA only at the step moment (on the accumulated gradient)\n",
        "            if GRAD_CLIP and GRAD_CLIP > 0:\n",
        "                # DCN clip\n",
        "                _ = clip_dcnv4_grads(model, max_norm=0.3)\n",
        "                #  global clip\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_value)\n",
        "\n",
        "            grad_norm = get_gradient_norm(model)\n",
        "            running_grad_norm += grad_norm\n",
        "            step_count += 1\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # pbar\n",
        "        avg_loss = running_loss / it\n",
        "        avg_grad = (running_grad_norm / max(1, step_count))\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "        pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'grad': f'{avg_grad:.2f}', 'lr': f'{lr:.2e}'})\n",
        "\n",
        "    epoch_loss = running_loss / max(1, len(loader))\n",
        "    return epoch_loss\n",
        "\n",
        "def main_train():\n",
        "    set_seed(SEED)\n",
        "\n",
        "    print(\"Loading datasets...\")\n",
        "    train07 = VOCDataset(VOC_ROOT, \"2007\", \"trainval\", size=IMG_SIZE)\n",
        "    train12 = VOCDataset(VOC_ROOT, \"2012\", \"trainval\", size=IMG_SIZE)\n",
        "    train_ds = ConcatDataset([train07, train12])\n",
        "    val_ds = VOCDataset(VOC_ROOT, \"2007\", \"test\", size=IMG_SIZE)\n",
        "\n",
        "    print(f\"Train dataset: {len(train_ds)} samples\")\n",
        "    print(f\"Val dataset: {len(val_ds)} samples\")\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS,\n",
        "                            pin_memory=True, drop_last=True, collate_fn=collate_fn, persistent_workers=(NUM_WORKERS > 0))\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n",
        "                          pin_memory=True, drop_last=False, collate_fn=collate_fn, persistent_workers=(NUM_WORKERS > 0))\n",
        "\n",
        "    # Train loader for evaluation (subset for speed)\n",
        "    train_eval_ds = torch.utils.data.Subset(train_ds, range(min(2000, len(train_ds))))\n",
        "    train_eval_loader = DataLoader(train_eval_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS,\n",
        "                                  pin_memory=True, drop_last=False, collate_fn=collate_fn, persistent_workers=(NUM_WORKERS > 0))\n",
        "\n",
        "    print(\"Building model...\")\n",
        "    model = build_model(num_classes=NUM_CLASSES, num_queries=NUM_QUERIES, depths=[2,2,4,1],\n",
        "                       drop_path_max=0.15, voc_prior=True, norm=\"gn\").to(DEVICE)\n",
        "\n",
        "    # Print model statistics\n",
        "    print(f\"\\n📊 Model Architecture Statistics:\")\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"  Total parameters: {total_params:,}\")\n",
        "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"  Non-trainable parameters: {total_params - trainable_params:,}\")\n",
        "\n",
        "    # Count parameters by module\n",
        "    module_params = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        module = name.split('.')[0]\n",
        "        if module not in module_params:\n",
        "            module_params[module] = 0\n",
        "        module_params[module] += param.numel()\n",
        "\n",
        "    print(f\"\\n  Parameters by module:\")\n",
        "    for module, count in sorted(module_params.items()):\n",
        "        print(f\"    {module}: {count:,} ({count/total_params*100:.1f}%)\")\n",
        "\n",
        "\n",
        "    bb_weights = torch.cat([p.flatten() for n, p in model.named_parameters()\n",
        "                            if 'backbone' in n and p.dim() >= 2])\n",
        "    print(f\"  Backbone mean: {bb_weights.abs().mean().item():.6f}\")\n",
        "\n",
        "    # Neck\n",
        "    neck_weights = torch.cat([p.flatten() for n, p in model.named_parameters()\n",
        "                              if 'neck' in n and p.dim() >= 2])\n",
        "    print(f\"  Neck mean: {neck_weights.abs().mean().item():.6f}\")\n",
        "\n",
        "    criterion = build_criterion(num_classes=NUM_CLASSES)\n",
        "    optimizer = build_optimizer(model, base_lr=BASE_LR)\n",
        "    from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
        "\n",
        "    warmup_epochs = 2\n",
        "    # 1% → 100% warmup\n",
        "    warmup = LinearLR(optimizer, start_factor=0.01, total_iters=warmup_epochs)\n",
        "    # After Warmup cosine\n",
        "    cosine = CosineAnnealingLR(optimizer, T_max=EPOCHS - warmup_epochs, eta_min=BASE_LR * 0.01)\n",
        "\n",
        "    scheduler = SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs])\n",
        "\n",
        "\n",
        "    start_epoch = 1\n",
        "    best_ap = -1.0\n",
        "    history = {\"train_loss\": [], \"train_mAP_50_95\": [], \"train_mAP_50\": [],\n",
        "              \"val_mAP_50_95\": [], \"val_mAP_50\": []}\n",
        "\n",
        "\n",
        "        # -----------------------------------------------------\n",
        "    # Loading strategy:\n",
        "    # 1) If PRETRAIN_PATH file EXISTS → load only backbone/neck (start from epoch=1)\n",
        "    # 2) Otherwise, and if RESUME_PATH EXISTS → resume fully (from the epoch where it left off)\n",
        "    # 3) If neither exists → start from scratch\n",
        "    # -----------------------------------------------------\n",
        "    PRETRAIN_PATH = \"/content/pretrained_backbone_ablation.pth\"\n",
        "\n",
        "\n",
        "    # Defaults (training from scratch)\n",
        "    start_epoch = 1\n",
        "    best_ap = -1.0\n",
        "    history = {\"train_loss\": [], \"train_mAP_50_95\": [], \"train_mAP_50\": [],\n",
        "               \"val_mAP_50_95\": [], \"val_mAP_50\": []}\n",
        "    reports = {}\n",
        "    mode_used = \"scratch\"\n",
        "\n",
        "    if PRETRAIN_PATH and os.path.isfile(PRETRAIN_PATH):\n",
        "        print(\"\\n============================================================\")\n",
        "        print(\"Pretrained backbone/neck yükleniyor…\")\n",
        "        print(\"============================================================\")\n",
        "        reports = load_pretrained_backbone_neck(\n",
        "            det_model=model,\n",
        "            pretrained_path=PRETRAIN_PATH,\n",
        "            device=\"cpu\",\n",
        "            backbone_attr=\"backbone\",\n",
        "            neck_attr=\"neck\",              # neck or None\n",
        "            freeze_backbone=False,\n",
        "            freeze_neck=False,\n",
        "            verbose=True,\n",
        "            show_first=25,\n",
        "            auto_cast=True\n",
        "        )\n",
        "        mode_used = \"pretrain\"\n",
        "\n",
        "    elif RESUME_PATH and os.path.isfile(RESUME_PATH):\n",
        "        print(\"\\n============================================================\")\n",
        "        print(\"Resume checkpoint loading..\")\n",
        "        print(\"============================================================\")\n",
        "        info = load_resume_checkpoint(\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            resume_path=RESUME_PATH,\n",
        "            device=\"cpu\",\n",
        "            strict=True,\n",
        "            verbose=True,\n",
        "        )\n",
        "        if info is not None:\n",
        "            start_epoch = info[\"start_epoch\"]\n",
        "            best_ap = info[\"best_ap\"]\n",
        "            history = info[\"history\"]\n",
        "            mode_used = \"resume\"\n",
        "        else:\n",
        "            print(\"⚠️ Resume failed; starting from scratch.\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nℹ️ No pretrained model or resume file was found: training starts from scratch.\")\n",
        "\n",
        "    print(f\"\\nMod: {mode_used}\")\n",
        "\n",
        "    # Hızlı ağırlık kontrolü (rapor için faydalı)\n",
        "    print(\"\\n🔍 Quick Weight Check:\")\n",
        "    try:\n",
        "        bb_weights = torch.cat([p.flatten() for n, p in model.named_parameters()\n",
        "                                if 'backbone' in n and p.dim() >= 2])\n",
        "        print(f\"  Backbone mean: {bb_weights.abs().mean().item():.6f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Backbone check skipped: {e}\")\n",
        "\n",
        "    try:\n",
        "        neck_weights = torch.cat([p.flatten() for n, p in model.named_parameters()\n",
        "                                  if 'neck' in n and p.dim() >= 2])\n",
        "        print(f\"  Neck mean: {neck_weights.abs().mean().item():.6f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Neck check skipped: {e}\")\n",
        "\n",
        "    # Programatik sum (if any)\n",
        "    if reports:\n",
        "        bb_stats = reports.get(\"backbone\", {}).get(\"stats\", {})\n",
        "        nk_stats = reports.get(\"neck\", {}).get(\"stats\", {})\n",
        "        if bb_stats:\n",
        "            print(f\"\\n📊 Backbone Summary: {bb_stats.get('loaded_total', 0)} tensors loaded\")\n",
        "        if nk_stats:\n",
        "            print(f\"📊 Neck Summary: {nk_stats.get('loaded_total', 0)} tensors loaded\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\nStarting training...\")\n",
        "\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Epoch {epoch}/{EPOCHS} - LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        scheduler.step()\n",
        "        # Train\n",
        "        train_loss = train_one_epoch(model, criterion, optimizer, train_loader, epoch)\n",
        "        history[\"train_loss\"].append(float(train_loss))\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Evaluate\n",
        "        if epoch % EVAL_EVERY == 0:\n",
        "            print(\"\\nEvaluating on validation set...\")\n",
        "            val_metrics = evaluate_map(model, val_loader, device=DEVICE)\n",
        "            val_mAP_50_95 = val_metrics[\"mAP_50_95\"]\n",
        "            val_mAP_50 = val_metrics[\"mAP_50\"]\n",
        "            history[\"val_mAP_50_95\"].append(float(val_mAP_50_95))\n",
        "            history[\"val_mAP_50\"].append(float(val_mAP_50))\n",
        "\n",
        "            print(f\"Val mAP@0.5:0.95 = {val_mAP_50_95:.4f}\")\n",
        "            print(f\"Val mAP@0.5 = {val_mAP_50:.4f}\")\n",
        "\n",
        "            print(\"\\nEvaluating on train set...\")\n",
        "            train_metrics = evaluate_map(model, train_eval_loader, device=DEVICE)\n",
        "            train_mAP_50_95 = train_metrics[\"mAP_50_95\"]\n",
        "            train_mAP_50 = train_metrics[\"mAP_50\"]\n",
        "            history[\"train_mAP_50_95\"].append(float(train_mAP_50_95))\n",
        "            history[\"train_mAP_50\"].append(float(train_mAP_50))\n",
        "\n",
        "            print(f\"Train mAP@0.5:0.95 = {train_mAP_50_95:.4f}\")\n",
        "            print(f\"Train mAP@0.5 = {train_mAP_50:.4f}\")\n",
        "\n",
        "            # Save best model\n",
        "            is_best = val_mAP_50_95 > best_ap\n",
        "            if is_best:\n",
        "                best_ap = val_mAP_50_95\n",
        "                torch.save({\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model\": model.state_dict(),\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"scheduler\": scheduler.state_dict(),\n",
        "                    \"best_ap\": best_ap,\n",
        "                    \"history\": history\n",
        "                }, SAVE_BEST)\n",
        "                print(f\"✓ Saved BEST model (mAP@0.5:0.95={best_ap:.4f})\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Save latest checkpoint\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"scheduler\": scheduler.state_dict(),\n",
        "            \"best_ap\": best_ap,\n",
        "            \"history\": history\n",
        "        }, SAVE_LATEST)\n",
        "\n",
        "        # Save history\n",
        "        try:\n",
        "            with open(HIST_PATH, \"w\") as f:\n",
        "                json.dump(history, f, indent=2)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        print(f\"\\nSummary - Epoch {epoch}:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "        if history['val_mAP_50_95']:\n",
        "            print(f\"  Val mAP@0.5:0.95: {history['val_mAP_50_95'][-1]:.4f}\")\n",
        "            print(f\"  Val mAP@0.5: {history['val_mAP_50'][-1]:.4f}\")\n",
        "        if history['train_mAP_50_95']:\n",
        "            print(f\"  Train mAP@0.5:0.95: {history['train_mAP_50_95'][-1]:.4f}\")\n",
        "            print(f\"  Train mAP@0.5: {history['train_mAP_50'][-1]:.4f}\")\n",
        "        print(f\"  Best mAP@0.5:0.95: {best_ap:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training finished!\")\n",
        "    print(f\"Best mAP@0.5:0.95: {best_ap:.4f}\")\n",
        "    print(f\"Best model saved at: {SAVE_BEST}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
