{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUo1poVofs1W"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "\n",
        "!pip install torch==2.3.1+cu121 torchvision --extra-index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r4pY1HqfyI0"
      },
      "outputs": [],
      "source": [
        "!pip install mmcv==2.2.0 \\\n",
        "  -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.3/index.html\n",
        "!pip install -q  pycocotools tqdm torchmetrics lxml scipy\n",
        "!pip install -U \"datasets>=2.17.0\"  \"pyarrow>=14.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaDTrIysfzS9"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y numpy\n",
        "\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Yq1570Af_bT"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install -U cmake ninja wheel\n",
        "\n",
        "git clone --depth 1 --branch v0.14.6 https://github.com/SHI-Labs/NATTEN.git\n",
        "cd NATTEN\n",
        "\n",
        "export FORCE_CUDA=1\n",
        "export TORCH_CUDA_ARCH_LIST=\"8.0\"\n",
        "\n",
        "pip install .\n",
        "\n",
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDXPSqtWgBCP"
      },
      "outputs": [],
      "source": [
        "!rm -rf DAT && git clone -q https://github.com/LeapLabTHU/DAT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWDxhY7AgCTK"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "git clone --depth 1 https://github.com/OpenGVLab/DCNv4.git\n",
        "cd DCNv4/DCNv4_op\n",
        "export FORCE_CUDA=1\n",
        "export TORCH_CUDA_ARCH_LIST=\"8.0\"\n",
        "python -m pip install . --no-build-isolation -v   # 4-5 dk\n",
        "cd ../..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX_lD4eMgu9O"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!pip -q install opendatasets\n",
        "import opendatasets as od\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/vijayabhaskar96/pascal-voc-2007-and-2012\",\n",
        "    data_dir=\"/content/voc\")          # → /content/voc/VOCdevkit/{VOC2007,VOC2012}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_qDpzlgl8vB"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import VOCDetection\n",
        "from torch.utils.data import ConcatDataset\n",
        "ROOT = \"/content/voc/pascal-voc-2007-and-2012\"\n",
        "train07 = VOCDetection(ROOT, \"2007\", \"trainval\")   # 5011\n",
        "train12 = VOCDetection(ROOT, \"2012\", \"trainval\")   # 11540\n",
        "combined = ConcatDataset([train07, train12])       # 16551\n",
        "test07   = VOCDetection(ROOT, \"2007\", \"test\")      # 4952x\n",
        "\n",
        "print(len(train07), len(train12), len(combined), len(test07))\n",
        "# 5011 11540 16551 4952"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B0W_sajonB2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import from actual DAT and DCNv4 modules\n",
        "import importlib\n",
        "from DAT.models.dat import DAT, LayerScale, TransformerStage\n",
        "from DAT.models.dat_blocks import LayerNormProxy, TransformerMLP, TransformerMLPWithConv\n",
        "from DAT.models.dat_blocks import LocalAttention, DAttentionBaseline, ShiftWindowAttention, PyramidAttention\n",
        "from DCNv4.modules.dcnv4 import DCNv4\n",
        "dat_mod = importlib.import_module(\"DAT.models.dat\")\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from __future__ import annotations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.layers import DropPath\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from mmcv.ops.multi_scale_deform_attn import MultiScaleDeformableAttention\n",
        "from timm.models.layers import create_act_layer\n",
        "from typing import Optional, List, Dict, Tuple,  Union\n",
        "\n",
        "\n",
        "\n",
        "class NormFactory:\n",
        "    SUPPORTED = {\"bn\", \"gn\", \"lnp\"}\n",
        "\n",
        "    def __init__(self, kind: str = \"gn\", gn_groups: int = 32):\n",
        "        kind = kind.lower()\n",
        "        if kind not in self.SUPPORTED:\n",
        "            raise ValueError(f\"kind must be one of {self.SUPPORTED}\")\n",
        "        self.kind = kind\n",
        "        self.gn_groups = gn_groups\n",
        "\n",
        "    def __call__(self, num_feat: int) -> nn.Module:\n",
        "        if self.kind == \"bn\":\n",
        "            return nn.BatchNorm2d(num_feat)\n",
        "        if self.kind == \"gn\":\n",
        "            g = math.gcd(self.gn_groups, num_feat) or 1\n",
        "            return nn.GroupNorm(g, num_feat)\n",
        "\n",
        "        if self.kind == \"lnp\":\n",
        "            return LayerNormProxy(num_feat)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. CrossScaleInjection\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class CrossScaleInjection(nn.Module):\n",
        "    \"\"\"Cross‑scale feature enjeksiyonu – kanal‑başına gating versiyonu\"\"\"\n",
        "    def __init__(self, low_ch: int, high_ch: int):\n",
        "        super().__init__()\n",
        "        self.align = nn.Conv2d(low_ch, high_ch, 1, bias=False)\n",
        "        self.norm = LayerNormProxy(high_ch)\n",
        "\n",
        "        # Channel based weight; starting 0.1\n",
        "        self.weight = nn.Parameter(torch.ones(1, high_ch, 1, 1) * 0.15)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.align.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, low_res: torch.Tensor, high_res: torch.Tensor) -> torch.Tensor:\n",
        "        low_aligned = self.align(low_res)\n",
        "        low_up = F.interpolate(low_aligned, size=high_res.shape[-2:],\n",
        "                              mode='bilinear', align_corners=False)\n",
        "        low_up = self.norm(low_up)\n",
        "        # sigmoid → [0,1] for per channel λ\n",
        "        return high_res + torch.sigmoid(self.weight) * low_up\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PGI– Programmable Gradient Injection\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class PGIModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Programmable Gradient Injection v2 – DDP‑safe\n",
        "    * If aux_channels is provided, a 1x1 ‘aux_adapter’ is set up during initialization (static, DDP‑safe).\n",
        "    * The auxiliary contribution is only added in training mode.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        reduction: int = 4,\n",
        "        lambda_bounds: tuple[float, float] = (0.2, 0.7),\n",
        "        init_lambda: float = 0.5,\n",
        "        use_bn: bool = True,\n",
        "        drop_path: float = 0.0,\n",
        "        norm_factory: NormFactory = NormFactory(\"gn\"),\n",
        "        aux_channels: int | None = None,   # ◀︎ yeni\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.lambda_min, self.lambda_max = lambda_bounds\n",
        "\n",
        "        # main branch\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, 3, 1, 1,\n",
        "                      groups=max(1, channels // 4), bias=False),\n",
        "            norm_factory(channels) if use_bn else nn.Identity(),\n",
        "        )\n",
        "\n",
        "        mid = max(channels // reduction, 32)\n",
        "        self.aux = nn.Sequential(\n",
        "            nn.Conv2d(channels, mid, 1, bias=False),\n",
        "            norm_factory(mid) if use_bn else nn.Identity(),\n",
        "            nn.SiLU(inplace=True),\n",
        "            nn.Conv2d(mid, channels, 1, bias=False),\n",
        "            norm_factory(channels) if use_bn else nn.Identity(),\n",
        "        )\n",
        "\n",
        "        # Aux channel adapter (static)\n",
        "        if aux_channels is None or aux_channels == channels:\n",
        "            self.aux_adapter = nn.Identity()\n",
        "        else:\n",
        "            self.aux_adapter = nn.Conv2d(aux_channels, channels, 1, bias=False)\n",
        "            nn.init.kaiming_normal_(self.aux_adapter.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "        # λ (learnable)\n",
        "        import math\n",
        "        init_logit = math.log((init_lambda - self.lambda_min) / (self.lambda_max - init_lambda))\n",
        "        self._lambda_logit = nn.Parameter(torch.tensor(init_logit))\n",
        "\n",
        "        # Gate start 0.6\n",
        "        self.gate = nn.Parameter(torch.ones(1, channels, 1, 1) * 0.5)\n",
        "\n",
        "        self.dp = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "        self._init_weights()\n",
        "\n",
        "    @property\n",
        "    def lambda_val(self) -> torch.Tensor:\n",
        "        σ = torch.sigmoid(self._lambda_logit)\n",
        "        return self.lambda_min + (self.lambda_max - self.lambda_min) * σ\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                if hasattr(m, 'weight'): nn.init.ones_(m.weight)\n",
        "                if hasattr(m, 'bias'):   nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _align_spatial(self, feat: torch.Tensor, target_hw: tuple[int, int]) -> torch.Tensor:\n",
        "        if feat.shape[-2:] != target_hw:\n",
        "            feat = F.interpolate(feat, size=target_hw, mode='bilinear', align_corners=False)\n",
        "        return feat\n",
        "\n",
        "    def forward(self, x: torch.Tensor, aux_input: torch.Tensor | None = None):\n",
        "        main_out = self.main(x)\n",
        "\n",
        "        if aux_input is not None and self.training:\n",
        "            aux = self.aux_adapter(aux_input)\n",
        "            aux = self._align_spatial(aux, x.shape[-2:])\n",
        "            aux = self.aux(aux) * self.gate\n",
        "            main_out = main_out + self.lambda_val * aux\n",
        "\n",
        "        return x + self.dp(main_out)\n",
        "\n",
        "def get_drop_path_rates(num_layers: int, max_rate: float) -> List[float]:\n",
        "    \"\"\"Generate drop path rates with cosine scheduling\n",
        "\n",
        "    Args:\n",
        "        num_layers: Total number of layers\n",
        "        max_rate: Maximum drop path rate\n",
        "\n",
        "    Returns:\n",
        "        List of drop path rates for each layer\n",
        "    \"\"\"\n",
        "    if num_layers <= 1:\n",
        "        return [0.0] * num_layers\n",
        "\n",
        "    # Cosine scheduling for smooth progression\n",
        "    rates = [\n",
        "        max_rate * (1.0 - math.cos(math.pi * i / (num_layers - 1))) * 0.5\n",
        "        for i in range(num_layers)\n",
        "    ]\n",
        "\n",
        "    return rates\n",
        "\n",
        "\n",
        "def init_weights_improved(model: nn.Module):\n",
        "    \"\"\"Improved weight initialization for all module types\"\"\"\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            # Special handling for different conv types\n",
        "            if 'dw' in name or module.groups > 1:  # Depthwise\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='linear')\n",
        "            elif 'pw' in name or module.kernel_size == (1, 1):  # Pointwise\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "            else:  # Regular conv\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.BatchNorm2d):\n",
        "            nn.init.ones_(module.weight)\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, LayerNormProxy):\n",
        "            if hasattr(module, 'weight'):\n",
        "                nn.init.ones_(module.weight)\n",
        "            if hasattr(module, 'bias'):\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            nn.init.trunc_normal_(module.weight, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, std=0.02)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Channel-wise Attention (SE)\n",
        "# -----------------------------------------------------------------------------\n",
        "class ChannelAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, channels: int, reduction: int = 8):\n",
        "        super().__init__()\n",
        "        mid = max(channels // reduction, 16)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(channels, mid, 1, bias=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Conv2d(mid, channels, 1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # --- parameters\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_out', nonlinearity='sigmoid')\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "        # ►–– init scale ≈\n",
        "        self.residual_scale = nn.Parameter(torch.tensor(-2.0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        y = self.avgpool(x)\n",
        "        y = self.relu(self.fc1(y))\n",
        "        y = self.sigmoid(self.fc2(y))\n",
        "\n",
        "        scale = torch.sigmoid(self.residual_scale)   # ∈(0,1)\n",
        "        return x * (1 - scale) + x * y * scale\n",
        "# 5. Gradient-clip helpers\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def clip_dcnv4_grads(model: nn.Module, max_norm: float = 0.2) -> None:\n",
        "    \"\"\"Clip gradients for DCNv4 offset/mask parameters - more aggressive\"\"\"\n",
        "    target_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is None:\n",
        "            continue\n",
        "        n_low = name.lower()\n",
        "        if \"offset\" in n_low or \"mask\" in n_low:\n",
        "            target_params.append(param)\n",
        "\n",
        "    if target_params:\n",
        "        torch.nn.utils.clip_grad_norm_(target_params, max_norm=max_norm)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Helper classes\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def fix_zero_init_params(model):\n",
        "    \"\"\"Fix parameters that are initialized to zero\"\"\"\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.dim() > 0 and torch.all(param == 0):\n",
        "            print(f\"Fixing zero-initialized parameter: {name}\")\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant_(param, 0.01)\n",
        "            else:\n",
        "                nn.init.normal_(param, std=0.01)\n",
        "\n",
        "def apply_gradient_checkpointing(model):\n",
        "    \"\"\"Apply gradient checkpointing to backbone stages - IMPROVED\"\"\"\n",
        "    if hasattr(model, 'backbone'):\n",
        "        # Apply a checkpoint for each stage\n",
        "        for stage_name in ['st0', 'st1', 'st2', 'st3']:\n",
        "            if hasattr(model.backbone, stage_name):\n",
        "                stage = getattr(model.backbone, stage_name)\n",
        "                # Make the sequential module checkpoint-friendly\n",
        "                class CheckpointedSequential(nn.Module):\n",
        "                    def __init__(self, *layers):\n",
        "                        super().__init__()\n",
        "                        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "                    def forward(self, x):\n",
        "                        for layer in self.layers:\n",
        "                            x = checkpoint(layer, x, use_reentrant=False)\n",
        "                        return x\n",
        "\n",
        "                # Recreate Stage\n",
        "                checkpointed = CheckpointedSequential(*[block for block in stage])\n",
        "                setattr(model.backbone, stage_name, checkpointed)\n",
        "\n",
        "def init_conv(layer: nn.Conv2d, fan_out: bool = True):\n",
        "    \"\"\"Kaiming normal initialization\"\"\"\n",
        "    nn.init.kaiming_normal_(\n",
        "        layer.weight,\n",
        "        mode=\"fan_out\" if fan_out else \"fan_in\",\n",
        "        nonlinearity=\"relu\",\n",
        "    )\n",
        "    if layer.bias is not None:\n",
        "        nn.init.zeros_(layer.bias)\n",
        "\n",
        "\n",
        "class LNAct(nn.Module):\n",
        "    \"\"\"LayerNormProxy + SiLU\"\"\"\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNormProxy(channels)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.norm(x))\n",
        "\n",
        "\n",
        "class ConvLNAct(nn.Sequential):\n",
        "    \"\"\"Conv + LayerNorm + Act fusion\"\"\"\n",
        "    def __init__(self, in_ch: int, out_ch: int, k: int = 3,\n",
        "                s: int = 1, p: int = None, g: int = 1):\n",
        "        p = p if p is not None else k // 2\n",
        "        super().__init__(\n",
        "            nn.Conv2d(in_ch, out_ch, k, s, p, groups=g, bias=False),\n",
        "            LNAct(out_ch)\n",
        "        )\n",
        "        nn.init.kaiming_normal_(self[0].weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# LayerScale\n",
        "# -----------------------------------------------------------------------------\n",
        "class LayerScale(nn.Module):\n",
        "    \"\"\"\n",
        "    Learnable scalar per layer (γ) – Broadcast-Safe version.\n",
        "    Works with both (B,C,H,W) and (B,N,C) tensor layouts.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        inplace: bool = False,\n",
        "        init_values: float = 1.0,\n",
        "        depth: int | None = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.inplace = inplace\n",
        "        if depth is not None:\n",
        "            init_values = min(1.0, 0.5 * depth / 3)\n",
        "        self.weight = nn.Parameter(torch.ones(dim) * init_values)\n",
        "\n",
        "    def _shape_for(self, x: torch.Tensor) -> tuple[int, ...]:\n",
        "        \"\"\"\n",
        "        Determines the shape to broadcast the weight based on x's placement.\n",
        "        Priority: channel-end (… , C) → channel-first (B, C, …) → singular cases.\n",
        "        \"\"\"\n",
        "        C = self.weight.numel()\n",
        "        nd = x.dim()\n",
        "\n",
        "        # 4D  image: (B, C, H, W) veya (B, H, W, C)\n",
        "        if nd == 4:\n",
        "            if x.shape[1] == C:          # (B, C, H, W)\n",
        "                return (1, C, 1, 1)\n",
        "            if x.shape[-1] == C:         # (B, H, W, C) - nadir\n",
        "                return (1, 1, 1, C)\n",
        "\n",
        "        # 3D: (B, N, C) or (B, C, N) or (C, H, W)\n",
        "        if nd == 3:\n",
        "            if x.shape[-1] == C:         # (B, N, C)\n",
        "                return (1, 1, C)\n",
        "            if x.shape[1] == C:          # (B, C, N)\n",
        "                return (1, C, 1)\n",
        "            if x.shape[0] == C:          # (C, H, W) / (C, N, ?)such as extreme cases\n",
        "                return (C, 1, 1)\n",
        "\n",
        "        # 2D: (B, C) veya (C, B)\n",
        "        if nd == 2:\n",
        "            if x.shape[-1] == C:         # (B, C)\n",
        "                return (1, C)\n",
        "            if x.shape[0] == C:          # (C, B)\n",
        "                return (C, 1)\n",
        "\n",
        "\n",
        "        shape = [1] * max(1, nd)\n",
        "        shape[-1] = C\n",
        "        return tuple(shape)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        w = self.weight.view(*self._shape_for(x))\n",
        "        if self.inplace:\n",
        "            return x.mul_(w)\n",
        "        return x * w\n",
        "\n",
        "# Synchronize the LayerScale within DAT with this version (old behavior is preserved)\n",
        "dat_mod.LayerScale = LayerScale\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Stem\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class Stem(nn.Module):\n",
        "    \"\"\"Three 3×3 convolutions with stride pattern (2,1,2)\"\"\"\n",
        "    def __init__(self, out_channels: int = 128):\n",
        "        super().__init__()\n",
        "        c_mid = out_channels // 2\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, c_mid, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(c_mid, c_mid, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(c_mid, out_channels, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "        self.norm = LayerNormProxy(out_channels)\n",
        "        self.act = nn.GELU()\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in [self.conv1, self.conv2, self.conv3]:\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.act(self.conv1(x))\n",
        "        x = self.act(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# DCNv4Lite\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class DCNv4Lite(nn.Module):\n",
        "    \"\"\"\n",
        "    • Fully wraps the DCNv4 path (2D→3D→2D), NO baseline conv/residual mix.\n",
        "    • Offset/mask parameters are initialized with small std (for stable fp32 training).\n",
        "    • Silently discards ‘mix_*’ and ‘conv.*’ keys in old checkpoints.\n",
        "\n",
        "    Args:\n",
        "        channels (int)            : Num of channels\n",
        "        group (int)               : DCNv4 group\n",
        "        kernel_size (int)         : Kernel\n",
        "        stride (int)              : Stride\n",
        "        pad (int)                 : Padding\n",
        "        dilation (int)            : Dilation\n",
        "        offset_scale (float)      : DCNv4 offset scale\n",
        "        without_pointwise (bool)  : Enable/disable the internal 1x1 project in DCNv4 (default: True)\n",
        "        init_offset_mask_std (float): offset/mask start std (default: 0.01)\n",
        "        **kwargs                  : It is forwarded to DCNv4 exactly as is.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 channels: int,\n",
        "                 group: int = 8,\n",
        "                 kernel_size: int = 3,\n",
        "                 stride: int = 1,\n",
        "                 pad: int = 1,\n",
        "                 dilation: int = 1,\n",
        "                 offset_scale: float = 0.1,\n",
        "                 *,\n",
        "                 without_pointwise: bool = True,\n",
        "                 init_offset_mask_std: float = 0.01,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "        self.channels = int(channels)\n",
        "\n",
        "        #  DCNv4\n",
        "        self.dcn = DCNv4(\n",
        "            channels=channels, kernel_size=kernel_size, stride=stride, pad=pad,\n",
        "            dilation=dilation, group=group, offset_scale=offset_scale,\n",
        "            center_feature_scale=False, remove_center=False,\n",
        "            output_bias=True, without_pointwise=without_pointwise, **kwargs\n",
        "        )\n",
        "\n",
        "        # Offset/Mask secure init\n",
        "        self._init_dcn_params(std=init_offset_mask_std)\n",
        "\n",
        "        # Save geometry for representation (debug)\n",
        "        self._geom = dict(k=kernel_size, s=stride, p=pad, d=dilation, g=group,\n",
        "                          wop=without_pointwise)\n",
        "\n",
        "    # ---------------- init helpers ----------------\n",
        "    def _init_dcn_params(self, std: float = 0.01):\n",
        "        \"\"\"\n",
        "        Offset/mask parameters are initialized with small normals so that\n",
        "        sigmoid(0)=0.5 exits the plateau lock; other parameters remain in DCNv4's\n",
        "        default init.\n",
        "        \"\"\"\n",
        "        std = float(max(std, 1e-5))\n",
        "        for n, p in self.dcn.named_parameters():\n",
        "            n_low = n.lower()\n",
        "            if ('offset' in n_low) or ('mask' in n_low):\n",
        "                nn.init.normal_(p, std=std)\n",
        "\n",
        "    # ---------------- forward ----------------------\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, C, H, W) → y: (B, C, H, W)\n",
        "        The DCNv4 interface (B, HW, C) requires a 2D→3D→2D conversion.\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        x3d = x.permute(0, 2, 3, 1).contiguous().view(B, H * W, C)   # (B, HW, C)\n",
        "        y3d = self.dcn(x3d, shape=(H, W))                            # (B, HW, C)\n",
        "        y   = y3d.view(B, H, W, C).permute(0, 3, 1, 2).contiguous()  # (B, C, H, W)\n",
        "        return y\n",
        "\n",
        "    # --------------- helpers -------------------\n",
        "    @torch.no_grad()\n",
        "    def freeze_offsets(self, flag: bool = True):\n",
        "        \"\"\"Freeze/unfreeze offset/mask parameters (e.g., for warming).\"\"\"\n",
        "        for n, p in self.dcn.named_parameters():\n",
        "            n_low = n.lower()\n",
        "            if ('offset' in n_low) or ('mask' in n_low):\n",
        "                p.requires_grad = (not flag)\n",
        "\n",
        "    def offset_mask_parameters(self):\n",
        "        \"\"\"In the Optimizer, it yields the offset/mask parameters to provide separate LR/clip.\"\"\"\n",
        "        for n, p in self.dcn.named_parameters():\n",
        "            n_low = n.lower()\n",
        "            if ('offset' in n_low) or ('mask' in n_low):\n",
        "                yield p\n",
        "\n",
        "    # Ignore the mix/baseline keys remaining in old checkpoints\n",
        "    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):\n",
        "        obsolete = (\n",
        "            \"mix_weight\", \"mix_logit\", \"_mix_tau\", \"_mix_eps\", \"_mix_clamp\",\n",
        "            \"conv.weight\", \"conv.bias\"\n",
        "        )\n",
        "        for key in list(state_dict.keys()):\n",
        "            if key.startswith(prefix) and any(key.endswith(obs) for obs in obsolete):\n",
        "                state_dict.pop(key)\n",
        "        return super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        g = self._geom\n",
        "        return (f\"channels={self.channels}, k={g['k']}, s={g['s']}, p={g['p']}, \"\n",
        "                f\"d={g['d']}, group={g['g']}, without_pointwise={g['wop']}\")\n",
        "\n",
        "\n",
        "def boost_relative_position_bias(model: nn.Module, std: float = 0.02):\n",
        "    \"\"\"\n",
        "    Resets all `relative_position_bias_table` parameters in all ShiftWindow/LocalAttention modules belonging to the model.\n",
        "    \"\"\"\n",
        "    for m in model.modules():\n",
        "        if hasattr(m, \"relative_position_bias_table\"):\n",
        "            nn.init.trunc_normal_(m.relative_position_bias_table, std=std)\n",
        "\n",
        "def init_decoder_sampling_offsets(decoder: nn.Module, bias_val: float = 0.1):\n",
        "    \"\"\"\n",
        "    Set the sampling_offsets bias in the deformable decoder layers\n",
        "    to a fixed value; a small shift triggers the gradient.\n",
        "    \"\"\"\n",
        "    for n, p in decoder.named_parameters():\n",
        "        if \"sampling_offsets.bias\" in n:\n",
        "            nn.init.constant_(p, bias_val)\n",
        "\n",
        "\n",
        "def apply_hybrid_fixup(model: nn.Module):\n",
        "    \"\"\"\n",
        "    a) Relative-pos bias table\n",
        "    b) Decoder sampling_offsets bias\n",
        "    c) (Optional) DCNv4 offset/mask gradient LR boost\n",
        "    \"\"\"\n",
        "    boost_relative_position_bias(model)\n",
        "    if hasattr(model, \"decoder\"):\n",
        "        init_decoder_sampling_offsets(model.decoder, 0.1)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Stage-0 Block\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class StageZeroBlock(nn.Module):\n",
        "    def __init__(self, channels=128, drop_path_rate=0.0,\n",
        "                 use_layer_scale=True, layer_scale_init=1.0,\n",
        "                 norm_factory=NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.conv3 = nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\n",
        "        nn.init.kaiming_normal_(self.conv3.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "        self.se   = ChannelAttention(channels, reduction=8)\n",
        "        self.norm = norm_factory(channels)\n",
        "        self.scale = LayerScale(channels, init_values=layer_scale_init) if use_layer_scale else nn.Identity()\n",
        "        self.dp = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        out = self.conv3(x)\n",
        "        out = self.se(out)\n",
        "        out = self.norm(out)\n",
        "        out = self.scale(out)\n",
        "        out = self.dp(out)\n",
        "        return res + out\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Stage Blocks\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class StageOneBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    DCNv4Lite + SE + (optional) LayerScale + DropPath\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                channels: int = 128,\n",
        "                drop_path: float = 0.0,\n",
        "                use_layer_scale: bool = True,\n",
        "                layer_scale_init: float = 1.0,\n",
        "                norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.dcn = DCNv4Lite(channels, group=8)\n",
        "        self.se  = ChannelAttention(channels, reduction=8)\n",
        "        self.post_norm = norm_factory(channels)\n",
        "\n",
        "        self.scale = (LayerScale(channels, init_values=layer_scale_init)\n",
        "                      if use_layer_scale else nn.Identity())\n",
        "\n",
        "        self.dp = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        out = self.dcn(x)\n",
        "        out = self.se(out)\n",
        "        out = self.post_norm(out)\n",
        "        out = self.scale(out)\n",
        "        out = self.dp(out)\n",
        "        return res + out\n",
        "\n",
        "class DownABlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Depthwise‑Conv (s=2)  +  Pointwise‑Conv  +  Norm + Act\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                in_ch: int = 128,\n",
        "                out_ch: int = 256,\n",
        "                norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1) Depth‑wise strided conv\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1,\n",
        "                            groups=in_ch, bias=False)\n",
        "\n",
        "        # 2) Point‑wise projection\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "\n",
        "        # 3) Normalization + Activation\n",
        "        #    a) If the norm type is LayerNormProxy, use LNAct\n",
        "        #    b) In other cases, use norm + SiLU\n",
        "        nf_layer = norm_factory(out_ch)\n",
        "        if isinstance(nf_layer, LayerNormProxy):\n",
        "            # LNAct = LayerNormProxy + SiLU\n",
        "            from DAT.models.dat_blocks import LNAct as _LNAct\n",
        "            self.norm_act = _LNAct(out_ch)\n",
        "        else:\n",
        "            self.norm_act = nn.Sequential(nf_layer, nn.SiLU(inplace=True))\n",
        "\n",
        "        # --- Kaiming init ---\n",
        "        nn.init.kaiming_normal_(self.dw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.pw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.dw(x)\n",
        "        x = self.pw(x)\n",
        "        return self.norm_act(x)\n",
        "\n",
        "\n",
        "class StageTwoBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Stage‑2: Only DCNv4Lite path (no DAT, no GateFuse).\n",
        "    CSP split is kept for comparable FLOPs/latency.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 channels: int = 256,\n",
        "                 dcn_group: int = 16,\n",
        "                 drop_path: float = 0.1,\n",
        "                 norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        assert channels % 2 == 0, \"StageTwoBlock expects even channel count.\"\n",
        "        self.split_channels = channels // 2\n",
        "\n",
        "        # --- DCNv4 branch (only this one active) ---\n",
        "        self.dcn = DCNv4Lite(self.split_channels, group=max(1, dcn_group // 2))\n",
        "        self.post_norm = norm_factory(self.split_channels)\n",
        "        self.post_act  = nn.SiLU(inplace=True)\n",
        "\n",
        "        # --- Skip proj (for grad out) ---\n",
        "        self.skip_proj = nn.Sequential(\n",
        "            nn.Conv2d(self.split_channels, self.split_channels, 1, bias=False),\n",
        "            norm_factory(self.split_channels)\n",
        "        )\n",
        "\n",
        "        # --- Output merging ---\n",
        "        self.fusion = nn.Conv2d(channels, channels, 1, bias=False)\n",
        "        self.ln     = norm_factory(channels)\n",
        "        self.act    = nn.SiLU(inplace=True)\n",
        "        self.dp     = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "        # Init\n",
        "        nn.init.kaiming_normal_(self.fusion.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.skip_proj[0].weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # CSP split\n",
        "        x1, x2 = torch.split(x, self.split_channels, dim=1)\n",
        "\n",
        "        # Skip branch\n",
        "        x1 = self.skip_proj(x1)\n",
        "\n",
        "        # DCNv4 branch\n",
        "        x2 = self.dcn(x2)\n",
        "        x2 = self.post_act(self.post_norm(x2))\n",
        "\n",
        "        # Fuse & residual\n",
        "        out = torch.cat([x1, x2], dim=1)\n",
        "        out = self.fusion(out)\n",
        "        out = self.act(self.ln(out))\n",
        "        return x + self.dp(out)\n",
        "\n",
        "\n",
        "class DownBBlock(nn.Module):\n",
        "    \"\"\"DW 3×3 s2 → PW 1×1, 256c → 640c\"\"\"\n",
        "    def __init__(self, in_ch: int = 256, out_ch: int = 640,norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1,\n",
        "                          groups=in_ch, bias=False)\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.norm = norm_factory(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "        for m in [self.dw, self.pw]:\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.act(self.norm(self.pw(self.dw(x))))\n",
        "\n",
        "\n",
        "class StageThreeBlock(nn.Module):\n",
        "    \"\"\"Double DATHubLite + ChannelAttention\"\"\"\n",
        "    def __init__(self, in_ch: int = 640, out_ch: int = 640, drop_path: float = 0.15,norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.norm = norm_factory(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "        heads = max(4, out_ch // 64)\n",
        "        self.dat1 = DATBlock(out_ch, heads=heads)\n",
        "        self.dat2 = DATBlock(out_ch, heads=heads)\n",
        "        self.dp1 = DropPath(drop_path)\n",
        "        self.dp2 = DropPath(drop_path)\n",
        "        self.ca = ChannelAttention(out_ch, reduction=8)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.proj.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.act(self.norm(self.proj(x)))\n",
        "        res = x\n",
        "        x = self.dat1(x)\n",
        "        x = self.dp1(x) + res\n",
        "        res = x\n",
        "        x = self.dat2(x)\n",
        "        x = self.dp2(x) + res\n",
        "        x = self.ca(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DownCBlock(nn.Module):\n",
        "    \"\"\"DW 3×3 s2 → PW 1×1, 640c → 768c\"\"\"\n",
        "    def __init__(self, in_ch: int = 640, out_ch: int = 768,norm_factory: NormFactory = NormFactory(\"gn\")):\n",
        "        super().__init__()\n",
        "        self.dw = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1,\n",
        "                          groups=in_ch, bias=False)\n",
        "        self.pw = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.norm = norm_factory(out_ch)\n",
        "        self.act = nn.SiLU(inplace=True)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.dw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        nn.init.kaiming_normal_(self.pw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dw(x)\n",
        "        x = self.pw(x)\n",
        "\n",
        "        return self.act(self.norm(x))\n",
        "# -----------------------------------------------------------------------------\n",
        "# StageAwareBackbone\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class StageAwareBackbone(nn.Module):\n",
        "    \"\"\"\n",
        "    Four-tier backbone based on DAT + DCN\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                depths: list[int] = (3, 4, 8, 3),\n",
        "                drop_path_max: float = 0.0,\n",
        "                num_classes: int = 80,\n",
        "                voc_prior: bool = False,\n",
        "                norm_factory: NormFactory = NormFactory(\"gn\"),\n",
        "                use_layer_scale: bool = True,\n",
        "                layer_scale_init: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.voc_prior   = voc_prior\n",
        "\n",
        "        # ---- DropPath rates ----\n",
        "        total_blocks = sum(depths)\n",
        "        dpr = get_drop_path_rates(total_blocks, drop_path_max)\n",
        "        for i in range(depths[0] + depths[1]):   # st0, st1\n",
        "            dpr[i] = 0.0\n",
        "\n",
        "        # ---- Stem ----\n",
        "        self.stem = Stem(128)\n",
        "\n",
        "        # ---- Stage‑0 ----\n",
        "        dp_idx = 0\n",
        "        self.st0 = nn.Sequential(*[\n",
        "          StageZeroBlock(\n",
        "              128,\n",
        "              drop_path_rate=dpr[i],\n",
        "              use_layer_scale=use_layer_scale,\n",
        "              layer_scale_init=layer_scale_init,\n",
        "              norm_factory=norm_factory\n",
        "          )\n",
        "          for i in range(depths[0])\n",
        "      ])\n",
        "        dp_idx += depths[0]\n",
        "\n",
        "        # ---- Stage‑1 ----\n",
        "        self.st1 = nn.Sequential(*[\n",
        "            StageOneBlock(\n",
        "                128,\n",
        "                drop_path=dpr[dp_idx + i],\n",
        "                use_layer_scale=use_layer_scale,\n",
        "                layer_scale_init=layer_scale_init,\n",
        "                norm_factory=norm_factory)\n",
        "            for i in range(depths[1])\n",
        "        ])\n",
        "        dp_idx += depths[1]\n",
        "\n",
        "        # ---- PGI + Down/CSI ----\n",
        "        self.pgi_s1 = PGIModule(128, norm_factory=norm_factory, aux_channels=128)   # s0_out → s1_raw\n",
        "        self.da     = DownABlock(128, 256, norm_factory=norm_factory)\n",
        "        self.csi_s1_s2 = CrossScaleInjection(low_ch=128, high_ch=256)\n",
        "        self.pgi_s2 = PGIModule(256, norm_factory=norm_factory, aux_channels=128)\n",
        "\n",
        "        # ---- Stage‑2 ----\n",
        "        self.st2 = nn.Sequential(*[\n",
        "            StageTwoBlock(\n",
        "                256, dcn_group=16,\n",
        "                drop_path=dpr[dp_idx + i],\n",
        "                norm_factory=norm_factory)\n",
        "            for i in range(depths[2])\n",
        "        ])\n",
        "        dp_idx += depths[2]\n",
        "\n",
        "        # ---- DownB / PGI ----\n",
        "        self.db     = DownBBlock(256, 640, norm_factory=norm_factory)\n",
        "        self.pgi_s3 = PGIModule(640, norm_factory=norm_factory, aux_channels=256)\n",
        "\n",
        "        # ---- Stage‑3 ----\n",
        "        self.st3 = nn.Sequential(*[\n",
        "            StageThreeBlock(\n",
        "                640, 640,\n",
        "                drop_path=dpr[dp_idx + i],\n",
        "                norm_factory=norm_factory)\n",
        "            for i in range(depths[3])\n",
        "        ])\n",
        "\n",
        "        # ---- DownC ----\n",
        "        self.dc = DownCBlock(640, 768, norm_factory=norm_factory)\n",
        "\n",
        "        # ---- Auxiliary dense heads ----\n",
        "        self.aux_head_s1 = nn.Conv2d(128, num_classes + 4, 1, bias=True)\n",
        "        self.aux_head_s2 = nn.Conv2d(256, num_classes + 4, 1, bias=True)\n",
        "        self.aux_head_s3 = nn.Conv2d(640, num_classes + 4, 1, bias=True)\n",
        "        self._init_aux_heads()\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def _init_aux_heads(self):\n",
        "        prior_value = 19 if self.voc_prior else 99\n",
        "        for head in (self.aux_head_s1, self.aux_head_s2, self.aux_head_s3):\n",
        "            nn.init.normal_(head.weight, std=0.01)\n",
        "            nn.init.constant_(head.bias[: self.num_classes], -math.log(prior_value))\n",
        "            nn.init.constant_(head.bias[self.num_classes :], 0.0)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    def forward(self, x: torch.Tensor, need_aux: bool = False):\n",
        "        x      = self.stem(x)\n",
        "        s0_out = self.st0(x)\n",
        "\n",
        "        s1_raw = self.st1(s0_out)\n",
        "        p3     = self.pgi_s1(s1_raw, aux_input=s0_out)\n",
        "\n",
        "        p3_down = self.da(p3)\n",
        "        s2_in   = self.csi_s1_s2(p3, p3_down)\n",
        "        s2_in   = self.pgi_s2(s2_in, aux_input=p3)\n",
        "\n",
        "        p4 = self.st2(s2_in)\n",
        "        p4_down = self.db(p4)\n",
        "        s3_in   = self.pgi_s3(p4_down, aux_input=p4)\n",
        "\n",
        "        p5 = self.st3(s3_in)\n",
        "        p6 = self.dc(p5)\n",
        "\n",
        "        if self.training and need_aux:\n",
        "            aux = {\n",
        "                's1': self.aux_head_s1(p3),\n",
        "                's2': self.aux_head_s2(p4),\n",
        "                's3': self.aux_head_s3(p5),\n",
        "            }\n",
        "            return (p3, p4, p5, p6), aux\n",
        "        return p3, p4, p5, p6\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# SpatialFuseNode\n",
        "# ----------------------------------------------------------\n",
        "class SpatialFuseNode(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatial-aware fuse (group-wise, production-ready, RPB-friendly)\n",
        "      • Group-average energy map per input (B,G,K,H,W)\n",
        "      • 1×1 projection per group (K→K), softplus-normalized\n",
        "      • Initial fully uniform: proj.weight=0, bias=0  → equal w for each branch\n",
        "      • Learnable temperature τ (with clamp), optional uniform floor (no dead branches)\n",
        "      • No detach anywhere → gradients are unclipped (including RPBs)\n",
        "\n",
        "    Args:\n",
        "        n_inputs (int): K, how many features to combine (>=2)\n",
        "        channels (int): C, number of channels\n",
        "        groups (int): G, number of groups (automatically drops to 1 if C % G ≠ 0)\n",
        "        tau_init (float): τ start (recommended: 0.9)\n",
        "        learnable_tau (bool): Should τ be learned?\n",
        "        eps (float): Numerical epsilon for normalization and splitting\n",
        "        init_noise (float): (backward compatibility parameter; not used with uniform init)\n",
        "        gate_floor (float): w ← (1-α)·w + α·(1/K); α∈[0,0.1] recommended, 0 closed\n",
        "        tau_bounds (tuple): τ lower/upper bounds (e.g., (0.5, 2.0))\n",
        "        uniform_init (bool): True ⇒ weight=0, bias=0 (full uniform initialization)\n",
        "        save_last (bool): If True, last w is saved (for debug/regularizer)\n",
        "        prenorm (str): “none” | “rms”  — RMS prenorm on the K axis (reduces saturation)\n",
        "\n",
        "    Notes:\n",
        "      • Returns extra_loss() for the entropy regularizer (optional, can be added to the loss).\n",
        "      • The last w (B,G,K,H,W) can be inspected with get_last_weights() (if save_last=True).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_inputs: int,\n",
        "                 channels: int,\n",
        "                 groups: int = 4,\n",
        "                 tau_init: float = 0.9,\n",
        "                 learnable_tau: bool = True,\n",
        "                 eps: float = 5e-4,\n",
        "                 init_noise: float = 1e-3,\n",
        "                 *,\n",
        "                 gate_floor: float = 0.0,\n",
        "                 tau_bounds: tuple = (0.5, 2.0),\n",
        "                 uniform_init: bool = True,\n",
        "                 save_last: bool = False,\n",
        "                 prenorm: str = \"none\"):\n",
        "        super().__init__()\n",
        "        assert n_inputs >= 2, \"SpatialFuseNode: should be n_inputs >= 2 .\"\n",
        "        self.K = int(n_inputs)\n",
        "        self.C = int(channels)\n",
        "        self.G = int(groups) if (groups > 0 and channels % groups == 0) else 1\n",
        "        self.gC = self.C // self.G\n",
        "        self.eps = float(eps)\n",
        "        self.gate_floor = float(gate_floor)\n",
        "        self.tau_lo, self.tau_hi = float(tau_bounds[0]), float(tau_bounds[1])\n",
        "        self.save_last = bool(save_last)\n",
        "        self.prenorm = str(prenorm).lower()\n",
        "        self._last_w = None  # debug amaçlı\n",
        "\n",
        "        # Group-wise K→K projection (produces spatially-varying logit)\n",
        "        # Takes input divided into G groups (B, G*K, H, W) → Output (B, G*K, H, W)\n",
        "        self.proj = nn.Conv2d(self.G * self.K, self.G * self.K, kernel_size=1,\n",
        "                              groups=self.G, bias=True)\n",
        "\n",
        "        # --- Uniform start (to prevent early arm saturation) ---\n",
        "        if uniform_init:\n",
        "            nn.init.zeros_(self.proj.bias)\n",
        "            with torch.no_grad():\n",
        "                self.proj.weight.zero_()\n",
        "        else:\n",
        "            nn.init.kaiming_uniform_(self.proj.weight, a=math.sqrt(5))\n",
        "            nn.init.zeros_(self.proj.bias)\n",
        "\n",
        "        # --- temperature τ ---\n",
        "        if learnable_tau:\n",
        "            self.log_tau = nn.Parameter(torch.log(torch.tensor(float(tau_init))))\n",
        "        else:\n",
        "            self.register_buffer(\"log_tau\", torch.log(torch.tensor(float(tau_init))), persistent=False)\n",
        "\n",
        "    # ------------------ helpers ------------------\n",
        "    @torch.no_grad()\n",
        "    def set_tau(self, tau: float):\n",
        "        \"\"\"To adjust τ during heating.\"\"\"\n",
        "        v = max(1e-3, float(tau))\n",
        "        t = torch.log(torch.tensor(v, device=self.log_tau.device, dtype=self.log_tau.dtype))\n",
        "        self.log_tau.copy_(t)\n",
        "\n",
        "    def get_last_weights(self):\n",
        "        \"\"\"The last calculated w (B, G, K, H, W). If save_last=True, it is saved.\"\"\"\n",
        "        return self._last_w\n",
        "\n",
        "    def extra_loss(self) -> dict:\n",
        "        \"\"\"\n",
        "        Optional regulator: gate entropy (higher values result in more balanced branching).\n",
        "        You can add a small coefficient on the loss side (e.g., 1e-4..5e-4).\n",
        "        \"\"\"\n",
        "        if self._last_w is None:\n",
        "            return {}\n",
        "        p = self._last_w.clamp_min(1e-8)\n",
        "        # Entropy: -sum_k p log p / log(K)  → [0,1]\n",
        "        ent = -(p * p.log()).sum(dim=2) / math.log(self.K)   # (B,G,H,W)\n",
        "        return {\"gate_entropy\": ent.mean()}\n",
        "\n",
        "    def _group_pool(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # (B,C,H,W) → (B,G,H,W), average per group\n",
        "        B, C, H, W = x.shape\n",
        "        return x.view(B, self.G, self.gC, H, W).mean(dim=2)\n",
        "\n",
        "    # ------------------ forward ------------------\n",
        "    def forward(self, *features: torch.Tensor) -> torch.Tensor:\n",
        "        # All inputs must be in the same format.\n",
        "        K = len(features)\n",
        "        assert K == self.K, f\"SpatialFuseNode: {self.K} giriş bekleniyordu, {K} geldi.\"\n",
        "        B, C, H, W = features[0].shape\n",
        "        for f in features:\n",
        "            assert f.shape == (B, C, H, W), \"Tüm giriş feature'lar (B,C,H,W) aynı şekil olmalı.\"\n",
        "\n",
        "        # 1) Grup havuzu → (B,G,K,H,W)\n",
        "        gp = [self._group_pool(f) for f in features]\n",
        "        gp_cat = torch.stack(gp, dim=2)  # (B,G,K,H,W)\n",
        "\n",
        "        # 2) (optional) RMS prenorm → logit scale control on the K axis\n",
        "        if self.prenorm == \"rms\":\n",
        "            rms = gp_cat.pow(2).mean(dim=2, keepdim=True).add(1e-6).sqrt()\n",
        "            gp_cat = gp_cat / rms\n",
        "\n",
        "        # 3) Projection and gate logits\n",
        "        x = gp_cat.flatten(1, 2)                      # (B, G*K, H, W)\n",
        "        logits = self.proj(x).view(B, self.G, self.K, H, W)\n",
        "\n",
        "        # 4) Softplus-normalize + τ\n",
        "        tau = self.log_tau.exp().clamp(self.tau_lo, self.tau_hi)\n",
        "        w = F.softplus(logits / tau) + 1e-9           # (B,G,K,H,W), her yerde >0\n",
        "        w = w / (w.sum(dim=2, keepdim=True) + self.eps)\n",
        "\n",
        "        # 5) Uniform floor (no dead leg, gradient flows to every leg)\n",
        "        if self.gate_floor > 0.0:\n",
        "            u = 1.0 / float(self.K)\n",
        "            w = (1.0 - self.gate_floor) * w + self.gate_floor * u  # no need to normalize again\n",
        "\n",
        "        if self.save_last:\n",
        "            self._last_w = w.detach()\n",
        "\n",
        "        # 6) Distribute weights to channels efficiently (no repeats, memory-friendly)\n",
        "        #    w_k: (B,G,1,H,W), f: (B,G,gC,H,W) → contribution (B,C,H,W)\n",
        "        out = None\n",
        "        for k, f in enumerate(features):\n",
        "            wk = w[:, :, k, :, :].unsqueeze(2)                 # (B,G,1,H,W)\n",
        "            contrib = (f.view(B, self.G, self.gC, H, W) * wk).view(B, C, H, W)\n",
        "            out = contrib if out is None else (out + contrib)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class RMSNorm2d(nn.Module):\n",
        "    def __init__(self, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = float(eps)\n",
        "    def forward(self, x):\n",
        "        return x / (x.pow(2).mean(dim=(2,3), keepdim=True).add(self.eps).sqrt())\n",
        "\n",
        "def _init_laplacian_dw(dw: nn.Conv2d):\n",
        "    with torch.no_grad():\n",
        "        k = torch.tensor([[0., 1., 0.],\n",
        "                          [1.,-4., 1.],\n",
        "                          [0., 1., 0.]], dtype=dw.weight.dtype, device=dw.weight.device)\n",
        "        w = torch.zeros_like(dw.weight)\n",
        "        w[:, 0, :, :] = k\n",
        "        dw.weight.copy_(w)\n",
        "\n",
        "def _ste_boost(x: torch.Tensor, gain: float) -> torch.Tensor:\n",
        "    return x if gain <= 0 else (x + gain * (x - x.detach()))\n",
        "\n",
        "class _FPNResBlock(nn.Module):\n",
        "    def __init__(self, channels: int, drop_path: float = 0.0, norm_factory: 'NormFactory' = None):\n",
        "        super().__init__()\n",
        "        nf = norm_factory or NormFactory(\"gn\")\n",
        "        self.conv = ConvLNAct(channels, channels, k=3)\n",
        "        self.ls   = LayerScale(channels, init_values=0.6)\n",
        "        self.dp   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "    def forward(self, x):\n",
        "        return x + self.dp(self.ls(self.conv(x)))\n",
        "\n",
        "class LightBiFPN(nn.Module):\n",
        "    \"\"\"\n",
        "    LightBiFPN v4.1 — spatial‑aware, RPB‑friendly, repeat‑shared up/edge\n",
        "      • SpatialFuseNode (group‑based 2D weights)\n",
        "      • No‑blur in first top‑down (bilinear + norm/act)\n",
        "      • Up-refine: DW 3×3 + (optional) Laplacian residual (small LS)\n",
        "      • RMS prenorm (optional)\n",
        "      • Low DropPath schedule (0.0 → 0.01)\n",
        "      • Light grad-boost for p3\n",
        "      • **New**: 3 up/edge blocks and **shared between iterations**\n",
        "                  → No “NEVER UPDATED”, no parameter waste\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: tuple[int,int,int,int] = (128, 256, 640, 768),\n",
        "        out: int = 256,\n",
        "        repeats: int = 2,\n",
        "        *,\n",
        "        norm_factory: 'NormFactory' | None = None,\n",
        "        # ---- fuse ----\n",
        "        use_spatial_fuse: bool = True,\n",
        "        fuse_groups: int = 4,\n",
        "        fuse_tau_init: float = 0.9,\n",
        "        fuse_learn_tau: bool = True,\n",
        "        fuse_eps: float = 5e-4,\n",
        "        init_noise: float = 1e-4,\n",
        "        # ---- refine & norm ----\n",
        "        prenorm: str = \"rms\",            # \"none\" | \"rms\"\n",
        "        no_blur_first: bool = True,\n",
        "        edge_enhance: bool = True,\n",
        "        edge_ls_init: float = 0.03,\n",
        "        # ---- drop path ----\n",
        "        dp_top_second: float = 0.01,\n",
        "        dp_bot_second: float = 0.01,\n",
        "        # ---- grad boost ----\n",
        "        grad_boost_low: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        in3, in4, in5, in6 = in_channels\n",
        "        self.repeats = int(repeats)\n",
        "        self.out = int(out)\n",
        "        self.norm_factory = norm_factory or NormFactory(\"gn\")\n",
        "        self.prenorm = prenorm.lower()\n",
        "        self.no_blur_first = bool(no_blur_first)\n",
        "        self.edge_enhance = bool(edge_enhance)\n",
        "        self.grad_boost_low = float(grad_boost_low)\n",
        "        self.use_spatial_fuse = bool(use_spatial_fuse)\n",
        "        self.fuse_groups = int(fuse_groups)\n",
        "        self.fuse_tau_init = float(fuse_tau_init)\n",
        "        self.fuse_learn_tau = bool(fuse_learn_tau)\n",
        "        self.fuse_eps = float(fuse_eps)\n",
        "        self.init_noise = float(init_noise)\n",
        "\n",
        "        # 1x1 projections\n",
        "        self.p3_in = ConvLNAct(in3, out, k=1, p=0)\n",
        "        self.p4_in = ConvLNAct(in4, out, k=1, p=0)\n",
        "        self.p5_in = ConvLNAct(in5, out, k=1, p=0)\n",
        "        self.p6_in = ConvLNAct(in6, out, k=1, p=0)\n",
        "\n",
        "        # DropPath schedule\n",
        "        top_dps = [0.0, float(dp_top_second)]\n",
        "        bot_dps = [0.0, float(dp_bot_second)]\n",
        "        self.top_convs = nn.ModuleList([\n",
        "            _FPNResBlock(out, drop_path=top_dps[i // 3], norm_factory=self.norm_factory)\n",
        "            for i in range(3 * self.repeats)\n",
        "        ])\n",
        "        self.bot_convs = nn.ModuleList([\n",
        "            _FPNResBlock(out, drop_path=bot_dps[i // 4], norm_factory=self.norm_factory)\n",
        "            for i in range(4 * self.repeats)\n",
        "        ])\n",
        "\n",
        "        # Fuse nodes\n",
        "        def _make_fuse(K: int):\n",
        "            from typing import Callable\n",
        "\n",
        "            return SpatialFuseNode(\n",
        "                                      K, channels=out, groups=self.fuse_groups,\n",
        "                                      tau_init=1.4, learnable_tau=True,\n",
        "                                      tau_bounds=(0.8, 1.8),\n",
        "                                      prenorm=\"rms\",\n",
        "                                      gate_floor=0.02,          # no death\n",
        "                                      uniform_init=True,\n",
        "                                      eps=5e-4,\n",
        "                                      save_last=True\n",
        "                                  )\n",
        "\n",
        "        self.fuse2_top = nn.ModuleList([_make_fuse(2) for _ in range(3 * self.repeats)])\n",
        "        self.fuse2_bot = nn.ModuleList([_make_fuse(2) for _ in range(3 * self.repeats)])\n",
        "        self.fuse3_bot = nn.ModuleList([_make_fuse(3) for _ in range(1 * self.repeats)])\n",
        "\n",
        "        # ---  3 up/edge blocks and shared between repeats ---\n",
        "        self.up_dw, self.up_na = nn.ModuleList(), nn.ModuleList()\n",
        "        self.edge_dw, self.edge_ls = nn.ModuleList(), nn.ModuleList()\n",
        "        for _ in range(3):  # only for 3 items\n",
        "            dw = nn.Conv2d(out, out, 3, 1, 1, groups=out, bias=False)\n",
        "            nn.init.kaiming_normal_(dw.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            self.up_dw.append(dw)\n",
        "\n",
        "            nf_layer = self.norm_factory(out)\n",
        "            if isinstance(nf_layer, LayerNormProxy):\n",
        "                from DAT.models.dat_blocks import LNAct as _LNAct\n",
        "                self.up_na.append(_LNAct(out))\n",
        "            else:\n",
        "                self.up_na.append(nn.Sequential(nf_layer, nn.SiLU(inplace=True)))\n",
        "\n",
        "            edw = nn.Conv2d(out, out, 3, 1, 1, groups=out, bias=False)\n",
        "            _init_laplacian_dw(edw)\n",
        "            self.edge_dw.append(edw)\n",
        "            self.edge_ls.append(LayerScale(out, init_values=edge_ls_init))\n",
        "\n",
        "        # High-level passthrough LS (p5, p4, p3)\n",
        "        self.keep_top = nn.ModuleList([LayerScale(out, init_values=0.10) for _ in range(3)])\n",
        "\n",
        "        # Input balancing\n",
        "        self.balance = RMSNorm2d(eps=1e-6) if self.prenorm == \"rms\" else nn.Identity()\n",
        "\n",
        "    # --- helpers ---\n",
        "    def _up_refine_shared(self, x: torch.Tensor, size_hw: tuple[int,int], step_id: int, rep_idx: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        step_id: [0,1,2] → corresponds to each top-down step, independent of repeat\n",
        "        \"\"\"\n",
        "        x = F.interpolate(x, size=size_hw, mode=\"bilinear\", align_corners=False)\n",
        "        if self.no_blur_first and rep_idx == 0:\n",
        "            # no-blur first repeat: only norm/act\n",
        "            x = self.up_na[step_id](x)\n",
        "        else:\n",
        "            x = self.up_dw[step_id](x)\n",
        "            if self.edge_enhance:\n",
        "                x = x + self.edge_ls[step_id](self.edge_dw[step_id](x))\n",
        "            x = self.up_na[step_id](x)\n",
        "        return x\n",
        "\n",
        "    def _bal(self, *xs):\n",
        "        return (tuple(self.balance(x) for x in xs) if self.prenorm != \"none\" else xs)\n",
        "\n",
        "    def forward(self, p3, p4, p5, p6):\n",
        "        # Proj\n",
        "        p3, p4, p5, p6 = self.p3_in(p3), self.p4_in(p4), self.p5_in(p5), self.p6_in(p6)\n",
        "\n",
        "        # Low-level grad-boost\n",
        "        if self.training and self.grad_boost_low > 0:\n",
        "            p3 = _ste_boost(p3, self.grad_boost_low)\n",
        "\n",
        "        tconv = bconv = f2t = f2b = f3b = 0\n",
        "\n",
        "        for rep in range(self.repeats):\n",
        "            # ---------- Top‑down ----------\n",
        "            # step_id = 0: p6→p5\n",
        "            u5 = self._up_refine_shared(p6, p5.shape[-2:], step_id=0, rep_idx=rep)\n",
        "            p5_b, u5_b = self._bal(p5, u5)\n",
        "            p5_td = self.fuse2_top[f2t](p5_b, u5_b); f2t += 1\n",
        "            p5_td = self.top_convs[tconv](p5_td); tconv += 1\n",
        "            p5_td = p5_td + self.keep_top[0](p5)\n",
        "\n",
        "            # step_id = 1: p5_td→p4\n",
        "            u4 = self._up_refine_shared(p5_td, p4.shape[-2:], step_id=1, rep_idx=rep)\n",
        "            p4_b, u4_b = self._bal(p4, u4)\n",
        "            p4_td = self.fuse2_top[f2t](p4_b, u4_b); f2t += 1\n",
        "            p4_td = self.top_convs[tconv](p4_td); tconv += 1\n",
        "            p4_td = p4_td + self.keep_top[1](p4)\n",
        "\n",
        "            # step_id = 2: p4_td→p3\n",
        "            u3 = self._up_refine_shared(p4_td, p3.shape[-2:], step_id=2, rep_idx=rep)\n",
        "            p3_b, u3_b = self._bal(p3, u3)\n",
        "            p3_td = self.fuse2_top[f2t](p3_b, u3_b); f2t += 1\n",
        "            p3_td = self.top_convs[tconv](p3_td); tconv += 1\n",
        "            p3_td = p3_td + self.keep_top[2](p3)\n",
        "\n",
        "            # ---------- Bottom‑up ----------\n",
        "            p3 = self.fuse2_bot[f2b](*self._bal(p3, p3_td)); f2b += 1\n",
        "            p3 = self.bot_convs[bconv](p3); bconv += 1\n",
        "\n",
        "            p4 = self.fuse2_bot[f2b](*self._bal(p4_td, F.max_pool2d(p3, 2))); f2b += 1\n",
        "            p4 = self.bot_convs[bconv](p4); bconv += 1\n",
        "\n",
        "            p5 = self.fuse3_bot[f3b](*self._bal(p5, F.max_pool2d(p4, 2), p5_td)); f3b += 1\n",
        "            p5 = self.bot_convs[bconv](p5); bconv += 1\n",
        "\n",
        "            p6 = self.fuse2_bot[f2b](*self._bal(p6, F.max_pool2d(p5, 2))); f2b += 1\n",
        "            p6 = self.bot_convs[bconv](p6); bconv += 1\n",
        "\n",
        "        return p3, p4, p5, p6\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Positional Encoding\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class Pos2d(nn.Module):\n",
        "    \"\"\"2D sinusoidal positional encoding\"\"\"\n",
        "    def __init__(self, c: int = 320, max_h: int = 640, max_w: int = 640):\n",
        "        super().__init__()\n",
        "        assert c % 4 == 0\n",
        "        self.cq = c // 4\n",
        "        pos = self._build(max_h, max_w, c)\n",
        "        self.register_buffer(\"pos_table\", pos, persistent=False)\n",
        "\n",
        "    def _build(self, H: int, W: int, C: int) -> torch.Tensor:\n",
        "        yv, xv = torch.meshgrid(\n",
        "            torch.linspace(0, 1, H), torch.linspace(0, 1, W), indexing=\"ij\"\n",
        "        )\n",
        "        div = torch.exp(torch.arange(0, self.cq) * (-math.log(10000.0) / self.cq))\n",
        "        pos_x = (xv[..., None] * div).reshape(H, W, -1)\n",
        "        pos_y = (yv[..., None] * div).reshape(H, W, -1)\n",
        "        pos = torch.cat([pos_y.sin(), pos_y.cos(), pos_x.sin(), pos_x.cos()], dim=2)\n",
        "        return pos.permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        _, _, H, W = x.shape\n",
        "        return self.pos_table[:, :, :H, :W]\n",
        "\n",
        "\n",
        "\n",
        "import inspect\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# DATBlock –  TransformerStage wrapper\n",
        "# -----------------------------------------------------------------------------\n",
        "class DATBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                channels: int,\n",
        "                depth: int = 2,\n",
        "                heads: int | None = None,\n",
        "                window_size: int = 8,\n",
        "                drop: float = 0.0):\n",
        "        super().__init__()\n",
        "        heads = heads or max(4, channels // 32)\n",
        "\n",
        "        cfg = dict(\n",
        "            fmap_size           = (window_size, window_size),\n",
        "            window_size         = window_size,\n",
        "            ns_per_pt           = 4,\n",
        "            dim_in              = channels,\n",
        "            dim_embed           = channels,\n",
        "            depths              = depth,               # INT\n",
        "            stage_spec          = 'N' * depth,\n",
        "            n_groups            = 1,\n",
        "            use_pe              = True,\n",
        "            sr_ratio            = 1,\n",
        "            heads               = heads,               # INT\n",
        "            heads_q             = [heads] * depth,     # LIST\n",
        "            stride              = 1,\n",
        "            offset_range_factor = 2,\n",
        "            dwc_pe              = True,\n",
        "            no_off              = False,\n",
        "            fixed_pe            = False,\n",
        "            attn_drop           = drop,\n",
        "            proj_drop           = drop,\n",
        "            expansion           = 4,\n",
        "            drop                = drop,\n",
        "            drop_path_rate      = [drop] * depth,      # LIST\n",
        "            use_dwc_mlp         = False,\n",
        "            ksize               = 3,\n",
        "            nat_ksize           = 7,\n",
        "            k_qna               = 8,\n",
        "            nq_qna              = 9,\n",
        "            qna_activation      = \"relu\",\n",
        "            layer_scale_value   = 0.3,\n",
        "            use_lpu             = False,\n",
        "            log_cpb             = True,\n",
        "        )\n",
        "\n",
        "        from DAT.models.dat import TransformerStage\n",
        "        self.stage = TransformerStage(**cfg)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x = (B,C,H,W) –\n",
        "        if hasattr(self.stage, 'fmap_size'):\n",
        "            self.stage.fmap_size = x.shape[-2:]\n",
        "        try:\n",
        "            return self.stage(x)\n",
        "        except TypeError:                           # some versions (x,H,W) require\n",
        "            H, W = x.shape[-2:]\n",
        "            return self.stage(x, H, W)\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Deformable Encoder\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class GLUFFN(nn.Module):\n",
        "    \"\"\"\n",
        "    GLU-based FFN:\n",
        "      - First linear: d_model -> 2*inner\n",
        "      - Gate: SwiGLU (SiLU) or GEGLU (GELU)\n",
        "      - Second linear: inner -> d_model\n",
        "\n",
        "    inner width:\n",
        "      • if ffn_dim is given: inner = ffn_dim // 2  (2*inner = ffn_dim)\n",
        "      • otherwise: inner = round(d_model * ffn_mult)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        ffn_dim: Optional[int] = None,\n",
        "        ffn_mult: float = 2.0,\n",
        "        act: str = \"swiglu\",\n",
        "        drop: float = 0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if ffn_dim is not None:\n",
        "            inner = max(32, int(ffn_dim) // 2)\n",
        "        else:\n",
        "            inner = max(32, int(round(d_model * float(ffn_mult))))\n",
        "        self.fc1 = nn.Linear(d_model, inner * 2, bias=True)\n",
        "        self.fc2 = nn.Linear(inner, d_model, bias=True)\n",
        "        self.drop = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "\n",
        "        act = act.lower()\n",
        "        if act not in (\"swiglu\", \"geglu\"):\n",
        "            raise ValueError(\"GLUFFN.act must be 'swiglu' or 'geglu'\")\n",
        "        self.act_kind = act\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight); nn.init.zeros_(self.fc1.bias)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight); nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        xh = self.fc1(x)\n",
        "        x_lin, x_gate = xh.chunk(2, dim=-1)\n",
        "        if self.act_kind == \"swiglu\":\n",
        "            gated = F.silu(x_gate) * x_lin\n",
        "        else:  # 'geglu'\n",
        "            gated = F.gelu(x_gate) * x_lin\n",
        "        return self.drop(self.fc2(gated))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DeformEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    MS-Deformable self-attention + GLU-FFN, Pre-Norm + LayerScale + DropPath.\n",
        "\n",
        "    Args (backward-compatible):\n",
        "        d_model, n_heads, n_levels, n_points\n",
        "        ffn_dim:   optional (if given, GLU inner width is ffn_dim//2)\n",
        "        ffn_mult:  used if ffn_dim is not specified (default 2.0)\n",
        "        ffn_act:   ‘swiglu’ (default) or ‘geglu’\n",
        "        drop, drop_path\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int = 320,\n",
        "        n_heads: int = 10,\n",
        "        n_levels: int = 4,\n",
        "        n_points: int = 4,\n",
        "        *,\n",
        "        ffn_dim: Optional[int] = None,\n",
        "        ffn_mult: float = 2.0,\n",
        "        ffn_act: str = \"swiglu\",\n",
        "        drop: float = 0.0,\n",
        "        drop_path: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn = MultiScaleDeformableAttention(\n",
        "            embed_dims=d_model, num_heads=n_heads,\n",
        "            num_levels=n_levels, num_points=n_points,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ls1   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop1 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp1   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        self.ffn   = GLUFFN(d_model, ffn_dim=ffn_dim, ffn_mult=ffn_mult, act=ffn_act, drop=drop)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ls2   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop2 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp2   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_encoder_reference_points(\n",
        "        spatial_shapes: torch.Tensor, B: int, device, dtype\n",
        "    ) -> torch.Tensor:\n",
        "        ref_list = []\n",
        "        for (H, W) in spatial_shapes.tolist():\n",
        "            ref_y, ref_x = torch.meshgrid(\n",
        "                torch.linspace(0.5, H - 0.5, H, device=device, dtype=dtype) / H,\n",
        "                torch.linspace(0.5, W - 0.5, W, device=device, dtype=dtype) / W,\n",
        "                indexing=\"ij\",\n",
        "            )\n",
        "            ref = torch.stack((ref_x, ref_y), dim=-1).reshape(-1, 2)  # (HW,2)\n",
        "            ref_list.append(ref)\n",
        "        return torch.cat(ref_list, dim=0)[None, :, None, :].repeat(B, 1, spatial_shapes.size(0), 1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.Tensor,             # (B, sumHW, C)\n",
        "        pos: torch.Tensor,             # (B, sumHW, C)\n",
        "        spatial_shapes: torch.Tensor,  # (L, 2)\n",
        "        lvl_start_idx: torch.Tensor,   # (L,)\n",
        "        key_padding_mask: Optional[torch.Tensor] = None,\n",
        "    ) -> torch.Tensor:\n",
        "\n",
        "        B, N, C = src.shape\n",
        "        ref_pts = self._make_encoder_reference_points(\n",
        "            spatial_shapes=spatial_shapes, B=B, device=src.device, dtype=src.dtype\n",
        "        )  # (B,N,L,2)\n",
        "\n",
        "        x = src\n",
        "        q = self.norm1(x)\n",
        "        attn_out = self.self_attn(\n",
        "            query=q, value=q,\n",
        "            reference_points=ref_pts,\n",
        "            spatial_shapes=spatial_shapes,\n",
        "            level_start_index=lvl_start_idx,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            query_pos=pos,\n",
        "        )\n",
        "        x = x + self.dp1(self.ls1(self.drop1(attn_out)))\n",
        "\n",
        "        y = self.ffn(self.norm2(x))\n",
        "        x = x + self.dp2(self.ls2(self.drop2(y)))\n",
        "        return x\n",
        "\n",
        "class TinyDeformEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Two layers are recommended (lightweight and effective).\n",
        "    get_drop_path_rates() is available;\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_layers: int = 1,\n",
        "                 d_model: int = 320,\n",
        "                 n_heads: int = 10,\n",
        "                 n_levels: int = 4,\n",
        "                 n_points: int = 4,\n",
        "                 ffn_dim: int = 1024,\n",
        "                 drop: float = 0.0,\n",
        "                 drop_path_max: float = 0.1):\n",
        "        super().__init__()\n",
        "        dpr = get_drop_path_rates(num_layers, drop_path_max)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DeformEncoderLayer(d_model=d_model,\n",
        "                               n_heads=n_heads,\n",
        "                               n_levels=n_levels,\n",
        "                               n_points=n_points,\n",
        "                               ffn_dim=ffn_dim,\n",
        "                               drop=drop,\n",
        "                               drop_path=dpr[i])\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self,\n",
        "                src: torch.Tensor,             # (B, sumHW, C)\n",
        "                pos: torch.Tensor,             # (B, sumHW, C)\n",
        "                spatial_shapes: torch.Tensor,  # (L,2)\n",
        "                lvl_start_idx: torch.Tensor,   # (L,)\n",
        "                key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        for ly in self.layers:\n",
        "            src = ly(src, pos, spatial_shapes, lvl_start_idx, key_padding_mask)\n",
        "        return src\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# RT-Deform Decoder (Fixed IoU-aware padding)\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class DeformDecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder:\n",
        "      Pre-Norm + LayerScale + DropPath + GLU-FFN + tanh-bounded ref update.\n",
        "\n",
        "    Args (backward-compatible):\n",
        "        d_model, n_heads, n_levels, n_points\n",
        "        ffn_dim:   optional (for GLU inner width, 2*inner = ffn_dim)\n",
        "        ffn_mult:  used if ffn_dim is not specified\n",
        "        ffn_act:   ‘swiglu’ | ‘geglu’\n",
        "        drop, drop_path\n",
        "        refine_scale: float or (lo,hi)  — you can provide layer_id/num_layers for the schedule\n",
        "        grad_eps: very small nudge\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int = 320,\n",
        "        n_heads: int = 10,\n",
        "        n_levels: int = 4,\n",
        "        n_points: int = 4,\n",
        "        *,\n",
        "        ffn_dim: Optional[int] = None,\n",
        "        ffn_mult: float = 2.0,\n",
        "        ffn_act: str = \"swiglu\",\n",
        "        drop: float = 0.0,\n",
        "        drop_path: float = 0.1,\n",
        "        refine_scale: Union[float, Tuple[float, float]] = 0.5,\n",
        "        layer_id: Optional[int] = None,\n",
        "        num_layers: Optional[int] = None,\n",
        "        grad_eps: float = 1e-4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.grad_eps = float(grad_eps)\n",
        "\n",
        "        # refine scale\n",
        "        if isinstance(refine_scale, (tuple, list)):\n",
        "            lo, hi = float(refine_scale[0]), float(refine_scale[1])\n",
        "            if (num_layers is not None) and (layer_id is not None) and (num_layers > 1):\n",
        "                t = float(layer_id) / float(num_layers - 1)\n",
        "                self.refine_scale = lo + (hi - lo) * t\n",
        "            else:\n",
        "                self.refine_scale = 0.5 * (lo + hi)\n",
        "        else:\n",
        "            self.refine_scale = float(refine_scale)\n",
        "\n",
        "        # Self-Attn (MHA) + Pre-Norm\n",
        "        self.self_attn = nn.MultiheadAttention(\n",
        "            embed_dim=d_model, num_heads=n_heads, batch_first=True\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ls1   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop1 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp1   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # Cross-Attn (MS-Deformable) + Pre-Norm\n",
        "        self.cross_attn = MultiScaleDeformableAttention(\n",
        "            embed_dims=d_model, num_heads=n_heads,\n",
        "            num_levels=n_levels, num_points=n_points,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ls2   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop2 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp2   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # FFN (GLU) + Pre-Norm\n",
        "        self.ffn   = GLUFFN(d_model, ffn_dim=ffn_dim, ffn_mult=ffn_mult, act=ffn_act, drop=drop)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.ls3   = LayerScale(d_model, init_values=0.4)\n",
        "        self.drop3 = nn.Dropout(drop) if drop > 0 else nn.Identity()\n",
        "        self.dp3   = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
        "\n",
        "        # Ref delta head (tanh clamp)\n",
        "        self.ref_mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model, 2),\n",
        "        )\n",
        "        for m in self.ref_mlp:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
        "\n",
        "    @staticmethod\n",
        "    def _inv_sigmoid(x: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
        "        x = x.clamp(eps, 1.0 - eps)\n",
        "        return torch.log(x) - torch.log(1.0 - x)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        tgt: torch.Tensor,                 # (B, N, C)\n",
        "        ref_pts: torch.Tensor,             # (B, N, L, 2)\n",
        "        src: torch.Tensor,                 # (B, S, C)\n",
        "        query_pos: torch.Tensor,           # (B, N, C)\n",
        "        spatial_shapes: torch.Tensor,      # (L, 2)\n",
        "        lvl_start_idx: torch.Tensor,       # (L,)\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        x = tgt\n",
        "        q = self.norm1(x)\n",
        "        sa_out = self.self_attn(q, q, q, need_weights=False)[0]\n",
        "        x = x + self.dp1(self.ls1(self.drop1(sa_out)))\n",
        "\n",
        "        q = self.norm2(x)\n",
        "        ca_out = self.cross_attn(\n",
        "            query=q, value=src,\n",
        "            reference_points=ref_pts,\n",
        "            spatial_shapes=spatial_shapes,\n",
        "            level_start_index=lvl_start_idx,\n",
        "            query_pos=query_pos,\n",
        "        )\n",
        "        x = x + self.dp2(self.ls2(self.drop2(ca_out)))\n",
        "\n",
        "        y = self.ffn(self.norm3(x))\n",
        "        x = x + self.dp3(self.ls3(self.drop3(y)))\n",
        "\n",
        "        # tanh-bounded iterative ref update\n",
        "        delta = torch.tanh(self.ref_mlp(x)) * self.refine_scale      # (B,N,2)\n",
        "        new_ref = torch.sigmoid(self._inv_sigmoid(ref_pts) + delta.unsqueeze(2).to(ref_pts.dtype))\n",
        "\n",
        "        # tiny nudge\n",
        "        x = x + self.grad_eps * delta.mean(dim=2, keepdim=True)\n",
        "        return x, new_ref\n",
        "\n",
        "\n",
        "class HeadPrep(nn.Module):\n",
        "    def __init__(self, d_model: int, ls_init: float = 0.80, ln_eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(d_model, eps=ln_eps)\n",
        "        self.ls   = LayerScale(d_model, init_values=ls_init)\n",
        "    def forward(self, x):  # x: (B, N, C)\n",
        "        return self.ls(self.norm(x))\n",
        "\n",
        "class RTDeformDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    ABLATION — RT‑Deform Decoder (No IoU head, No Denoising)\n",
        "\n",
        "\n",
        "    * Denoising (DN) queries and the associated label/box noise mechanism have been removed.\n",
        "    * The interface signature is preserved; the dn_queries / use_iou_aware parameters are ignored for ablation.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_obj_classes: int = 20,\n",
        "                 include_background: bool = True,\n",
        "                 *,\n",
        "                 num_queries: int = 300,\n",
        "                 num_layers: int = 4,\n",
        "                 d_model: int = 320,\n",
        "                 attn_n_heads: int = 10,\n",
        "                 dn_queries: int = 0,                 # ignored\n",
        "                 use_iou_aware: bool = False,         # ignored\n",
        "                 # Encoder\n",
        "                 use_encoder: bool = True,\n",
        "                 encoder_layers: int = 1,\n",
        "                 encoder_drop_path_max: float = 0.1,\n",
        "                 # (The following are retained for interface compatibility, not used)\n",
        "                 iou_k_ratio: float = 0.75,\n",
        "                 mqs_enable: bool = True,             # seed/MQS mechanism is not present here\n",
        "                 mqs_obj_ratio: float = 0.30,\n",
        "                 mqs_grid_ratio: float = 0.20,\n",
        "                 mqs_levels: Tuple[int, ...] = (1, 2, 3),\n",
        "                 mqs_local_max_kernel: int = 3,\n",
        "                 mqs_train_only: bool = False,\n",
        "                 seed_enable: bool = True,\n",
        "                 seed_alpha_obj_init: float = 0.55,\n",
        "                 seed_alpha_grid_init: float = 0.4,\n",
        "                 seed_mlp_expansion: float = 1.0,\n",
        "                 use_proposals: bool = True,\n",
        "                 proposal_topk: Optional[int] = None,\n",
        "                 proposal_ratio: float = 0.70,\n",
        "                 min_mqs: int = 60,\n",
        "                 min_left_queries: int = 16,\n",
        "                 emit_obj_logits_in_eval: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        # Number of classes\n",
        "        self.num_obj_classes = num_obj_classes\n",
        "        self.include_background = include_background\n",
        "        self.num_pred = num_obj_classes + int(include_background)\n",
        "\n",
        "        self.num_queries = int(num_queries)\n",
        "        self.num_layers  = int(num_layers)\n",
        "        self.d_model     = int(d_model)\n",
        "        self.use_encoder = bool(use_encoder)\n",
        "\n",
        "        # Position and level embed\n",
        "        self.query_pos  = nn.Embedding(self.num_queries, d_model)  # learned positional (N,C)\n",
        "        self.query_feat = nn.Embedding(self.num_queries, d_model)  # learned content   (N,C)\n",
        "        self.pos_embed   = Pos2d(d_model)\n",
        "        self.level_embed = nn.Parameter(torch.randn(4, d_model))\n",
        "\n",
        "        # Encoder (optional)\n",
        "        if self.use_encoder:\n",
        "            self.encoder = TinyDeformEncoder(\n",
        "                num_layers=encoder_layers,\n",
        "                d_model=d_model, n_heads=attn_n_heads,\n",
        "                n_levels=4, n_points=4,\n",
        "                ffn_dim=d_model * 4,\n",
        "                drop=0.0,\n",
        "                drop_path_max=encoder_drop_path_max\n",
        "            )\n",
        "\n",
        "        # Decoder\n",
        "        n_points_list = [4] * (num_layers - 2) + [2, 2] if num_layers >= 2 else [4]\n",
        "        self.layers = nn.ModuleList([\n",
        "            DeformDecoderLayer(d_model=d_model,\n",
        "                               n_heads=attn_n_heads,\n",
        "                               n_levels=4,\n",
        "                               n_points=n_points_list[i],\n",
        "                               ffn_dim=d_model * 4,\n",
        "                               refine_scale=(0.15, 0.45),\n",
        "                               layer_id=i, num_layers=num_layers)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Head prep + classification / box heads\n",
        "        self.head_prep = HeadPrep(d_model, ls_init=0.80, ln_eps=1e-6)\n",
        "        self.cls_head  = nn.Linear(d_model, self.num_pred)\n",
        "        self.box_head  = nn.Linear(d_model, 4)\n",
        "\n",
        "        # Aux head'ler (except last layer)\n",
        "        if self.num_layers > 1:\n",
        "            self.aux_cls_heads = nn.ModuleList(nn.Linear(d_model, self.num_pred) for _ in range(self.num_layers - 1))\n",
        "            self.aux_box_heads = nn.ModuleList(nn.Linear(d_model, 4)            for _ in range(self.num_layers - 1))\n",
        "        else:\n",
        "            self.aux_cls_heads = nn.ModuleList([])\n",
        "            self.aux_box_heads = nn.ModuleList([])\n",
        "\n",
        "        # Small MLP for initial reference\n",
        "        self.ref_init = nn.Linear(d_model, 2)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    # ------------------- init helpers -------------------\n",
        "    def _init_weights(self):\n",
        "        nn.init.normal_(self.query_pos.weight,  std=0.02)\n",
        "        nn.init.normal_(self.query_feat.weight, std=0.02)\n",
        "        nn.init.normal_(self.level_embed,       std=0.02)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.ref_init.weight); nn.init.zeros_(self.ref_init.bias)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.box_head.weight, gain=1.0); nn.init.zeros_(self.box_head.bias)\n",
        "        nn.init.xavier_uniform_(self.cls_head.weight, gain=1.0)\n",
        "\n",
        "        for c, b in zip(self.aux_cls_heads, self.aux_box_heads):\n",
        "            nn.init.xavier_uniform_(c.weight, gain=1.0)\n",
        "            nn.init.xavier_uniform_(b.weight, gain=1.0); nn.init.zeros_(b.bias)\n",
        "\n",
        "        self.reset_class_bias()\n",
        "\n",
        "    def reset_class_bias(self, object_prior: float = .2, no_object_bias: float = -2.):\n",
        "        obj_bias = -math.log((1. - object_prior) / object_prior)\n",
        "        with torch.no_grad():\n",
        "            self.cls_head.bias[:] = 0.\n",
        "            self.cls_head.bias[: self.num_obj_classes] = obj_bias\n",
        "            if self.include_background:\n",
        "                self.cls_head.bias[-1] = no_object_bias\n",
        "            for aux in self.aux_cls_heads:\n",
        "                aux.bias[:] = self.cls_head.bias\n",
        "\n",
        "    # ------------------- forward -------------------\n",
        "    def forward(self,\n",
        "                p3: torch.Tensor, p4: torch.Tensor,\n",
        "                p5: torch.Tensor, p6: torch.Tensor,\n",
        "                targets: Optional[List[Dict]] = None\n",
        "                ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        B = p3.size(0)\n",
        "        device = p3.device\n",
        "        feats = [p6, p5, p4, p3]  # from largest to smallest (B, C, H, W)\n",
        "        L = len(feats)\n",
        "\n",
        "        # Per‑level src & pos (B, ΣHW, C)\n",
        "        src_list, pos_list = [], []\n",
        "        spatial_shapes = torch.zeros(L, 2, dtype=torch.long, device=device)  # (L,2) = (H,W)\n",
        "\n",
        "        for i, f in enumerate(feats):\n",
        "            B_, C, H, W = f.shape\n",
        "            pos_lvl = self.pos_embed(f).expand(B_, -1, -1, -1) + self.level_embed[i].view(1, -1, 1, 1)\n",
        "            src_list.append(f.flatten(2).transpose(1, 2))              # (B, HW, C)\n",
        "            pos_list.append(pos_lvl.flatten(2).transpose(1, 2))         # (B, HW, C)\n",
        "            spatial_shapes[i, 0] = H\n",
        "            spatial_shapes[i, 1] = W\n",
        "\n",
        "        src = torch.cat(src_list, dim=1)                         # (B, ΣHW, C)\n",
        "        pos = torch.cat(pos_list, dim=1).to(dtype=src.dtype)     # (B, ΣHW, C)\n",
        "\n",
        "        # level start index: (L,)\n",
        "        numel_per_level = spatial_shapes[:, 0] * spatial_shapes[:, 1]\n",
        "        lvl_start_idx = torch.cat(\n",
        "            [numel_per_level.new_zeros(1), numel_per_level.cumsum(0)[:-1]],\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        # Encoder (optional)\n",
        "        memory = self.encoder(src, pos, spatial_shapes, lvl_start_idx) if self.use_encoder else src\n",
        "        if pos.size(0) != memory.size(0):\n",
        "            pos = pos.expand(memory.size(0), -1, -1)\n",
        "        pos = pos.to(dtype=memory.dtype)\n",
        "\n",
        "        # Learned queries\n",
        "        content = self.query_feat.weight.unsqueeze(0).expand(B, -1, -1)  # (B,N,C)\n",
        "        qpos    = self.query_pos.weight.unsqueeze(0).expand(B, -1, -1)   # (B,N,C)\n",
        "        init_ref = torch.sigmoid(self.ref_init(qpos))                    # (B,N,2)\n",
        "        ref_pts  = init_ref.unsqueeze(2).expand(-1, -1, 4, -1)           # (B,N,4,2)\n",
        "\n",
        "        tgt = content\n",
        "        aux_out = []\n",
        "\n",
        "        # Decoder\n",
        "        for lid, layer in enumerate(self.layers):\n",
        "            tgt, ref_pts = layer(\n",
        "                tgt=tgt, ref_pts=ref_pts, src=memory,\n",
        "                query_pos=qpos, spatial_shapes=spatial_shapes, lvl_start_idx=lvl_start_idx\n",
        "            )\n",
        "\n",
        "            if self.training and lid < self.num_layers - 1:\n",
        "                h_aux = self.head_prep(tgt)  # LN + LayerScale\n",
        "                aux_logits = self.aux_cls_heads[lid](h_aux)\n",
        "                aux_boxes  = self.aux_box_heads[lid](h_aux).sigmoid()\n",
        "                aux_out.append({\"pred_logits\": aux_logits, \"pred_boxes\": aux_boxes})\n",
        "\n",
        "        # Final heads\n",
        "        h = self.head_prep(tgt)\n",
        "        logits = self.cls_head(h)\n",
        "        boxes  = self.box_head(h).sigmoid()\n",
        "\n",
        "        out: Dict[str, torch.Tensor] = {\"pred_logits\": logits, \"pred_boxes\": boxes,\n",
        "                                        \"final_ref_pts\": ref_pts.contiguous()}\n",
        "        if aux_out and self.training:\n",
        "            out[\"aux_outputs\"] = aux_out\n",
        "        return out\n",
        "# -----------------------------------------------------------------------------\n",
        "# MiniBackbone\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class MiniBackbone(nn.Module):\n",
        "    \"\"\"Lightweight version with CSI and PGI\"\"\"\n",
        "    def __init__(self, depths: List[int] = [6, 6, 12, 6],\n",
        "                drop_path_max: float = 0.2,\n",
        "                num_classes: int = 20):\n",
        "        super().__init__()\n",
        "        self.backbone = StageAwareBackbone(depths, drop_path_max, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, need_aux: bool = False):\n",
        "        # Match the return format of StageAwareBackbone\n",
        "        return self.backbone(x, need_aux)\n",
        "\n",
        "\n",
        "\n",
        "def _looks_like_norm_name(name: str) -> bool:\n",
        "    low = name.lower()\n",
        "\n",
        "    return any(k in low for k in [\n",
        "        \".norm\", \"bn\", \"groupnorm\", \"layernorm\", \"gn\", \"ln.\", \"ln_\", \"lnact\", \"ln_act\", \"lnproxy\", \"layernormproxy\", \"rmsnorm\"\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "class HybridDCDATRT(nn.Module):\n",
        "    \"\"\"\n",
        "    End‑to‑End Hybrid‑DCDAT‑RT detector (DAT + DCNv4 backbone + RT‑Deform Decoder)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_classes:   int = 20,\n",
        "                 num_queries:   int = 100,\n",
        "                 depths:        List[int] = (2, 2, 2, 2),\n",
        "                 drop_path_max: float = 0.0,\n",
        "                 backbone_norm_factory: 'NormFactory' = None,\n",
        "                 neck_norm_factory:     'NormFactory' = None,\n",
        "                 use_layer_scale: bool = True,\n",
        "                 layer_scale_init: float = 1.0,\n",
        "                 # Decoder\n",
        "                 d_model:    int = 320,\n",
        "                 dec_layers: int = 4,\n",
        "                 dn_queries: int = 100,\n",
        "                 # Features\n",
        "                 use_aux_loss: bool = True,\n",
        "                 use_iou_aware: bool = False,\n",
        "                 # LR multipliers\n",
        "                 backbone_lr: float = 0.1,\n",
        "                 head_lr:     float = 1.0,\n",
        "                 # Dataset\n",
        "                 voc_prior: bool = False,\n",
        "                 # Decoder Encoder kontrolü\n",
        "                 decoder_use_encoder: bool = True,\n",
        "                 decoder_encoder_layers: int = 2,\n",
        "                 decoder_encoder_drop_path_max: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # default NormFactory\n",
        "        backbone_norm_factory = backbone_norm_factory or NormFactory(\"gn\")\n",
        "        neck_norm_factory     = neck_norm_factory     or NormFactory(\"gn\")\n",
        "\n",
        "        self.num_classes   = num_classes\n",
        "        self.use_aux_loss  = use_aux_loss\n",
        "\n",
        "        # ---------- Backbone (DAT + CSI + PGI) ----------\n",
        "        self.backbone = StageAwareBackbone(\n",
        "            depths          = depths,\n",
        "            drop_path_max   = drop_path_max,\n",
        "            num_classes     = num_classes,\n",
        "            voc_prior       = voc_prior,\n",
        "            norm_factory    = backbone_norm_factory,\n",
        "            use_layer_scale = use_layer_scale,\n",
        "            layer_scale_init= layer_scale_init\n",
        "        )\n",
        "\n",
        "        # ---------- Neck (Light‑BiFPN) ----------\n",
        "\n",
        "        self.neck = LightBiFPN(\n",
        "          in_channels=(128,256,640,768),\n",
        "          out=d_model,\n",
        "          repeats=2,\n",
        "          use_spatial_fuse=True,\n",
        "          fuse_groups=4,\n",
        "          fuse_tau_init=1.4,\n",
        "          fuse_learn_tau=True,\n",
        "          fuse_eps=5e-4,\n",
        "          grad_boost_low=0.1,     # << 0.5 → 0.3\n",
        "          dp_top_second=0.01, dp_bot_second=0.01,\n",
        "          no_blur_first=True,\n",
        "          edge_enhance=True,\n",
        "          edge_ls_init=0.03,\n",
        "          norm_factory=neck_norm_factory,\n",
        "      )\n",
        "\n",
        "        # ---------- Decoder (RT‑Deform) ----------\n",
        "        self.decoder = RTDeformDecoder(\n",
        "            num_obj_classes = num_classes,\n",
        "            include_background = True,\n",
        "            num_queries     = num_queries,\n",
        "            num_layers      = dec_layers,\n",
        "            d_model         = d_model,\n",
        "            dn_queries      = dn_queries,\n",
        "            use_iou_aware   = use_iou_aware,\n",
        "            use_encoder                 = decoder_use_encoder,\n",
        "            encoder_layers              = decoder_encoder_layers,\n",
        "            encoder_drop_path_max       = decoder_encoder_drop_path_max,\n",
        "        )\n",
        "\n",
        "        # (optional)\n",
        "        self._lr_mult = {\n",
        "            \"backbone\": backbone_lr,\n",
        "            \"head\":     head_lr,\n",
        "            \"bias\":     2.0,\n",
        "            \"dcn_bias\": 10.0,\n",
        "        }\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                targets: Optional[List[Dict]] = None) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        The output dictionary is compatible with RTDeformDecoder outputs:\n",
        "          - ‘pred_logits’, ‘pred_boxes’, optional ‘pred_ious’\n",
        "          - in training: ‘aux_outputs’, ‘dn_meta’, ‘query_selection_mask’\n",
        "          - optional: ‘aux_dense’ (auxiliary dense headers from the backbone)\n",
        "        \"\"\"\n",
        "        if not x.is_cuda:\n",
        "            raise RuntimeError(\"HybridDCDATRT expects CUDA tensor input.\")\n",
        "\n",
        "        # Backbone\n",
        "        if self.training and self.use_aux_loss:\n",
        "            (p3, p4, p5, p6), aux_dense = self.backbone(x, need_aux=True)\n",
        "        else:\n",
        "            p3, p4, p5, p6 = self.backbone(x, need_aux=False)\n",
        "            aux_dense      = None\n",
        "\n",
        "        # Neck\n",
        "        p3, p4, p5, p6 = self.neck(p3, p4, p5, p6)\n",
        "\n",
        "        # Decoder\n",
        "        if self.training and targets is not None:\n",
        "            dec_out = self.decoder(p3, p4, p5, p6, targets)\n",
        "        else:\n",
        "            dec_out = self.decoder(p3, p4, p5, p6)\n",
        "\n",
        "        if aux_dense is not None:\n",
        "            dec_out[\"aux_dense\"] = aux_dense\n",
        "        return dec_out\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "\n",
        "    def param_groups(self,\n",
        "                    base_lr: float = 1e-4,\n",
        "                    *,\n",
        "                    weight_decay: float = 0.02,\n",
        "                    bb_mult: float = 0.5,\n",
        "                    dec_mult: float = 2.0,\n",
        "                    bias_mult: float = 1.0,      # Bias boost\n",
        "                    dcn_mult: float = 1.5,       # DCN offset/mask boost\n",
        "                    rpb_mult: float = 5.0,       # RPB boost\n",
        "                    gate_mult: float = 1.0,      # Gate params boost\n",
        "                    ls_mult: float = 0.3,        # LayerScale reduction\n",
        "                    pos_mult: float = 1.5):\n",
        "        \"\"\"\n",
        "        parameter grouping with optimized LR scheduling.\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        buckets = {\n",
        "            # Backbone\n",
        "            \"bb_w\": [], \"bb_b\": [], \"bb_norm\": [],\n",
        "            \"dcn_off\": [],  # DCN offset/mask\n",
        "\n",
        "            # Neck / Head\n",
        "            \"hd_w\": [], \"hd_b\": [], \"hd_norm\": [],\n",
        "            \"fuse_gate\": [],  # Fusion gates\n",
        "\n",
        "            # Decoder\n",
        "            \"dec_w\": [], \"dec_b\": [], \"dec_norm\": [],\n",
        "            \"deform_off_b\": [],  # Deformable attention bias\n",
        "\n",
        "            # Special\n",
        "            \"pos_embed\": [], \"rpb_p\": [], \"ls_gamma\": [],\n",
        "            \"dat_scale_p\": [], \"scalars_gain\": [],\n",
        "        }\n",
        "\n",
        "        def is_norm_name(n: str) -> bool:\n",
        "            return any(x in n for x in [\".norm.\", \".bn.\", \".ln.\", \".gn.\"])\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            if not param.requires_grad:\n",
        "                continue\n",
        "\n",
        "            # --- Special cases (prefix-independent) ---\n",
        "\n",
        "            # RPB tables (needs high LR)\n",
        "            if name.endswith(\".rpb\") or \"relative_position_bias\" in name:\n",
        "                buckets[\"rpb_p\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Positional embeddings\n",
        "            if re.search(r\"(level_embed(_seed)?|query_pos|query_feat|label_enc|pos_table)\", name):\n",
        "                buckets[\"pos_embed\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # LayerScale gamma\n",
        "            if re.search(r\"(layer_scales\\.\\d+\\.weight|\\.scale\\.weight$|\\.ls\\.weight$)\", name):\n",
        "                buckets[\"ls_gamma\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Scalar gates\n",
        "            if any(name.endswith(x) for x in [\"residual_scale\", \"_lambda_logit\", \"dc_res_logit\", \"seed_logit_obj\", \"seed_logit_grid\"]):\n",
        "                buckets[\"scalars_gain\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # DAT scale parameters\n",
        "            if \"dat_logit\" in name or \"dat_log_tau\" in name:\n",
        "                buckets[\"dat_scale_p\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Neck fusion gates\n",
        "            if \"neck.fuse\" in name and \"log_tau\" in name:\n",
        "                buckets[\"fuse_gate\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Deformable attention sampling bias\n",
        "            if \"sampling_offsets.bias\" in name:\n",
        "                buckets[\"deform_off_b\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # --- Module-based categorization ---\n",
        "\n",
        "            # Backbone\n",
        "            if name.startswith(\"backbone.\"):\n",
        "                # DCN offset/mask (special treatment)\n",
        "                if \"offset_mask\" in name and \"dcn\" in name:\n",
        "                    buckets[\"dcn_off\"].append(param)\n",
        "                elif name.endswith(\".bias\") and not is_norm_name(name):\n",
        "                    buckets[\"bb_b\"].append(param)\n",
        "                elif is_norm_name(name):\n",
        "                    buckets[\"bb_norm\"].append(param)\n",
        "                else:\n",
        "                    buckets[\"bb_w\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Neck\n",
        "            if name.startswith(\"neck.\"):\n",
        "                if name.endswith(\".bias\") and not is_norm_name(name):\n",
        "                    buckets[\"hd_b\"].append(param)\n",
        "                elif is_norm_name(name):\n",
        "                    buckets[\"hd_norm\"].append(param)\n",
        "                else:\n",
        "                    buckets[\"hd_w\"].append(param)\n",
        "                continue\n",
        "\n",
        "            # Decoder (default)\n",
        "            if name.endswith(\".bias\") and not is_norm_name(name):\n",
        "                buckets[\"dec_b\"].append(param)\n",
        "            elif is_norm_name(name):\n",
        "                buckets[\"dec_norm\"].append(param)\n",
        "            else:\n",
        "                buckets[\"dec_w\"].append(param)\n",
        "\n",
        "        # --- Build parameter groups ---\n",
        "        wd = weight_decay\n",
        "        groups = []\n",
        "\n",
        "        # Backbone groups\n",
        "        if buckets[\"bb_w\"]:\n",
        "            groups.append({\"params\": buckets[\"bb_w\"], \"lr\": base_lr*bb_mult, \"weight_decay\": wd, \"name\": \"bb_w\"})\n",
        "        if buckets[\"bb_b\"]:\n",
        "            groups.append({\"params\": buckets[\"bb_b\"], \"lr\": base_lr*bb_mult*bias_mult, \"weight_decay\": 0.0, \"name\": \"bb_b\"})\n",
        "        if buckets[\"bb_norm\"]:\n",
        "            groups.append({\"params\": buckets[\"bb_norm\"], \"lr\": base_lr*bb_mult, \"weight_decay\": 0.0, \"name\": \"bb_norm\"})\n",
        "\n",
        "        # DCN offset/mask (HIGH LR!)\n",
        "        if buckets[\"dcn_off\"]:\n",
        "            groups.append({\"params\": buckets[\"dcn_off\"], \"lr\": base_lr*dcn_mult, \"weight_decay\": 0.0, \"name\": \"dcn_off\"})\n",
        "\n",
        "        # Neck/Head groups\n",
        "        if buckets[\"hd_w\"]:\n",
        "            groups.append({\"params\": buckets[\"hd_w\"], \"lr\": base_lr, \"weight_decay\": wd, \"name\": \"hd_w\"})\n",
        "        if buckets[\"hd_b\"]:\n",
        "            groups.append({\"params\": buckets[\"hd_b\"], \"lr\": base_lr*bias_mult, \"weight_decay\": 0.0, \"name\": \"hd_b\"})\n",
        "        if buckets[\"hd_norm\"]:\n",
        "            groups.append({\"params\": buckets[\"hd_norm\"], \"lr\": base_lr, \"weight_decay\": 0.0, \"name\": \"hd_norm\"})\n",
        "\n",
        "        # Decoder groups\n",
        "        if buckets[\"dec_w\"]:\n",
        "            groups.append({\"params\": buckets[\"dec_w\"], \"lr\": base_lr*dec_mult, \"weight_decay\": wd, \"name\": \"dec_w\"})\n",
        "        if buckets[\"dec_b\"]:\n",
        "            groups.append({\"params\": buckets[\"dec_b\"], \"lr\": base_lr*dec_mult*bias_mult, \"weight_decay\": 0.0, \"name\": \"dec_b\"})\n",
        "        if buckets[\"dec_norm\"]:\n",
        "            groups.append({\"params\": buckets[\"dec_norm\"], \"lr\": base_lr*dec_mult, \"weight_decay\": 0.0, \"name\": \"dec_norm\"})\n",
        "\n",
        "        # Special parameters with custom LR\n",
        "        if buckets[\"deform_off_b\"]:\n",
        "            groups.append({\"params\": buckets[\"deform_off_b\"], \"lr\": base_lr*dcn_mult, \"weight_decay\": 0.0, \"name\": \"deform_off_b\"})\n",
        "        if buckets[\"fuse_gate\"]:\n",
        "            groups.append({\"params\": buckets[\"fuse_gate\"], \"lr\": base_lr*gate_mult, \"weight_decay\": 0.0, \"name\": \"fuse_gate\"})\n",
        "        if buckets[\"rpb_p\"]:\n",
        "            groups.append({\"params\": buckets[\"rpb_p\"], \"lr\": base_lr*rpb_mult, \"weight_decay\": 0.0, \"name\": \"rpb_p\"})\n",
        "        if buckets[\"pos_embed\"]:\n",
        "            groups.append({\"params\": buckets[\"pos_embed\"], \"lr\": base_lr*pos_mult, \"weight_decay\": 0.0, \"name\": \"pos_embed\"})\n",
        "        if buckets[\"ls_gamma\"]:\n",
        "            groups.append({\"params\": buckets[\"ls_gamma\"], \"lr\": base_lr*ls_mult, \"weight_decay\": 0.0, \"name\": \"ls_gamma\"})\n",
        "        if buckets[\"dat_scale_p\"]:\n",
        "            groups.append({\"params\": buckets[\"dat_scale_p\"], \"lr\": base_lr*gate_mult, \"weight_decay\": 0.0, \"name\": \"dat_scale_p\"})\n",
        "        if buckets[\"scalars_gain\"]:\n",
        "            groups.append({\"params\": buckets[\"scalars_gain\"], \"lr\": base_lr*gate_mult, \"weight_decay\": 0.0, \"name\": \"scalars_gain\"})\n",
        "\n",
        "        # Validation\n",
        "        in_groups = {id(p) for g in groups for p in g[\"params\"]}\n",
        "        missing = [n for n,p in self.named_parameters() if p.requires_grad and id(p) not in in_groups]\n",
        "        assert not missing, f\"Missing params in groups: {len(missing)} params (e.g., {missing[:5]})\"\n",
        "\n",
        "        # Debug logging (optional)\n",
        "        if not hasattr(self, '_param_groups_logged'):\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"Parameter Groups Summary\")\n",
        "            print(\"=\"*60)\n",
        "            for g in groups:\n",
        "                n_params = sum(p.numel() for p in g[\"params\"])\n",
        "                if n_params > 0:\n",
        "                    lr_mult = g[\"lr\"] / base_lr\n",
        "                    print(f\"{g['name']:20s}: {n_params:10,} params | LR: {lr_mult:6.1f}x | WD: {g['weight_decay']:.3f}\")\n",
        "            print(\"=\"*60 + \"\\n\")\n",
        "            self._param_groups_logged = True\n",
        "\n",
        "        return groups\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# build_optimizer – returns only Optimizer (scheduler opsiyonel)\n",
        "# -----------------------------------------------------------------------------\n",
        "def build_optimizer(model: nn.Module,\n",
        "                    base_lr: float = 2e-4,\n",
        "                    weight_decay: float = 0.02,\n",
        "                    *,\n",
        "                    return_scheduler: bool = False):\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # ---------- 1) LAZY-PARAM WARM-UP (only backbone) ----------\n",
        "    was_training = model.training\n",
        "    model.train()  # PGI routes should be in train mode\n",
        "    with torch.no_grad():\n",
        "        for s in (640, 320):\n",
        "            dummy = torch.zeros(1, 3, s, s, device=device)\n",
        "            _ = model.backbone(dummy, need_aux=True)\n",
        "    model.train(was_training)\n",
        "\n",
        "    # ---------- 2) PARAM GRUPLARI ----------\n",
        "    groups = model.param_groups(base_lr)\n",
        "\n",
        "    # Norm weights WD=0 (GN/LNProxy vb.)\n",
        "    ln_keys = ('.norm', '.ln_act', 'ln_proxy')\n",
        "    #  Note: This check is heuristic by name; it can be customized as needed.\n",
        "    for g in groups:\n",
        "        params_in_group = set(map(id, g['params']))\n",
        "        if any(any(k in n for k in ln_keys)\n",
        "               for n, p in model.named_parameters() if id(p) in params_in_group):\n",
        "            g[\"weight_decay\"] = 0.0\n",
        "\n",
        "    # ---------- 3) OPTIMIZER ----------\n",
        "    optim = torch.optim.AdamW(\n",
        "        groups, lr=base_lr,\n",
        "        betas=(0.9, 0.999), eps=1e-6,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    if not return_scheduler:\n",
        "        return optim\n",
        "\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optim, T_max=100, eta_min=base_lr * 0.05)\n",
        "    return optim, sched\n",
        "\n",
        "\n",
        "def build_model(num_classes: int = 20,\n",
        "                norm: str = \"gn\",            # \"gn\", \"lnp\" veya \"bn\"\n",
        "                use_layer_scale: bool = True,\n",
        "                layer_scale_init: float = 1.0,\n",
        "                **kwargs) -> HybridDCDATRT:\n",
        "    \"\"\"\n",
        "    High-level constructor. The most commonly used settings come with defaults.\n",
        "    HybridDCDATRT __init__ parameters (e.g., dec_layers, dn_queries,\n",
        "    decoder_use_encoder, etc.) can be overridden via `kwargs`.\n",
        "    \"\"\"\n",
        "    defaults = dict(\n",
        "        # Backbone\n",
        "        depths         = (2, 2, 4, 1),\n",
        "        drop_path_max  = 0.2,\n",
        "        # Decoder\n",
        "        d_model        = 320,\n",
        "        dec_layers     = 4,\n",
        "        dn_queries     = 100,\n",
        "        num_queries    = 200,\n",
        "        use_aux_loss   = True,\n",
        "        use_iou_aware  = False,\n",
        "        # Dataset\n",
        "        voc_prior      = False,\n",
        "\n",
        "        decoder_use_encoder           = True,\n",
        "        decoder_encoder_layers        = 2,\n",
        "        decoder_encoder_drop_path_max = 0.1,\n",
        "    )\n",
        "    defaults.update(kwargs)\n",
        "\n",
        "    nf = NormFactory(norm)\n",
        "    model = HybridDCDATRT(\n",
        "        num_classes           = num_classes,\n",
        "        backbone_norm_factory = nf,\n",
        "        neck_norm_factory     = nf,\n",
        "        use_layer_scale       = use_layer_scale,\n",
        "        layer_scale_init      = layer_scale_init,\n",
        "        **defaults\n",
        "    )\n",
        "    # init_weights_improved(model)\n",
        "    # apply_hybrid_fixup(model)\n",
        "    # boost_relative_position_bias(model, std=0.05)\n",
        "    return model\n",
        "# -----------------------------------------------------------------------------\n",
        "# Test dummy code\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     import time\n",
        "\n",
        "#     assert torch.cuda.is_available(), \"CUDA required for this model.\"\n",
        "#     device = torch.device(\"cuda\")\n",
        "\n",
        "#     print(\"=\"*80)\n",
        "#     print(\"Building Hybrid-DCDAT-RT Model...\")\n",
        "#     print(\"=\"*80)\n",
        "\n",
        "#     model = build_model(\n",
        "#         num_classes=80,\n",
        "#         depths=[2, 2, 2, 2],  # ResNet-50 like depth\n",
        "#         drop_path_max=0,\n",
        "#         num_queries=200,\n",
        "#         voc_prior=False,norm=\"gn\"  # Use COCO prior\n",
        "#     ).to(device)\n",
        "#     optimizer = build_optimizer(model, base_lr=2e-4)\n",
        "#     # Test forward pass\n",
        "#     print(\"\\n🚀 Testing forward pass...\")\n",
        "#     model.eval()\n",
        "#     dummy = torch.randn(2, 3, 640, 640, device=device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         out = model(dummy)\n",
        "#         print(\"✅ Forward pass successful!\")\n",
        "#         print(f\"Output keys: {list(out.keys())}\")\n",
        "#         print(f\"Predictions shape: {out['pred_logits'].shape}, {out['pred_boxes'].shape}\")\n",
        "\n",
        "#     # Test training mode with targets\n",
        "#     print(\"\\n🚀 Testing training mode...\")\n",
        "#     model.train()\n",
        "\n",
        "#     # Create more realistic targets with proper box format\n",
        "#     targets = []\n",
        "#     for _ in range(2):  # batch size 2\n",
        "#         num_objs = torch.randint(1, 10, (1,)).item()\n",
        "#         # Generate boxes in (cx, cy, w, h) format, all normalized to [0, 1]\n",
        "#         centers = torch.rand(num_objs, 2, device=device)\n",
        "#         sizes = torch.rand(num_objs, 2, device=device) * 0.5  # max size 0.5\n",
        "#         boxes = torch.cat([centers, sizes], dim=1)\n",
        "\n",
        "#         # Ensure boxes are valid (centers must be at least half-size from borders)\n",
        "#         half_sizes = boxes[:, 2:4] / 2\n",
        "#         boxes[:, 0] = boxes[:, 0].clamp(min=half_sizes[:, 0], max=1-half_sizes[:, 0])\n",
        "#         boxes[:, 1] = boxes[:, 1].clamp(min=half_sizes[:, 1], max=1-half_sizes[:, 1])\n",
        "\n",
        "#         targets.append({\n",
        "#             'boxes': boxes,\n",
        "#             'labels': torch.randint(0, 80, (num_objs,), device=device)\n",
        "#         })\n",
        "\n",
        "#     out = model(dummy, targets)\n",
        "#     print(\"✅ Training mode successful!\")\n",
        "#     print(f\"Output keys: {list(out.keys())}\")\n",
        "#     if 'aux_dense' in out:\n",
        "#         print(f\"Auxiliary dense outputs: {list(out['aux_dense'].keys())}\")\n",
        "#     if 'dn_meta' in out:\n",
        "#         print(f\"Denoising queries: {out['dn_meta']['dn_queries']}\")\n",
        "\n",
        "#     # Test gradient flow\n",
        "#     print(\"\\n🚀 Testing gradient flow...\")\n",
        "#     loss = out['pred_logits'].sum() + out['pred_boxes'].sum()\n",
        "#     loss.backward()\n",
        "\n",
        "#     # Check that gradients flow through all parts\n",
        "#     has_grad = {\n",
        "#         'backbone': any(p.grad is not None for n, p in model.named_parameters() if 'backbone' in n),\n",
        "#         'neck': any(p.grad is not None for n, p in model.named_parameters() if 'neck' in n),\n",
        "#         'decoder': any(p.grad is not None for n, p in model.named_parameters() if 'decoder' in n),\n",
        "#     }\n",
        "\n",
        "#     for module, has in has_grad.items():\n",
        "#         print(f\"  {module}: {'✅' if has else '❌'} gradients\")\n",
        "\n",
        "#     # Test parameter groups\n",
        "#     print(\"\\n🚀 Testing parameter groups...\")\n",
        "#     param_groups = model.param_groups(base_lr=2e-4)\n",
        "#     print(f\"Number of parameter groups: {len(param_groups)}\")\n",
        "#     for i, group in enumerate(param_groups):\n",
        "#         print(f\"  Group {i}: {len(group['params'])} params, lr={group['lr']:.6f}, wd={group['weight_decay']}\")\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*80)\n",
        "#     print(\"✅ All tests passed! Model is ready.\")\n",
        "#     print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBPaXZykoydr"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torchvision.ops import generalized_box_iou, box_convert\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Helpers: focal losses\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def softmax_focal_loss_weighted(\n",
        "    logits: torch.Tensor,          # (N, C+1)\n",
        "    targets: torch.Tensor,         # (N,) int64  (‑1 = ignore)\n",
        "    *,\n",
        "    alpha: float = 0.25,\n",
        "    gamma: float = 2.0,\n",
        "    sample_weight: Optional[torch.Tensor] = None,  # (N,) or None\n",
        "    reduction: str = \"mean\",\n",
        "    ignore_index: int = -1,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Multi-class focal loss (softmax) + sample weight.\n",
        "    • targets == ignore_index → not included in loss.\n",
        "    • BG class ID = C (i.e., num_classes).\n",
        "    • sample_weight to weight positive/negative samples separately (e.g., QFL).\n",
        "    \"\"\"\n",
        "    logp = F.log_softmax(logits, dim=-1)\n",
        "    p = logp.exp()  # (N, C+1)\n",
        "\n",
        "    if ignore_index is not None:\n",
        "        keep = targets.ne(ignore_index)\n",
        "        if keep.sum() == 0:\n",
        "            return logits.sum() * 0.0\n",
        "        logits = logits[keep]\n",
        "        logp = logp[keep]\n",
        "        p = p[keep]\n",
        "        targets = targets[keep]\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = sample_weight[keep]\n",
        "\n",
        "    idx = torch.arange(targets.size(0), device=targets.device)\n",
        "    log_pt = logp[idx, targets]\n",
        "    pt = p[idx, targets]\n",
        "\n",
        "    bg_id = logits.size(1) - 1\n",
        "    alpha_t = torch.where(targets == bg_id, 1.0 - alpha, alpha)\n",
        "\n",
        "    loss = -alpha_t * (1.0 - pt).pow(gamma) * log_pt  # (N,)\n",
        "\n",
        "    if sample_weight is not None:\n",
        "        loss = loss * sample_weight\n",
        "\n",
        "    if reduction == \"sum\":\n",
        "        return loss.sum()\n",
        "    if reduction == \"mean\":\n",
        "        return loss.mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def binary_focal_with_logits(\n",
        "    input: torch.Tensor,           # (..., C)\n",
        "    target: torch.Tensor,          # (..., C) in {0,1}\n",
        "    alpha: float = 0.25,\n",
        "    gamma: float = 2.0,\n",
        "    reduction: str = \"mean\",\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Sigmoid (channel-independent) focal loss. BG masked (target 0).\"\"\"\n",
        "    ce = F.binary_cross_entropy_with_logits(input, target, reduction=\"none\")\n",
        "    p = torch.sigmoid(input)\n",
        "    p_t = p * target + (1 - p) * (1 - target)\n",
        "    alpha_t = alpha * target + (1 - alpha) * (1 - target)\n",
        "    loss = alpha_t * (1 - p_t).pow(gamma) * ce\n",
        "\n",
        "    if reduction == \"sum\":\n",
        "        return loss.sum()\n",
        "    if reduction == \"mean\":\n",
        "        return loss.mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Dense aux (sigmoid focal): BG implicit\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def focal_loss_dense_sigmoid(logits, targets, num_classes, alpha=.5, gamma=1.5, use_or_map=False, radius: int = 1):\n",
        "    B, C, H, W = logits.shape\n",
        "    device = logits.device\n",
        "    assert C == num_classes\n",
        "    tgt = torch.zeros(B, C, H, W, device=device)\n",
        "\n",
        "    for b, t in enumerate(targets):\n",
        "        if len(t[\"boxes\"]) == 0: continue\n",
        "        ctr = t[\"boxes\"][:, :2] * torch.tensor([W, H], device=device)\n",
        "        gx = ctr[:, 0].long().clamp(0, W - 1)\n",
        "        gy = ctr[:, 1].long().clamp(0, H - 1)\n",
        "        cls = t[\"labels\"].clamp(0, C - 1)\n",
        "\n",
        "        for (x, y, c) in zip(gx.tolist(), gy.tolist(), cls.tolist()):\n",
        "            x0, x1 = max(0, x - radius), min(W - 1, x + radius)\n",
        "            y0, y1 = max(0, y - radius), min(H - 1, y + radius)\n",
        "            tgt[b, c, y0:y1+1, x0:x1+1] = 1.0  # a small window\n",
        "\n",
        "    logits_flat = logits.permute(0, 2, 3, 1).reshape(-1, C)\n",
        "    tgt_flat = tgt.permute(0, 2, 3, 1).reshape(-1, C)\n",
        "    return binary_focal_with_logits(logits_flat, tgt_flat, alpha=alpha, gamma=gamma, reduction=\"mean\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Hungarian matcher (log‑prob, cost scaling)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"One‑to‑one matching between predicted queries and GT boxes.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 cost_class: float = 2.,\n",
        "                 cost_bbox: float = 5.,\n",
        "                 cost_giou: float = 2.):\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self,\n",
        "                outputs: Dict[str, torch.Tensor],\n",
        "                targets: List[Dict]) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        outputs:\n",
        "            - pred_logits: (B, N, C+1)\n",
        "            - pred_boxes : (B, N, 4)  (cx,cy,w,h norm.)\n",
        "        NOTE: query_selection_mask is not used here; we use it to ignore negatives on the loss side.\n",
        "        \"\"\"\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "        device = outputs[\"pred_logits\"].device\n",
        "\n",
        "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)   # (B·N, C+1)\n",
        "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)                # (B·N, 4)\n",
        "\n",
        "        tgt_ids  = torch.cat([v[\"labels\"] for v in targets])          # (ΣT,)\n",
        "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])           # (ΣT, 4)\n",
        "\n",
        "        cost_class = -out_prob[:, tgt_ids].clamp(1e-8).log()          # (B·N, ΣT)\n",
        "        cost_bbox  = torch.cdist(out_bbox, tgt_bbox, p=1)             # L1\n",
        "        cost_giou  = -generalized_box_iou(\n",
        "            box_convert(out_bbox, \"cxcywh\", \"xyxy\"),\n",
        "            box_convert(tgt_bbox, \"cxcywh\", \"xyxy\")\n",
        "        )\n",
        "\n",
        "        C = (self.cost_class * cost_class +\n",
        "             self.cost_bbox  * cost_bbox  +\n",
        "             self.cost_giou  * cost_giou)                             # (B·N, ΣT)\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "\n",
        "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
        "        indices: list[Tuple[torch.Tensor, torch.Tensor]] = []\n",
        "        tgt_ptr = 0\n",
        "        for b in range(bs):\n",
        "            tgt_cnt = sizes[b]\n",
        "            if tgt_cnt == 0:\n",
        "                indices.append((torch.empty(0, dtype=torch.int64, device=device),\n",
        "                                torch.empty(0, dtype=torch.int64, device=device)))\n",
        "                continue\n",
        "            cost_b = C[b, :, tgt_ptr: tgt_ptr + tgt_cnt].numpy()\n",
        "            row_ind, col_ind = linear_sum_assignment(cost_b)\n",
        "            indices.append((torch.as_tensor(row_ind, dtype=torch.int64, device=device),\n",
        "                            torch.as_tensor(col_ind, dtype=torch.int64, device=device)))\n",
        "            tgt_ptr += tgt_cnt\n",
        "\n",
        "        return indices\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Utilities for QFL‑Softmax (quality weights for positives)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def _compute_matched_ious(\n",
        "    pred_boxes: torch.Tensor,  # (B,N,4)\n",
        "    targets: List[Dict],       # list of dict with \"boxes\"\n",
        "    indices: List[Tuple[torch.Tensor, torch.Tensor]],\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculates IoU(pred_box, gt_box) for matched positives.\n",
        "    Returns: (Σ_matches,) vector, [0,1].\n",
        "    \"\"\"\n",
        "    if len(indices) == 0:\n",
        "        return pred_boxes.new_zeros((0,))\n",
        "    b_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "    s_idx = torch.cat([src for (src, _) in indices])\n",
        "    if b_idx.numel() == 0:\n",
        "        return pred_boxes.new_zeros((0,))\n",
        "\n",
        "    src_boxes = pred_boxes[b_idx, s_idx]    # (Σ,4)\n",
        "    tgt_boxes = torch.cat([t[\"boxes\"][J] for t, (_, J) in zip(targets, indices)], 0)  # (Σ,4)\n",
        "\n",
        "    ious = torch.diag(generalized_box_iou(\n",
        "        box_convert(src_boxes, \"cxcywh\", \"xyxy\"),\n",
        "        box_convert(tgt_boxes, \"cxcywh\", \"xyxy\")\n",
        "    )).clamp(min=0., max=1.)\n",
        "    return ious  # (Σ,)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Main SetCriterion (production‑ready)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes all required losses for HybridDCDATRT training.\n",
        "\n",
        "    • Class loss: Softmax‑Focal + QFL‑Softmax (positive examples quality‑weighted).\n",
        "    • Box: L1 + GIoU.\n",
        "    • IoU regression: L1( pred_ious , IoU(gt, pred) ).\n",
        "    • Dense aux: Sigmoid‑focal (FG channels).\n",
        "    • DN (denoising): class + box (L1+GIoU).\n",
        "    • Aux decoder outputs: labels/boxes, weighted.\n",
        "    • (Optional) Objectness: If outputs[“pred_obj_logits”] exists, BCE loss is added.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes: int,\n",
        "                 matcher: HungarianMatcher,\n",
        "                 weight_dict: Dict[str, float],\n",
        "                 eos_coef: float = .1,\n",
        "                 losses: Optional[List[str]] = None,\n",
        "                 aux_weight: float = .4,\n",
        "                 *,\n",
        "                 # QFL‑Softmax settings\n",
        "                 use_qfl: bool = True,\n",
        "                 qfl_beta: float = 1.0,            # quality ^ beta\n",
        "                 qfl_lambda: float = 0.5,          # target IoU (1-λ) + sigmoid(pred) * λ\n",
        "                 qfl_use_pred: bool = True):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.aux_weight = aux_weight\n",
        "\n",
        "        self.losses = losses or [\n",
        "            \"labels\", \"boxes\", \"cardinality\",\n",
        "            \"dense_aux\",\n",
        "            \"dn_label\", \"dn_box\",\n",
        "            \"iou\",\n",
        "            \"obj\"      # optional; calculated if pred_obj_logits exists\n",
        "        ]\n",
        "\n",
        "        # background (no-object) class weight – (used with alpha in softmax focal)\n",
        "        empty_w = torch.ones(self.num_classes + 1)\n",
        "        empty_w[-1] = eos_coef\n",
        "        self.register_buffer(\"empty_weight\", empty_w)\n",
        "\n",
        "        # QFL parameters\n",
        "        self.use_qfl = use_qfl\n",
        "        self.qfl_beta = float(qfl_beta)\n",
        "        self.qfl_lambda = float(qfl_lambda)\n",
        "        self.qfl_use_pred = bool(qfl_use_pred)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  helpers\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        batch_idx = torch.cat([torch.full_like(src, i)\n",
        "                               for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _build_pos_mask(self, outputs, indices):\n",
        "        \"\"\"(B,N) bool; eşleşmiş (pozitif) sorgular True.\"\"\"\n",
        "        B, N = outputs[\"pred_logits\"].shape[:2]\n",
        "        pos = outputs[\"pred_logits\"].new_zeros((B, N), dtype=torch.bool)\n",
        "        b_idx, s_idx = self._get_src_permutation_idx(indices)\n",
        "        if b_idx.numel():\n",
        "            pos[b_idx, s_idx] = True\n",
        "        return pos\n",
        "\n",
        "    # -------------------------------- labels (QFL‑Softmax) -------------\n",
        "    # ---- SetCriterion.loss_labels  ----\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes, **_):\n",
        "        src_logits = outputs[\"pred_logits\"]            # (B,N,C+1)\n",
        "        B, N = src_logits.shape[:2]\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "\n",
        "        # all unmatched queries → background\n",
        "        tgt_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                dtype=torch.int64, device=src_logits.device)\n",
        "        if idx[0].numel():\n",
        "            tgt_classes[idx] = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)], 0)\n",
        "\n",
        "        # keep = sel | pos (ignore those that are not keep)\n",
        "        if \"query_selection_mask\" in outputs:\n",
        "            sel = outputs[\"query_selection_mask\"].to(torch.bool)\n",
        "            pos = self._build_pos_mask(outputs, indices)\n",
        "            keep = (sel | pos)\n",
        "            tgt_classes = tgt_classes.clone()\n",
        "            tgt_classes[~keep] = -1\n",
        "        else:\n",
        "            keep = torch.ones(B, N, dtype=torch.bool, device=src_logits.device)\n",
        "\n",
        "        # --- BG weighting: BG → self.empty_weight[-1] (e.g., 0.1), FG → 1.0\n",
        "        base_w = torch.ones(B, N, device=src_logits.device, dtype=src_logits.dtype)\n",
        "        bg_id = self.num_classes\n",
        "        base_w[tgt_classes == bg_id] = self.empty_weight[-1].item()\n",
        "\n",
        "        # --- QFL quality weight (apply only to positives)\n",
        "        sample_weight = base_w\n",
        "        if self.use_qfl and idx[0].numel():\n",
        "            with torch.no_grad():\n",
        "                ious_t = _compute_matched_ious(outputs[\"pred_boxes\"], targets, indices)  # (Σ_pos,)\n",
        "                if self.qfl_use_pred and (\"pred_ious\" in outputs):\n",
        "                    q_pred = torch.sigmoid(outputs[\"pred_ious\"][idx])\n",
        "                    q = ((1.0 - self.qfl_lambda) * ious_t + self.qfl_lambda * q_pred).clamp(0., 1.)\n",
        "                else:\n",
        "                    q = ious_t.clamp(0., 1.)\n",
        "\n",
        "                q = q.pow(self.qfl_beta)\n",
        "            sample_weight = sample_weight.clone()\n",
        "            sample_weight[idx] = sample_weight[idx] * q  # quality multiplier for positives\n",
        "\n",
        "        loss = softmax_focal_loss_weighted(\n",
        "            src_logits.flatten(0, 1),\n",
        "            tgt_classes.flatten(),\n",
        "            alpha=.25, gamma=2., reduction=\"sum\",\n",
        "            sample_weight=sample_weight.flatten(0, 1)\n",
        "        ) / max(num_boxes, 1.0)\n",
        "\n",
        "        return {\"loss_labels\": loss}\n",
        "\n",
        "    # -------------------------------- cardinality ----------------------\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes, **_):\n",
        "        probs = outputs[\"pred_logits\"].softmax(-1)   # (B,N,C+1)\n",
        "        bg = probs[..., -1]\n",
        "\n",
        "        if \"query_selection_mask\" in outputs:\n",
        "            sel = outputs[\"query_selection_mask\"].to(torch.bool)\n",
        "            pos = self._build_pos_mask(outputs, indices)\n",
        "            keep = (sel | pos).float()\n",
        "            card_pred = ((1. - bg) * keep).sum(1)    # only keep\n",
        "        else:\n",
        "            card_pred = (1. - bg).sum(1)\n",
        "\n",
        "        tgt_lens = torch.as_tensor([len(t[\"labels\"]) for t in targets],\n",
        "                                   dtype=torch.float, device=probs.device)\n",
        "        loss = F.l1_loss(card_pred, tgt_lens)\n",
        "        return {\"loss_cardinality\": loss}\n",
        "\n",
        "    # -------------------------------- boxes ----------------------------\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes, **_):\n",
        "        \"\"\"\n",
        "        If there is no match (no GT box) ⇒ 0 loss is returned.\n",
        "        \"\"\"\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        if idx[0].numel() == 0:                       # zero‑match guard\n",
        "            z = outputs[\"pred_boxes\"].sum() * 0.0\n",
        "            return {\"loss_bbox\": z, \"loss_giou\": z}\n",
        "\n",
        "        src_boxes = outputs[\"pred_boxes\"][idx]\n",
        "        tgt_boxes = torch.cat([t[\"boxes\"][i]\n",
        "                              for t, (_, i) in zip(targets, indices)], 0)\n",
        "\n",
        "        l1 = F.l1_loss(src_boxes, tgt_boxes, reduction=\"none\").sum() / max(num_boxes, 1.0)\n",
        "\n",
        "        giou = 1.0 - torch.diag(generalized_box_iou(\n",
        "            box_convert(src_boxes, \"cxcywh\", \"xyxy\"),\n",
        "            box_convert(tgt_boxes, \"cxcywh\", \"xyxy\")\n",
        "        )).sum() / max(num_boxes, 1.0)\n",
        "\n",
        "        return {\"loss_bbox\": l1, \"loss_giou\": giou}\n",
        "\n",
        "    # -------------------------------- dense aux ------------------------\n",
        "    def loss_dense_aux(self, outputs, targets, *_):\n",
        "        if \"aux_dense\" not in outputs:\n",
        "            return {}\n",
        "\n",
        "        total = 0.0\n",
        "        for pred in outputs[\"aux_dense\"].values():           # s1, s2, s3\n",
        "            logits = pred[:, :self.num_classes]              # (B,C,H,W) — FG channels\n",
        "            total = total + focal_loss_dense_sigmoid(\n",
        "                logits, targets, self.num_classes,\n",
        "                alpha=.25, gamma=2., use_or_map=False\n",
        "            )\n",
        "\n",
        "        return {\"loss_dense_aux\": total}\n",
        "\n",
        "    # ------------------------------- denoising cls ---------------------\n",
        "    def loss_dn_label(self, outputs, *_):\n",
        "        if \"dn_meta\" not in outputs:\n",
        "            return {}\n",
        "        meta = outputs[\"dn_meta\"]\n",
        "        loss = softmax_focal_loss_weighted(\n",
        "            meta[\"dn_logits\"].flatten(0, 1),\n",
        "            meta[\"dn_labels\"].flatten(),\n",
        "            alpha=.25, gamma=2., reduction=\"mean\",\n",
        "            sample_weight=None\n",
        "        )\n",
        "        return {\"loss_dn_label\": loss}\n",
        "\n",
        "    # ------------------------------- denoising box ---------------------\n",
        "    def loss_dn_box(self, outputs, *_):\n",
        "        \"\"\"\n",
        "         Denoisy box loss – memory-friendly (B-looped) GIoU.\n",
        "        \"\"\"\n",
        "        if \"dn_meta\" not in outputs:\n",
        "            return {}\n",
        "        meta = outputs[\"dn_meta\"]\n",
        "        dn_boxes, dn_gt = meta[\"dn_boxes\"], meta[\"dn_gt_boxes\"]   # (B,Q,4)\n",
        "\n",
        "        # L1\n",
        "        l1 = F.l1_loss(dn_boxes, dn_gt, reduction=\"none\").sum(-1).mean()\n",
        "\n",
        "        # GIoU (batch‑wise)\n",
        "        giou_acc = 0.0\n",
        "        for db, dg in zip(dn_boxes, dn_gt):\n",
        "            giou_acc += 1. - torch.diag(generalized_box_iou(\n",
        "                box_convert(db, 'cxcywh', 'xyxy'),\n",
        "                box_convert(dg, 'cxcywh', 'xyxy')\n",
        "            )).mean()\n",
        "        giou = giou_acc / dn_boxes.size(0)\n",
        "\n",
        "        return {\"loss_dn_bbox\": l1, \"loss_dn_giou\": giou}\n",
        "\n",
        "    # ------------------------------- IoU reg ---------------------------\n",
        "    def loss_iou(self, outputs, targets, indices, *_):\n",
        "        if \"pred_ious\" not in outputs:\n",
        "            return {}\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        if idx[0].numel() == 0:\n",
        "            return {\"loss_iou\": outputs[\"pred_ious\"].sum() * 0.0}\n",
        "\n",
        "        pred_ious = torch.sigmoid(outputs[\"pred_ious\"])[idx]  # <<< EK\n",
        "        src_boxes = outputs[\"pred_boxes\"][idx]\n",
        "        tgt_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], 0)\n",
        "\n",
        "        ious = torch.diag(generalized_box_iou(\n",
        "            box_convert(src_boxes, \"cxcywh\", \"xyxy\"),\n",
        "            box_convert(tgt_boxes, \"cxcywh\", \"xyxy\")\n",
        "        )).clamp(min=0., max=1.)\n",
        "\n",
        "        return {\"loss_iou\": F.l1_loss(pred_ious, ious)}\n",
        "\n",
        "    # ------------------------------- Objectness (optional) ------------\n",
        "    def loss_obj(self, outputs, targets, indices, num_boxes, **_):\n",
        "        \"\"\"\n",
        "        If the decoder has produced ‘pred_obj_logits’ (B,N), the simple BCE loss is calculated using the keep mask (sel|pos).\n",
        "        Otherwise, it returns empty.\n",
        "        \"\"\"\n",
        "        if \"pred_obj_logits\" not in outputs:\n",
        "            return {}\n",
        "        obj_logits = outputs[\"pred_obj_logits\"]  # (B,N)\n",
        "\n",
        "        # keep = sel|pos\n",
        "        if \"query_selection_mask\" in outputs:\n",
        "            sel = outputs[\"query_selection_mask\"].to(torch.bool)  # (B,N)\n",
        "        else:\n",
        "            # If there is no sel, assume all are negative; a positive mask is required to preserve the positives.\n",
        "            sel = torch.zeros_like(obj_logits, dtype=torch.bool)\n",
        "\n",
        "        pos = self._build_pos_mask(outputs, indices)              # (B,N) True=matched\n",
        "        keep = sel | pos\n",
        "\n",
        "        # target: pos→1, (keep & ~pos) → 0, ignore non keep\n",
        "        target = keep & pos\n",
        "        # ignore: keep=False ⇒ maskle\n",
        "        if keep.sum() == 0:\n",
        "            return {\"loss_obj\": obj_logits.sum() * 0.0}\n",
        "\n",
        "        loss = F.binary_cross_entropy_with_logits(\n",
        "            obj_logits[keep], target[keep].float(), reduction=\"mean\"\n",
        "        )\n",
        "        # fixed weight for object calibration\n",
        "        return {\"loss_obj\": loss}\n",
        "\n",
        "\n",
        "    def loss_center(self, outputs, targets, indices, num_boxes, **_):\n",
        "        if \"final_ref_pts\" not in outputs:\n",
        "            return {}\n",
        "\n",
        "        # Only POSITIVE matches\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        if idx[0].numel() == 0:\n",
        "            z = outputs[\"pred_boxes\"].sum() * 0.0\n",
        "            return {\"loss_center\": z}\n",
        "\n",
        "        # (Σ_pos, 4, 2) veya (Σ_pos, 2)\n",
        "        ref = outputs[\"final_ref_pts\"][idx[0], idx[1]]\n",
        "        ctr_ref = ref if ref.dim() == 2 else ref.mean(dim=1)  # (Σ_pos, 2)\n",
        "\n",
        "        ctr_pred = outputs[\"pred_boxes\"][idx][..., :2]        # (Σ_pos, 2)\n",
        "        loss = F.l1_loss(ctr_pred, ctr_ref, reduction=\"sum\") / max(num_boxes, 1.0)\n",
        "        return {\"loss_center\": loss}\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def get_loss(self, name, outputs, targets, indices, num_boxes):\n",
        "        return {\n",
        "            \"labels\":      self.loss_labels,\n",
        "            \"cardinality\": self.loss_cardinality,\n",
        "            \"boxes\":       self.loss_boxes,\n",
        "            \"dense_aux\":   self.loss_dense_aux,\n",
        "            \"dn_label\":    self.loss_dn_label,\n",
        "            \"dn_box\":      self.loss_dn_box,\n",
        "            \"iou\":         self.loss_iou,\n",
        "            \"obj\":         self.loss_obj,\n",
        "            \"center\":      self.loss_center,\n",
        "        }[name](outputs, targets, indices, num_boxes)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def forward(self,\n",
        "                outputs: Dict[str, torch.Tensor],\n",
        "                targets: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        # Hungarian matching (main outputs)\n",
        "        indices = self.matcher(outputs, targets)\n",
        "\n",
        "        # num_boxes (DDP senkronlu)\n",
        "        device = outputs[\"pred_logits\"].device\n",
        "        nb = torch.as_tensor([sum(len(t[\"labels\"]) for t in targets)],\n",
        "                     dtype=torch.float, device=device)\n",
        "        if dist.is_available() and dist.is_initialized():\n",
        "            # Sum all processes → then divide by the world size (average)\n",
        "            dist.all_reduce(nb, op=dist.ReduceOp.SUM)\n",
        "            world_size = dist.get_world_size()\n",
        "            nb = nb / max(world_size, 1)\n",
        "        num_boxes = max(nb.item(), 1.0)\n",
        "\n",
        "        # ---- actual losses ----\n",
        "        loss_dict: Dict[str, torch.Tensor] = {}\n",
        "        for name in self.losses:\n",
        "            if name.startswith(\"dn_\") and \"dn_meta\" not in outputs:\n",
        "                continue\n",
        "            loss_dict.update(self.get_loss(\n",
        "                name, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        # ---- helper decoder outs ----\n",
        "        sel_mask = outputs.get(\"query_selection_mask\", None)\n",
        "        if \"aux_outputs\" in outputs:\n",
        "            for i, aux in enumerate(outputs[\"aux_outputs\"]):\n",
        "                # mask'i aux'a geçir (shape: (B,N))\n",
        "                aux_in = aux if sel_mask is None else {**aux, \"query_selection_mask\": sel_mask}\n",
        "                aux_idx = self.matcher(aux_in, targets)\n",
        "                for l in (\"labels\", \"boxes\"):\n",
        "                    l_dict = self.get_loss(l, aux_in, targets, aux_idx, num_boxes)\n",
        "                    l_dict = {k + f\"_{i}\": v * self.aux_weight for k, v in l_dict.items()}\n",
        "                    loss_dict.update(l_dict)\n",
        "\n",
        "        # ---- apply weight ----\n",
        "        weighted: Dict[str, torch.Tensor] = {}\n",
        "        for k, v in loss_dict.items():\n",
        "            base = None\n",
        "            for b in self.weight_dict:\n",
        "                if k == b or k.startswith(b + \"_\"):\n",
        "                    base = b\n",
        "                    break\n",
        "            weighted[k] = v * self.weight_dict.get(base, 1.0)\n",
        "\n",
        "        weighted[\"loss\"] = sum(weighted.values())\n",
        "        return weighted\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Builder\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def build_criterion(\n",
        "    num_classes: int = 20,\n",
        "    weight_dict: Optional[Dict[str, float]] = None,\n",
        "    matcher_costs: Optional[Dict[str, float]] = None,\n",
        ") -> SetCriterion:\n",
        "\n",
        "    if weight_dict is None:\n",
        "        weight_dict = {\n",
        "            \"loss_labels\":      2.0,\n",
        "            \"loss_bbox\":        5.0,\n",
        "            \"loss_giou\":        2.5,\n",
        "            \"loss_center\":      0.1,  # small regulator\n",
        "            \"loss_cardinality\": 0.1,\n",
        "            \"loss_dense_aux\":   0.5,\n",
        "        }\n",
        "\n",
        "    if matcher_costs is None:\n",
        "        matcher_costs = {\"cost_class\": 2., \"cost_bbox\": 5., \"cost_giou\": 2.}\n",
        "\n",
        "    matcher = HungarianMatcher(**matcher_costs)\n",
        "\n",
        "    return SetCriterion(\n",
        "        num_classes=num_classes,\n",
        "        matcher=matcher,\n",
        "        weight_dict=weight_dict,\n",
        "        eos_coef=.1,\n",
        "        losses=[\n",
        "            \"labels\", \"boxes\", \"center\",\n",
        "            \"cardinality\", \"dense_aux\",\n",
        "        ],\n",
        "        aux_weight=.4,\n",
        "        # QFL only with matched IoU (no pred_ious)\n",
        "        use_qfl=True, qfl_beta=1.0, qfl_lambda=0.5, qfl_use_pred=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07mU7nWu1O6n"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "HybridDCDATRT Advanced Gradient Flow Diagnoser\n",
        "==============================================\n",
        "Focuses on vanishing gradients and non-updating parameters\n",
        "\"\"\"\n",
        "import os\n",
        "import random\n",
        "from typing import Dict, List, Tuple, Set\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.datasets import VOCDetection\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# CONFIG\n",
        "# -----------------------------------------------------------------------------\n",
        "VOC_ROOT = \"/content/voc/pascal-voc-2007-and-2012\"\n",
        "ITERS = 2000\n",
        "BATCH_SIZE = 8\n",
        "LR = 2e-4\n",
        "NUM_WORKERS = 8\n",
        "MIN_UPDATES = 5  # Low threshold - to catch those that are not updated\n",
        "IMG_SIZE = 480\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "# Gradient thresholds\n",
        "VANISHING_GRAD_THRESHOLD = 1e-7  # Vanishing gradient threshold\n",
        "LOW_GRAD_THRESHOLD = 1e-5        # Low gradient threshold\n",
        "HIGH_GRAD_THRESHOLD = 10.0       # High gradient threshold\n",
        "PARAM_CHANGE_THRESHOLD = 1e-8    # Minimum change to consider parameter updated\n",
        "\n",
        "CLASS2IDX = {\n",
        "    \"aeroplane\": 0, \"bicycle\": 1, \"bird\": 2, \"boat\": 3, \"bottle\": 4,\n",
        "    \"bus\": 5, \"car\": 6, \"cat\": 7, \"chair\": 8, \"cow\": 9,\n",
        "    \"diningtable\": 10, \"dog\": 11, \"horse\": 12, \"motorbike\": 13, \"person\": 14,\n",
        "    \"pottedplant\": 15, \"sheep\": 16, \"sofa\": 17, \"train\": 18, \"tvmonitor\": 19\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# VOC DATASET\n",
        "# -----------------------------------------------------------------------------\n",
        "class VOCDataset(Dataset):\n",
        "    \"\"\"VOC 2007+2012 dataset with cxcywh normalized boxes\"\"\"\n",
        "\n",
        "    def __init__(self, root: str, size: int = 640):\n",
        "        super().__init__()\n",
        "        train07 = VOCDetection(root=root, year=\"2007\", image_set=\"trainval\", download=False)\n",
        "        train12 = VOCDetection(root=root, year=\"2012\", image_set=\"trainval\", download=False)\n",
        "        self.ds = train07\n",
        "        self.size = int(size)\n",
        "        # print(f\"✓ VOC dataset loaded: {len(train07)} + {len(train12)} = {len(self.ds)} images\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_rgb(img):\n",
        "        img = TF.to_tensor(img)\n",
        "        if img.shape[0] == 1:\n",
        "            img = img.repeat(3, 1, 1)\n",
        "        elif img.shape[0] == 4:\n",
        "            img = img[:3]\n",
        "        return img\n",
        "\n",
        "    def _resize_and_pad(self, img: torch.Tensor, boxes_xyxy_norm: torch.Tensor):\n",
        "        H, W = img.shape[-2:]\n",
        "        S = self.size\n",
        "        scale = S / max(H, W)\n",
        "        new_h, new_w = int(H * scale), int(W * scale)\n",
        "\n",
        "        img = TF.resize(img, (new_h, new_w), antialias=True)\n",
        "        pad_h = (S - new_h) // 2\n",
        "        pad_w = (S - new_w) // 2\n",
        "        img = TF.pad(img, [pad_w, pad_h, S - new_w - pad_w, S - new_h - pad_h], fill=0.5)\n",
        "\n",
        "        # --- normalize → pixel → resize+pad → normalize ---\n",
        "        x1 = boxes_xyxy_norm[:, 0] * W\n",
        "        y1 = boxes_xyxy_norm[:, 1] * H\n",
        "        x2 = boxes_xyxy_norm[:, 2] * W\n",
        "        y2 = boxes_xyxy_norm[:, 3] * H\n",
        "\n",
        "        x1 = x1 * scale + pad_w\n",
        "        x2 = x2 * scale + pad_w\n",
        "        y1 = y1 * scale + pad_h\n",
        "        y2 = y2 * scale + pad_h\n",
        "\n",
        "        x1 /= S; x2 /= S; y1 /= S; y2 /= S\n",
        "\n",
        "        cx = (x1 + x2) / 2\n",
        "        cy = (y1 + y2) / 2\n",
        "        w  = (x2 - x1).clamp_min(1e-4)\n",
        "        h  = (y2 - y1).clamp_min(1e-4)\n",
        "\n",
        "        boxes_cxcywh = torch.stack([cx, cy, w, h], dim=1).clamp(0., 1.)\n",
        "        return img, boxes_cxcywh\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_pil, ann = self.ds[idx]\n",
        "\n",
        "        w, h = img_pil.width, img_pil.height\n",
        "        img = self._to_rgb(img_pil)\n",
        "\n",
        "        boxes, labels = [], []\n",
        "\n",
        "        if isinstance(ann, dict) and \"annotation\" in ann:\n",
        "            objs = ann[\"annotation\"].get(\"object\", [])\n",
        "            objs = objs if isinstance(objs, list) else [objs]\n",
        "\n",
        "            for o in objs:\n",
        "                cls = o[\"name\"].lower().strip()\n",
        "                if cls not in CLASS2IDX:\n",
        "                    continue\n",
        "\n",
        "                bnd = o[\"bndbox\"]\n",
        "                xmin = (float(bnd[\"xmin\"]) - 1) / w\n",
        "                ymin = (float(bnd[\"ymin\"]) - 1) / h\n",
        "                xmax = (float(bnd[\"xmax\"]) - 1) / w\n",
        "                ymax = (float(bnd[\"ymax\"]) - 1) / h\n",
        "\n",
        "                if xmax <= xmin or ymax <= ymin:\n",
        "                    continue\n",
        "                if (xmax - xmin) < 0.01 or (ymax - ymin) < 0.01:\n",
        "                    continue\n",
        "\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "                labels.append(CLASS2IDX[cls])\n",
        "\n",
        "        if not boxes:\n",
        "            boxes = [[0.5, 0.5, 0.1, 0.1]]\n",
        "            labels = [0]\n",
        "        else:\n",
        "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.tensor(labels, dtype=torch.int64)\n",
        "            img, boxes = self._resize_and_pad(img, boxes)\n",
        "\n",
        "        img = TF.normalize(img, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "\n",
        "        if len(boxes) > 0 and isinstance(boxes, torch.Tensor):\n",
        "            boxes[:, 2:4] = boxes[:, 2:4].clamp(min=0.01)\n",
        "            half_w = boxes[:, 2] / 2\n",
        "            half_h = boxes[:, 3] / 2\n",
        "            boxes[:, 0] = boxes[:, 0].clamp(min=half_w, max=1-half_w)\n",
        "            boxes[:, 1] = boxes[:, 1].clamp(min=half_h, max=1-half_h)\n",
        "        else:\n",
        "            boxes = torch.tensor([[0.5, 0.5, 0.1, 0.1]], dtype=torch.float32)\n",
        "            labels = torch.tensor([0], dtype=torch.int64)\n",
        "\n",
        "        return img, {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "def collate_fn(batch: List[Tuple[torch.Tensor, Dict]]):\n",
        "    imgs, targets = zip(*batch)\n",
        "    return torch.stack(imgs, 0), list(targets)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ADVANCED GRADIENT MONITOR\n",
        "# -----------------------------------------------------------------------------\n",
        "class AdvancedGradientMonitor:\n",
        "    \"\"\"Advanced gradient flow monitoring focusing on problematic parameters\"\"\"\n",
        "\n",
        "    def __init__(self, model: torch.nn.Module):\n",
        "        self.stats = {}\n",
        "        self.total_params = 0\n",
        "        self.module_info = defaultdict(lambda: {\"count\": 0, \"params\": 0})\n",
        "\n",
        "        # Initialize tracking for each parameter\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.total_params += param.numel()\n",
        "                module = name.split('.')[0]\n",
        "                self.module_info[module][\"count\"] += 1\n",
        "                self.module_info[module][\"params\"] += param.numel()\n",
        "\n",
        "                self.stats[name] = {\n",
        "                    \"shape\": list(param.shape),\n",
        "                    \"numel\": param.numel(),\n",
        "                    \"init_norm\": param.norm().item(),\n",
        "                    \"init_mean\": param.mean().item(),\n",
        "                    \"init_std\": param.std().item() if param.numel() > 1 else 0.0,\n",
        "                    \"current_norm\": param.norm().item(),\n",
        "                    \"total_change\": 0.0,\n",
        "                    \"grad_history\": [],\n",
        "                    \"vanishing_count\": 0,\n",
        "                    \"low_grad_count\": 0,\n",
        "                    \"high_grad_count\": 0,\n",
        "                    \"update_count\": 0,\n",
        "                    \"no_update_despite_grad\": 0,\n",
        "                    \"last_param\": param.detach().cpu().clone(),\n",
        "                    \"init_param\": param.detach().cpu().clone(),\n",
        "                }\n",
        "\n",
        "    def analyze_gradient(self, name: str, grad: torch.Tensor) -> Dict:\n",
        "        \"\"\"Analyze gradient characteristics\"\"\"\n",
        "        grad_norm = grad.norm().item()\n",
        "        grad_mean = grad.mean().item()\n",
        "        grad_std = grad.std().item() if grad.numel() > 1 else 0.0\n",
        "        grad_abs_mean = grad.abs().mean().item()\n",
        "\n",
        "        # Classify gradient\n",
        "        is_vanishing = grad_norm < VANISHING_GRAD_THRESHOLD\n",
        "        is_low = VANISHING_GRAD_THRESHOLD <= grad_norm < LOW_GRAD_THRESHOLD\n",
        "        is_high = grad_norm > HIGH_GRAD_THRESHOLD\n",
        "\n",
        "        # Count zero elements\n",
        "        zero_ratio = (grad.abs() < 1e-10).float().mean().item()\n",
        "\n",
        "        return {\n",
        "            \"norm\": grad_norm,\n",
        "            \"mean\": grad_mean,\n",
        "            \"std\": grad_std,\n",
        "            \"abs_mean\": grad_abs_mean,\n",
        "            \"is_vanishing\": is_vanishing,\n",
        "            \"is_low\": is_low,\n",
        "            \"is_high\": is_high,\n",
        "            \"zero_ratio\": zero_ratio,\n",
        "        }\n",
        "\n",
        "    def after_backward(self, model: torch.nn.Module):\n",
        "        \"\"\"Track gradients after backward pass\"\"\"\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None and name in self.stats:\n",
        "                grad_info = self.analyze_gradient(name, param.grad)\n",
        "\n",
        "                # Update statistics\n",
        "                self.stats[name][\"grad_history\"].append(grad_info)\n",
        "\n",
        "                if grad_info[\"is_vanishing\"]:\n",
        "                    self.stats[name][\"vanishing_count\"] += 1\n",
        "                elif grad_info[\"is_low\"]:\n",
        "                    self.stats[name][\"low_grad_count\"] += 1\n",
        "                elif grad_info[\"is_high\"]:\n",
        "                    self.stats[name][\"high_grad_count\"] += 1\n",
        "\n",
        "    def after_step(self, model: torch.nn.Module):\n",
        "        \"\"\"Track parameter updates after optimizer step\"\"\"\n",
        "        for name, param in model.named_parameters():\n",
        "            if name in self.stats:\n",
        "                current = param.detach().cpu()\n",
        "                last = self.stats[name][\"last_param\"]\n",
        "\n",
        "                # Check if parameter was updated\n",
        "                param_change = (current - last).abs().max().item()\n",
        "\n",
        "                if param_change > PARAM_CHANGE_THRESHOLD:\n",
        "                    self.stats[name][\"update_count\"] += 1\n",
        "                    self.stats[name][\"total_change\"] += param_change\n",
        "                    self.stats[name][\"last_param\"] = current.clone()\n",
        "                else:\n",
        "                    # Check if it had gradient but wasn't updated\n",
        "                    if (self.stats[name][\"grad_history\"] and\n",
        "                        self.stats[name][\"grad_history\"][-1][\"norm\"] > 0):\n",
        "                        self.stats[name][\"no_update_despite_grad\"] += 1\n",
        "\n",
        "                # Update current norm\n",
        "                self.stats[name][\"current_norm\"] = param.norm().item()\n",
        "\n",
        "    def get_problem_analysis(self):\n",
        "        \"\"\"Analyze and categorize problematic parameters\"\"\"\n",
        "        problems = {\n",
        "            \"never_updated\": [],\n",
        "            \"vanishing_grad\": [],\n",
        "            \"low_grad\": [],\n",
        "            \"no_update_despite_grad\": [],\n",
        "            \"unchanged_from_init\": [],\n",
        "            \"high_grad\": [],\n",
        "        }\n",
        "\n",
        "        for name, stats in self.stats.items():\n",
        "            total_grads = len(stats[\"grad_history\"])\n",
        "\n",
        "            # Never updated\n",
        "            if stats[\"update_count\"] == 0:\n",
        "                problems[\"never_updated\"].append({\n",
        "                    \"name\": name,\n",
        "                    \"shape\": stats[\"shape\"],\n",
        "                    \"grad_count\": total_grads,\n",
        "                })\n",
        "\n",
        "            # Vanishing gradients\n",
        "            if total_grads > 0:\n",
        "                vanish_ratio = stats[\"vanishing_count\"] / total_grads\n",
        "                if vanish_ratio > 0.5:  # More than 50% vanishing\n",
        "                    avg_grad = np.mean([g[\"norm\"] for g in stats[\"grad_history\"]])\n",
        "                    problems[\"vanishing_grad\"].append({\n",
        "                        \"name\": name,\n",
        "                        \"vanish_ratio\": vanish_ratio,\n",
        "                        \"avg_grad_norm\": avg_grad,\n",
        "                        \"shape\": stats[\"shape\"],\n",
        "                    })\n",
        "\n",
        "                # Low gradients\n",
        "                low_ratio = stats[\"low_grad_count\"] / total_grads\n",
        "                if low_ratio > 0.5:\n",
        "                    problems[\"low_grad\"].append({\n",
        "                        \"name\": name,\n",
        "                        \"low_ratio\": low_ratio,\n",
        "                        \"shape\": stats[\"shape\"],\n",
        "                    })\n",
        "\n",
        "                # High gradients\n",
        "                high_ratio = stats[\"high_grad_count\"] / total_grads\n",
        "                if high_ratio > 0.1:  # More than 10% high\n",
        "                    max_grad = max(g[\"norm\"] for g in stats[\"grad_history\"])\n",
        "                    problems[\"high_grad\"].append({\n",
        "                        \"name\": name,\n",
        "                        \"high_ratio\": high_ratio,\n",
        "                        \"max_grad\": max_grad,\n",
        "                        \"shape\": stats[\"shape\"],\n",
        "                    })\n",
        "\n",
        "            # No update despite gradient\n",
        "            if stats[\"no_update_despite_grad\"] > 5:\n",
        "                problems[\"no_update_despite_grad\"].append({\n",
        "                    \"name\": name,\n",
        "                    \"count\": stats[\"no_update_despite_grad\"],\n",
        "                    \"shape\": stats[\"shape\"],\n",
        "                })\n",
        "\n",
        "            # Unchanged from initialization\n",
        "            init_change = (stats[\"last_param\"] - stats[\"init_param\"]).abs().max().item()\n",
        "            if init_change < 1e-6 and total_grads > 10:\n",
        "                problems[\"unchanged_from_init\"].append({\n",
        "                    \"name\": name,\n",
        "                    \"init_norm\": stats[\"init_norm\"],\n",
        "                    \"shape\": stats[\"shape\"],\n",
        "                })\n",
        "\n",
        "        return problems\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"Generate comprehensive gradient flow report\"\"\"\n",
        "        print(\"\\n\" + \"=\"*120)\n",
        "        print(\"🔬 ADVANCED GRADIENT FLOW ANALYSIS\")\n",
        "        print(\"=\"*120)\n",
        "\n",
        "        # Overall statistics\n",
        "        print(f\"\\n📊 OVERALL STATISTICS:\")\n",
        "        print(f\"   Total trainable parameters: {self.total_params:,}\")\n",
        "        print(f\"\\n   Module breakdown:\")\n",
        "        for module, info in sorted(self.module_info.items()):\n",
        "            print(f\"      {module:<20}: {info['params']:>12,} params in {info['count']:>4} tensors\")\n",
        "\n",
        "        # Get problem analysis\n",
        "        problems = self.get_problem_analysis()\n",
        "\n",
        "        # Report critical issues\n",
        "        print(\"\\n\" + \"=\"*120)\n",
        "        print(\"🚨 CRITICAL ISSUES:\")\n",
        "        print(\"=\"*120)\n",
        "\n",
        "        # 1. Never updated parameters - DETAILED\n",
        "        if problems[\"never_updated\"]:\n",
        "            print(f\"\\n❌ NEVER UPDATED ({len(problems['never_updated'])} parameters):\")\n",
        "            print(\"   These parameters never changed despite training:\")\n",
        "            print(\"-\" * 120)\n",
        "\n",
        "            # Show ALL never updated parameters\n",
        "            for i, p in enumerate(problems[\"never_updated\"], 1):\n",
        "                name = p['name']\n",
        "                stats = self.stats[name]\n",
        "\n",
        "                # Get initial values\n",
        "                init_mean = stats[\"init_param\"].mean().item()\n",
        "                init_std = stats[\"init_param\"].std().item() if stats[\"init_param\"].numel() > 1 else 0\n",
        "                init_norm = stats[\"init_norm\"]\n",
        "\n",
        "                # Check if it received any gradients\n",
        "                grad_info = \"No gradients\" if p['grad_count'] == 0 else f\"{p['grad_count']} gradients\"\n",
        "\n",
        "                print(f\"\\n   [{i}] {name}\")\n",
        "                print(f\"       Shape: {p['shape']}\")\n",
        "                print(f\"       Init values: mean={init_mean:.6f}, std={init_std:.6f}, norm={init_norm:.6f}\")\n",
        "                print(f\"       Status: {grad_info}\")\n",
        "\n",
        "        # 2. Vanishing gradients - DETAILED\n",
        "        if problems[\"vanishing_grad\"]:\n",
        "            print(f\"\\n⚠️  VANISHING GRADIENTS ({len(problems['vanishing_grad'])} parameters):\")\n",
        "            print(f\"   Gradient norm < {VANISHING_GRAD_THRESHOLD:.1e} for >50% of iterations:\")\n",
        "            print(\"-\" * 120)\n",
        "\n",
        "            # Show ALL vanishing gradient parameters with details\n",
        "            for i, p in enumerate(sorted(problems[\"vanishing_grad\"], key=lambda x: x[\"vanish_ratio\"], reverse=True), 1):\n",
        "                name = p['name']\n",
        "                stats = self.stats[name]\n",
        "\n",
        "                # Calculate statistics\n",
        "                init_val = stats[\"init_param\"].mean().item()\n",
        "                final_val = stats[\"last_param\"].mean().item()\n",
        "                param_change = (stats[\"last_param\"] - stats[\"init_param\"]).abs().max().item()\n",
        "\n",
        "                print(f\"\\n   [{i}] {name}\")\n",
        "                print(f\"       Shape: {p['shape']}, Vanish ratio: {p['vanish_ratio']:.1%}\")\n",
        "                print(f\"       Init → Final: mean={init_val:.6f} → {final_val:.6f}\")\n",
        "                print(f\"       Total change: {param_change:.10f}, Avg gradient: {p['avg_grad_norm']:.2e}\")\n",
        "                print(f\"       Updates: {stats['update_count']}/{len(stats['grad_history'])}\")\n",
        "\n",
        "        # 3. Low gradients - DETAILED\n",
        "        if problems[\"low_grad\"]:\n",
        "            print(f\"\\n⚠️  LOW GRADIENTS ({len(problems['low_grad'])} parameters):\")\n",
        "            print(f\"   Gradient norm < {LOW_GRAD_THRESHOLD:.1e} for >50% of iterations:\")\n",
        "            print(\"-\" * 120)\n",
        "            for i, p in enumerate(problems[\"low_grad\"], 1):\n",
        "                name = p['name']\n",
        "                stats = self.stats[name]\n",
        "\n",
        "                # Calculate detailed statistics\n",
        "                init_val = stats[\"init_param\"].mean().item()\n",
        "                final_val = stats[\"last_param\"].mean().item()\n",
        "                init_norm = stats[\"init_norm\"]\n",
        "                final_norm = stats[\"current_norm\"]\n",
        "                total_change = (stats[\"last_param\"] - stats[\"init_param\"]).abs().max().item()\n",
        "                avg_grad = np.mean([g[\"norm\"] for g in stats[\"grad_history\"]]) if stats[\"grad_history\"] else 0\n",
        "\n",
        "                print(f\"\\n   [{i}] {name}\")\n",
        "                print(f\"       Shape: {stats['shape']}, Low ratio: {p['low_ratio']:.1%}\")\n",
        "                print(f\"       Init → Final: mean={init_val:.6f} → {final_val:.6f}, norm={init_norm:.6f} → {final_norm:.6f}\")\n",
        "                print(f\"       Total change: {total_change:.8f}, Avg gradient: {avg_grad:.2e}\")\n",
        "                print(f\"       Updates: {stats['update_count']}/{len(stats['grad_history'])}\")\n",
        "\n",
        "        # 4. No update despite gradients - DETAILED\n",
        "        if problems[\"no_update_despite_grad\"]:\n",
        "            print(f\"\\n❓ NO UPDATE DESPITE GRADIENTS ({len(problems['no_update_despite_grad'])} parameters):\")\n",
        "            print(\"   These parameters received gradients but weren't updated by optimizer:\")\n",
        "            print(\"-\" * 120)\n",
        "            for i, p in enumerate(sorted(problems[\"no_update_despite_grad\"], key=lambda x: x[\"count\"], reverse=True), 1):\n",
        "                name = p['name']\n",
        "                stats = self.stats[name]\n",
        "\n",
        "                # Get gradient statistics\n",
        "                grad_norms = [g[\"norm\"] for g in stats[\"grad_history\"]]\n",
        "                avg_grad = np.mean(grad_norms) if grad_norms else 0\n",
        "                max_grad = max(grad_norms) if grad_norms else 0\n",
        "                min_grad = min(grad_norms) if grad_norms else 0\n",
        "\n",
        "                # Parameter values\n",
        "                init_val = stats[\"init_param\"].mean().item()\n",
        "                final_val = stats[\"last_param\"].mean().item()\n",
        "                init_norm = stats[\"init_norm\"]\n",
        "                final_norm = stats[\"current_norm\"]\n",
        "                param_change = (stats[\"last_param\"] - stats[\"init_param\"]).abs().max().item()\n",
        "\n",
        "                print(f\"\\n   [{i}] {name}\")\n",
        "                print(f\"       Shape: {stats['shape']}, No-update count: {p['count']}\")\n",
        "                print(f\"       Init → Final: mean={init_val:.6f} → {final_val:.6f}, norm={init_norm:.6f} → {final_norm:.6f}\")\n",
        "                print(f\"       Parameter change: {param_change:.10f}\")\n",
        "                print(f\"       Gradient stats: avg={avg_grad:.2e}, min={min_grad:.2e}, max={max_grad:.2e}\")\n",
        "                print(f\"       Total gradients received: {len(grad_norms)}, Updates: {stats['update_count']}\")\n",
        "\n",
        "        # 5. Unchanged from initialization - DETAILED\n",
        "        if problems[\"unchanged_from_init\"]:\n",
        "            print(f\"\\n🔒 UNCHANGED FROM INITIALIZATION ({len(problems['unchanged_from_init'])} parameters):\")\n",
        "            print(\"   Parameters that haven't changed from their initial values:\")\n",
        "            print(\"-\" * 120)\n",
        "            for i, p in enumerate(problems[\"unchanged_from_init\"], 1):\n",
        "                name = p['name']\n",
        "                stats = self.stats[name]\n",
        "\n",
        "                # Detailed analysis\n",
        "                init_val = stats[\"init_param\"].mean().item()\n",
        "                init_std = stats[\"init_param\"].std().item() if stats[\"init_param\"].numel() > 1 else 0\n",
        "                grad_count = len(stats[\"grad_history\"])\n",
        "                avg_grad = np.mean([g[\"norm\"] for g in stats[\"grad_history\"]]) if grad_count > 0 else 0\n",
        "\n",
        "                print(f\"\\n   [{i}] {name}\")\n",
        "                print(f\"       Shape: {stats['shape']}, Init norm: {p['init_norm']:.6f}\")\n",
        "                print(f\"       Init mean±std: {init_val:.6f} ± {init_std:.6f}\")\n",
        "                print(f\"       Gradients received: {grad_count}, Avg gradient: {avg_grad:.2e}\")\n",
        "                print(f\"       Update attempts: {stats['update_count']}\")\n",
        "\n",
        "        # 6. High gradients (potential instability) - DETAILED\n",
        "        if problems[\"high_grad\"]:\n",
        "            print(f\"\\n📈 HIGH GRADIENTS ({len(problems['high_grad'])} parameters):\")\n",
        "            print(f\"   Gradient norm > {HIGH_GRAD_THRESHOLD} detected:\")\n",
        "            print(\"-\" * 120)\n",
        "\n",
        "            for i, p in enumerate(sorted(problems[\"high_grad\"], key=lambda x: x[\"max_grad\"], reverse=True), 1):\n",
        "                name = p['name']\n",
        "                stats = self.stats[name]\n",
        "\n",
        "                # Parameter change analysis\n",
        "                init_val = stats[\"init_param\"].mean().item()\n",
        "                final_val = stats[\"last_param\"].mean().item()\n",
        "                param_change = (stats[\"last_param\"] - stats[\"init_param\"]).abs().max().item()\n",
        "\n",
        "                print(f\"\\n   [{i}] {name}\")\n",
        "                print(f\"       Shape: {p['shape']}, High grad ratio: {p['high_ratio']:.1%}\")\n",
        "                print(f\"       Max gradient: {p['max_grad']:.2e}\")\n",
        "                print(f\"       Init → Final: mean={init_val:.6f} → {final_val:.6f}\")\n",
        "                print(f\"       Total change: {param_change:.6f}\")\n",
        "\n",
        "        # Healthy parameters\n",
        "        print(\"\\n\" + \"=\"*120)\n",
        "        print(\"✅ HEALTHY PARAMETERS:\")\n",
        "        print(\"=\"*120)\n",
        "\n",
        "        healthy_params = []\n",
        "        for name, stats in self.stats.items():\n",
        "            if (stats[\"update_count\"] > MIN_UPDATES and\n",
        "                stats[\"vanishing_count\"] < len(stats[\"grad_history\"]) * 0.1 and\n",
        "                name not in [p[\"name\"] for p in problems[\"unchanged_from_init\"]]):\n",
        "\n",
        "                avg_grad = np.mean([g[\"norm\"] for g in stats[\"grad_history\"]]) if stats[\"grad_history\"] else 0\n",
        "                total_change = (stats[\"last_param\"] - stats[\"init_param\"]).abs().mean().item()\n",
        "\n",
        "                healthy_params.append({\n",
        "                    \"name\": name,\n",
        "                    \"updates\": stats[\"update_count\"],\n",
        "                    \"avg_grad\": avg_grad,\n",
        "                    \"total_change\": total_change,\n",
        "                })\n",
        "\n",
        "        print(f\"\\nFound {len(healthy_params)} healthy parameters (updating properly)\")\n",
        "        print(\"\\nTop 10 most active parameters:\")\n",
        "        for p in sorted(healthy_params, key=lambda x: x[\"total_change\"], reverse=True)[:10]:\n",
        "            print(f\"   • {p['name']:<60} updates={p['updates']:>3}, \"\n",
        "                  f\"avg_grad={p['avg_grad']:.2e}, Δ={p['total_change']:.4f}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*120)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# TRAINING LOOP\n",
        "# -----------------------------------------------------------------------------\n",
        "def train_diagnose():\n",
        "    \"\"\"Main training and diagnosis function\"\"\"\n",
        "    print(\"🚀 Starting Advanced Gradient Flow Diagnosis\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Setup\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"✓ Device: {device}\")\n",
        "\n",
        "    # Build model and criterion\n",
        "    print(\"✓ Building model...\")\n",
        "    model = build_model(\n",
        "        num_classes=NUM_CLASSES,\n",
        "        depths=[2, 2, 4, 2],\n",
        "        drop_path_max=0.1,\n",
        "        num_queries=200,\n",
        "        voc_prior=True,\n",
        "        norm=\"gn\"\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"✓ Building criterion...\")\n",
        "    criterion = build_criterion(num_classes=NUM_CLASSES)\n",
        "\n",
        "    # Model info\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"✓ Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
        "\n",
        "    # Build optimizer\n",
        "    print(\"✓ Building optimizer...\")\n",
        "    optimizer = build_optimizer(model, base_lr=LR)\n",
        "\n",
        "    # Dataset\n",
        "    print(f\"✓ Loading VOC dataset from: {VOC_ROOT}\")\n",
        "    train_ds = VOCDataset(VOC_ROOT, size=IMG_SIZE)\n",
        "    loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    # Initialize monitor\n",
        "    monitor = AdvancedGradientMonitor(model)\n",
        "\n",
        "    # Training\n",
        "    print(f\"\\n🏃 Training for {ITERS} iterations...\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    model.train()\n",
        "    loader_iter = iter(loader)\n",
        "\n",
        "    for it in range(1, ITERS + 1):\n",
        "        # Get batch\n",
        "        try:\n",
        "            imgs, targets = next(loader_iter)\n",
        "        except StopIteration:\n",
        "            loader_iter = iter(loader)\n",
        "            imgs, targets = next(loader_iter)\n",
        "\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(imgs, targets)\n",
        "\n",
        "        # Compute losses\n",
        "        loss_dict = criterion(outputs, targets)\n",
        "        total_loss = loss_dict[\"loss\"]\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Monitor gradients BEFORE clipping\n",
        "        monitor.after_backward(model)\n",
        "\n",
        "        # Gradient clipping\n",
        "        if hasattr(model, 'clip_dcn_grads'):\n",
        "            model.clip_dcn_grads(max_norm=0.5)\n",
        "\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Monitor parameter updates\n",
        "        monitor.after_step(model)\n",
        "\n",
        "        # Logging\n",
        "        if it % 20 == 0 or it == ITERS:\n",
        "            print(f\"[Iter {it:>3}/{ITERS}] Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "            # Quick gradient health check\n",
        "            vanishing_count = sum(1 for s in monitor.stats.values()\n",
        "                                if s[\"grad_history\"] and\n",
        "                                s[\"grad_history\"][-1][\"is_vanishing\"])\n",
        "            print(f\"   Vanishing gradients: {vanishing_count} parameters\")\n",
        "\n",
        "    # Generate final report\n",
        "    print(\"\\n🏁 Training Complete!\")\n",
        "    monitor.generate_report()\n",
        "\n",
        "    # Additional detailed analysis for specific categories\n",
        "    print(\"\\n\" + \"=\"*120)\n",
        "    print(\"📋 COMPLETE PARAMETER CHANGE REPORT\")\n",
        "    print(\"=\"*120)\n",
        "\n",
        "    problems = monitor.get_problem_analysis()\n",
        "\n",
        "    # Create comprehensive parameter change table\n",
        "    print(\"\\n📊 ALL PROBLEMATIC PARAMETERS - DETAILED VALUES:\\n\")\n",
        "    print(f\"{'Parameter':<65} | {'Category':<20} | {'Init Mean':<12} | {'Final Mean':<12} | {'Δ Change':<12} | {'Avg Grad':<10} | {'Updates':<8}\")\n",
        "    print(\"-\"*140)\n",
        "\n",
        "    # Collect all problematic parameters with their categories\n",
        "    all_problematic_params = []\n",
        "\n",
        "    for p in problems[\"never_updated\"]:\n",
        "        all_problematic_params.append((p[\"name\"], \"NEVER_UPDATED\", p))\n",
        "    for p in problems[\"vanishing_grad\"]:\n",
        "        all_problematic_params.append((p[\"name\"], \"VANISHING_GRAD\", p))\n",
        "    for p in problems[\"low_grad\"]:\n",
        "        all_problematic_params.append((p[\"name\"], \"LOW_GRAD\", p))\n",
        "    for p in problems[\"no_update_despite_grad\"]:\n",
        "        all_problematic_params.append((p[\"name\"], \"NO_UPDATE\", p))\n",
        "    for p in problems[\"unchanged_from_init\"]:\n",
        "        all_problematic_params.append((p[\"name\"], \"UNCHANGED\", p))\n",
        "\n",
        "    # Sort by parameter name for easy reference\n",
        "    for param_name, category, prob_info in sorted(all_problematic_params):\n",
        "        stats = monitor.stats[param_name]\n",
        "\n",
        "        # Calculate values\n",
        "        init_mean = stats[\"init_param\"].mean().item()\n",
        "        final_mean = stats[\"last_param\"].mean().item()\n",
        "        change = final_mean - init_mean\n",
        "        abs_change = abs(change)\n",
        "        avg_grad = np.mean([g[\"norm\"] for g in stats[\"grad_history\"]]) if stats[\"grad_history\"] else 0\n",
        "        updates = stats[\"update_count\"]\n",
        "\n",
        "        # Format output\n",
        "        print(f\"{param_name:<65} | {category:<20} | {init_mean:>12.6f} | {final_mean:>12.6f} | {change:>+12.8f} | {avg_grad:>10.2e} | {updates:>8}\")\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\"*140)\n",
        "    print(\"\\n📈 SUMMARY STATISTICS:\")\n",
        "\n",
        "    # Count parameters by module\n",
        "    module_problems = defaultdict(lambda: defaultdict(int))\n",
        "    for param_name, category, _ in all_problematic_params:\n",
        "        module = param_name.split('.')[0]\n",
        "        module_problems[module][category] += 1\n",
        "\n",
        "    print(\"\\n   Problems by module:\")\n",
        "    for module, categories in sorted(module_problems.items()):\n",
        "        total = sum(categories.values())\n",
        "        print(f\"      {module:<20}: {total:>3} problems\")\n",
        "        for cat, count in sorted(categories.items()):\n",
        "            print(f\"         - {cat:<20}: {count:>3}\")\n",
        "\n",
        "    # Save detailed report\n",
        "    print(\"\\n💾 Saving complete analysis to file...\")\n",
        "    with open(\"gradient_flow_detailed_report.txt\", \"w\") as f:\n",
        "        f.write(\"GRADIENT FLOW DETAILED ANALYSIS\\n\")\n",
        "        f.write(\"=\"*100 + \"\\n\\n\")\n",
        "        f.write(f\"Model: {model.__class__.__name__}\\n\")\n",
        "        f.write(f\"Total iterations: {ITERS}\\n\")\n",
        "        f.write(f\"Learning rate: {LR}\\n\")\n",
        "        f.write(f\"Batch size: {BATCH_SIZE}\\n\\n\")\n",
        "\n",
        "        # Write all problematic parameters\n",
        "        f.write(\"PROBLEMATIC PARAMETERS:\\n\")\n",
        "        f.write(\"-\"*100 + \"\\n\")\n",
        "        f.write(f\"{'Parameter':<65} | {'Init Mean':>12} | {'Final Mean':>12} | {'Change':>12} | {'Category'}\\n\")\n",
        "        f.write(\"-\"*100 + \"\\n\")\n",
        "\n",
        "        for param_name, category, _ in sorted(all_problematic_params):\n",
        "            stats = monitor.stats[param_name]\n",
        "            init_mean = stats[\"init_param\"].mean().item()\n",
        "            final_mean = stats[\"last_param\"].mean().item()\n",
        "            change = final_mean - init_mean\n",
        "            f.write(f\"{param_name:<65} | {init_mean:>12.6f} | {final_mean:>12.6f} | {change:>+12.8f} | {category}\\n\")\n",
        "\n",
        "    print(\"✓ Complete report saved to: gradient_flow_detailed_report.txt\")\n",
        "\n",
        "    return model, monitor\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# MAIN ENTRY\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(VOC_ROOT):\n",
        "        print(\"⚠️  VOC dataset not found!\")\n",
        "        print(f\"Expected path: {VOC_ROOT}\")\n",
        "    else:\n",
        "        print(f\"✓ VOC dataset found: {VOC_ROOT}\")\n",
        "\n",
        "        try:\n",
        "            model, monitor = train_diagnose()\n",
        "\n",
        "            # Additional detailed analysis\n",
        "            print(\"\\n\" + \"=\"*120)\n",
        "            print(\"📊 DETAILED PROBLEM SUMMARY:\")\n",
        "            print(\"=\"*120)\n",
        "\n",
        "            problems = monitor.get_problem_analysis()\n",
        "\n",
        "            # Create a detailed parameter change report\n",
        "            print(\"\\n🔍 PARAMETER CHANGE ANALYSIS:\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "            # Collect all problematic parameters\n",
        "            all_problematic = set()\n",
        "            for category in [\"never_updated\", \"vanishing_grad\", \"low_grad\",\n",
        "                           \"no_update_despite_grad\", \"unchanged_from_init\"]:\n",
        "                all_problematic.update(p[\"name\"] for p in problems[category])\n",
        "\n",
        "            print(f\"\\nTotal problematic parameters: {len(all_problematic)}\")\n",
        "\n",
        "            # Show detailed statistics for each category\n",
        "            print(f\"\\n📈 Category breakdown:\")\n",
        "            print(f\"   Never updated: {len(problems['never_updated'])} parameters\")\n",
        "            print(f\"   Vanishing gradients: {len(problems['vanishing_grad'])} parameters\")\n",
        "            print(f\"   Low gradients: {len(problems['low_grad'])} parameters\")\n",
        "            print(f\"   No update despite gradients: {len(problems['no_update_despite_grad'])} parameters\")\n",
        "            print(f\"   Unchanged from init: {len(problems['unchanged_from_init'])} parameters\")\n",
        "\n",
        "            # Export problematic parameters to a file for further analysis\n",
        "            print(\"\\n💾 Saving detailed analysis...\")\n",
        "            with open(\"gradient_analysis_report.txt\", \"w\") as f:\n",
        "                f.write(\"GRADIENT FLOW ANALYSIS REPORT\\n\")\n",
        "                f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "                for category, params in problems.items():\n",
        "                    if params:\n",
        "                        f.write(f\"\\n{category.upper()} ({len(params)} parameters):\\n\")\n",
        "                        f.write(\"-\"*60 + \"\\n\")\n",
        "                        for p in params:\n",
        "                            f.write(f\"{p}\\n\")\n",
        "\n",
        "            print(\"✓ Detailed report saved to: gradient_analysis_report.txt\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rry19BB0CeBt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}